"use strict";(globalThis.webpackChunkgithub_docs=globalThis.webpackChunkgithub_docs||[]).push([[9919],{6181:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>a,contentTitle:()=>o,default:()=>d,frontMatter:()=>l,metadata:()=>s,toc:()=>c});const s=JSON.parse('{"id":"python/practical/web-scraping","title":"1. \uc6f9 \uc2a4\ud06c\ub798\ud551","description":"\uc6f9 \uc2a4\ud06c\ub798\ud551\uc740 \uc6f9\uc0ac\uc774\ud2b8\uc5d0\uc11c \ub370\uc774\ud130\ub97c \uc790\ub3d9\uc73c\ub85c \uc218\uc9d1\ud558\ub294 \uae30\uc220\uc785\ub2c8\ub2e4. \uc774 \ubb38\uc11c\uc5d0\uc11c\ub294 Python\uc744 \uc0ac\uc6a9\ud55c \uc6f9 \uc2a4\ud06c\ub798\ud551\uc758 \uae30\ucd08\ubd80\ud130 \uace0\uae09 \uae30\ubc95\uae4c\uc9c0 \uc2e4\ubb34\uc5d0\uc11c \ud544\uc694\ud55c \ubaa8\ub4e0 \ub0b4\uc6a9\uc744 \ub2e4\ub8f9\ub2c8\ub2e4.","source":"@site/docs/python/practical/web-scraping.md","sourceDirName":"python/practical","slug":"/python/practical/web-scraping","permalink":"/tobias-docs/comp-2/docs/python/practical/web-scraping","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/python/practical/web-scraping.md","tags":[],"version":"current","sidebarPosition":1,"frontMatter":{"sidebar_position":1},"sidebar":"pythonSidebar","previous":{"title":"Python \ud559\uc2b5 \uac00\uc774\ub4dc","permalink":"/tobias-docs/comp-2/docs/python/intro"},"next":{"title":"2. REST API \uac1c\ubc1c","permalink":"/tobias-docs/comp-2/docs/python/practical/apis"}}');var r=t(4848),i=t(8453);const l={sidebar_position:1},o="1. \uc6f9 \uc2a4\ud06c\ub798\ud551",a={},c=[{value:"1.1 \uc6f9 \uc2a4\ud06c\ub798\ud551\uc774\ub780?",id:"11-\uc6f9-\uc2a4\ud06c\ub798\ud551\uc774\ub780",level:2},{value:"\uc6f9 \uc2a4\ud06c\ub798\ud551 vs \uc6f9 \ud06c\ub864\ub9c1",id:"\uc6f9-\uc2a4\ud06c\ub798\ud551-vs-\uc6f9-\ud06c\ub864\ub9c1",level:3},{value:"\ubc95\uc801 \uace0\ub824\uc0ac\ud56d",id:"\ubc95\uc801-\uace0\ub824\uc0ac\ud56d",level:3},{value:"robots.txt \uc774\ud574\ud558\uae30",id:"robotstxt-\uc774\ud574\ud558\uae30",level:3},{value:"1.2 requests \ub77c\uc774\ube0c\ub7ec\ub9ac",id:"12-requests-\ub77c\uc774\ube0c\ub7ec\ub9ac",level:2},{value:"\uc124\uce58",id:"\uc124\uce58",level:3},{value:"GET \uc694\uccad",id:"get-\uc694\uccad",level:3},{value:"POST \uc694\uccad",id:"post-\uc694\uccad",level:3},{value:"\ud5e4\ub354(Headers) \uc124\uc815",id:"\ud5e4\ub354headers-\uc124\uc815",level:3},{value:"\uc138\uc158(Sessions) \uc0ac\uc6a9",id:"\uc138\uc158sessions-\uc0ac\uc6a9",level:3},{value:"\ud0c0\uc784\uc544\uc6c3 \uc124\uc815",id:"\ud0c0\uc784\uc544\uc6c3-\uc124\uc815",level:3},{value:"1.3 BeautifulSoup4\ub85c HTML \ud30c\uc2f1",id:"13-beautifulsoup4\ub85c-html-\ud30c\uc2f1",level:2},{value:"\uc124\uce58",id:"\uc124\uce58-1",level:3},{value:"\uae30\ubcf8 \uc0ac\uc6a9\ubc95",id:"\uae30\ubcf8-\uc0ac\uc6a9\ubc95",level:3},{value:"find()\uc640 find_all()",id:"find\uc640-find_all",level:3},{value:"CSS \uc120\ud0dd\uc790 \uc0ac\uc6a9",id:"css-\uc120\ud0dd\uc790-\uc0ac\uc6a9",level:3},{value:"\uc694\uc18c \ud0d0\uc0c9",id:"\uc694\uc18c-\ud0d0\uc0c9",level:3},{value:"\uc18d\uc131\uacfc \ud14d\uc2a4\ud2b8 \ucd94\ucd9c",id:"\uc18d\uc131\uacfc-\ud14d\uc2a4\ud2b8-\ucd94\ucd9c",level:3},{value:"1.4 CSS \uc120\ud0dd\uc790 \uc644\ubcbd \uac00\uc774\ub4dc",id:"14-css-\uc120\ud0dd\uc790-\uc644\ubcbd-\uac00\uc774\ub4dc",level:2},{value:"\uae30\ubcf8 \uc120\ud0dd\uc790",id:"\uae30\ubcf8-\uc120\ud0dd\uc790",level:3},{value:"\uad00\uacc4 \uc120\ud0dd\uc790",id:"\uad00\uacc4-\uc120\ud0dd\uc790",level:3},{value:"\uc18d\uc131 \uc120\ud0dd\uc790",id:"\uc18d\uc131-\uc120\ud0dd\uc790",level:3},{value:"\uc758\uc0ac \ud074\ub798\uc2a4",id:"\uc758\uc0ac-\ud074\ub798\uc2a4",level:3},{value:"1.5 \ud398\uc774\uc9c0\ub124\uc774\uc158 \ucc98\ub9ac",id:"15-\ud398\uc774\uc9c0\ub124\uc774\uc158-\ucc98\ub9ac",level:2},{value:"URL \ud30c\ub77c\ubbf8\ud130 \ubc29\uc2dd",id:"url-\ud30c\ub77c\ubbf8\ud130-\ubc29\uc2dd",level:3},{value:"\ub2e4\uc74c \ud398\uc774\uc9c0 \ub9c1\ud06c \ub530\ub77c\uac00\uae30",id:"\ub2e4\uc74c-\ud398\uc774\uc9c0-\ub9c1\ud06c-\ub530\ub77c\uac00\uae30",level:3},{value:"\ubb34\ud55c \uc2a4\ud06c\ub864 \ucc98\ub9ac (API \ud65c\uc6a9)",id:"\ubb34\ud55c-\uc2a4\ud06c\ub864-\ucc98\ub9ac-api-\ud65c\uc6a9",level:3},{value:"1.6 \ub3d9\uc801 \uc6f9\uc0ac\uc774\ud2b8 \uc2a4\ud06c\ub798\ud551 (Selenium)",id:"16-\ub3d9\uc801-\uc6f9\uc0ac\uc774\ud2b8-\uc2a4\ud06c\ub798\ud551-selenium",level:2},{value:"\uc124\uce58",id:"\uc124\uce58-2",level:3},{value:"\uae30\ubcf8 \uc0ac\uc6a9\ubc95",id:"\uae30\ubcf8-\uc0ac\uc6a9\ubc95-1",level:3},{value:"\uc694\uc18c \uc120\ud0dd \ubc29\ubc95",id:"\uc694\uc18c-\uc120\ud0dd-\ubc29\ubc95",level:3},{value:"\uc0c1\ud638\uc791\uc6a9",id:"\uc0c1\ud638\uc791\uc6a9",level:3},{value:"\ubb34\ud55c \uc2a4\ud06c\ub864 \ucc98\ub9ac",id:"\ubb34\ud55c-\uc2a4\ud06c\ub864-\ucc98\ub9ac",level:3},{value:"\ub300\uae30 \uc804\ub7b5",id:"\ub300\uae30-\uc804\ub7b5",level:3},{value:"1.7 \uc18d\ub3c4 \uc81c\ud55c\uacfc \uc608\uc758 \ubc14\ub978 \ud06c\ub864\ub9c1",id:"17-\uc18d\ub3c4-\uc81c\ud55c\uacfc-\uc608\uc758-\ubc14\ub978-\ud06c\ub864\ub9c1",level:2},{value:"\uc9c0\uc5f0 \uc2dc\uac04 \ucd94\uac00",id:"\uc9c0\uc5f0-\uc2dc\uac04-\ucd94\uac00",level:3},{value:"\uc18d\ub3c4 \uc81c\ud55c \uad6c\ud604 (Rate Limiting)",id:"\uc18d\ub3c4-\uc81c\ud55c-\uad6c\ud604-rate-limiting",level:3},{value:"\uc7ac\uc2dc\ub3c4 \ub85c\uc9c1 (Retry Logic)",id:"\uc7ac\uc2dc\ub3c4-\ub85c\uc9c1-retry-logic",level:3},{value:"\uc218\ub3d9 \uc7ac\uc2dc\ub3c4 \uad6c\ud604",id:"\uc218\ub3d9-\uc7ac\uc2dc\ub3c4-\uad6c\ud604",level:3},{value:"1.8 \uc624\ub958 \ucc98\ub9ac",id:"18-\uc624\ub958-\ucc98\ub9ac",level:2},{value:"\uae30\ubcf8 \uc624\ub958 \ucc98\ub9ac",id:"\uae30\ubcf8-\uc624\ub958-\ucc98\ub9ac",level:3},{value:"BeautifulSoup \uc624\ub958 \ucc98\ub9ac",id:"beautifulsoup-\uc624\ub958-\ucc98\ub9ac",level:3},{value:"\uc885\ud569 \uc624\ub958 \ucc98\ub9ac \uc2a4\ud06c\ub798\ud37c",id:"\uc885\ud569-\uc624\ub958-\ucc98\ub9ac-\uc2a4\ud06c\ub798\ud37c",level:3},{value:"1.9 \ub370\uc774\ud130 \ucd94\ucd9c \ud328\ud134",id:"19-\ub370\uc774\ud130-\ucd94\ucd9c-\ud328\ud134",level:2},{value:"\ud14c\uc774\ube14 \ub370\uc774\ud130 \ucd94\ucd9c",id:"\ud14c\uc774\ube14-\ub370\uc774\ud130-\ucd94\ucd9c",level:3},{value:"\ub9ac\uc2a4\ud2b8 \ub370\uc774\ud130 \ucd94\ucd9c",id:"\ub9ac\uc2a4\ud2b8-\ub370\uc774\ud130-\ucd94\ucd9c",level:3},{value:"JSON \ub370\uc774\ud130 \ucd94\ucd9c (\uc2a4\ud06c\ub9bd\ud2b8 \ud0dc\uadf8 \ub0b4)",id:"json-\ub370\uc774\ud130-\ucd94\ucd9c-\uc2a4\ud06c\ub9bd\ud2b8-\ud0dc\uadf8-\ub0b4",level:3},{value:"\uc0c1\ub300 URL\uc744 \uc808\ub300 URL\ub85c \ubcc0\ud658",id:"\uc0c1\ub300-url\uc744-\uc808\ub300-url\ub85c-\ubcc0\ud658",level:3},{value:"1.10 \uc2e4\uc804 \uc608\uc81c",id:"110-\uc2e4\uc804-\uc608\uc81c",level:2},{value:"\uc608\uc81c 1: \ub274\uc2a4 \uae30\uc0ac \uc2a4\ud06c\ub798\ud551",id:"\uc608\uc81c-1-\ub274\uc2a4-\uae30\uc0ac-\uc2a4\ud06c\ub798\ud551",level:3},{value:"\uc608\uc81c 2: \uc81c\ud488 \uac00\uaca9 \ubaa8\ub2c8\ud130\ub9c1",id:"\uc608\uc81c-2-\uc81c\ud488-\uac00\uaca9-\ubaa8\ub2c8\ud130\ub9c1",level:3},{value:"\uc608\uc81c 3: \ub0a0\uc528 \ub370\uc774\ud130 \uc218\uc9d1",id:"\uc608\uc81c-3-\ub0a0\uc528-\ub370\uc774\ud130-\uc218\uc9d1",level:3},{value:"\uc608\uc81c 4: \ucc44\uc6a9 \uacf5\uace0 \uc218\uc9d1",id:"\uc608\uc81c-4-\ucc44\uc6a9-\uacf5\uace0-\uc218\uc9d1",level:3},{value:"1.11 \ub370\uc774\ud130 \uc800\uc7a5",id:"111-\ub370\uc774\ud130-\uc800\uc7a5",level:2},{value:"CSV \ud30c\uc77c\ub85c \uc800\uc7a5",id:"csv-\ud30c\uc77c\ub85c-\uc800\uc7a5",level:3},{value:"JSON \ud30c\uc77c\ub85c \uc800\uc7a5",id:"json-\ud30c\uc77c\ub85c-\uc800\uc7a5",level:3},{value:"SQLite \ub370\uc774\ud130\ubca0\uc774\uc2a4\uc5d0 \uc800\uc7a5",id:"sqlite-\ub370\uc774\ud130\ubca0\uc774\uc2a4\uc5d0-\uc800\uc7a5",level:3},{value:"Pandas\ub85c \ub370\uc774\ud130 \ucc98\ub9ac \ubc0f \uc800\uc7a5",id:"pandas\ub85c-\ub370\uc774\ud130-\ucc98\ub9ac-\ubc0f-\uc800\uc7a5",level:3},{value:"1.12 \ubaa8\ubc94 \uc0ac\ub840",id:"112-\ubaa8\ubc94-\uc0ac\ub840",level:2},{value:"User-Agent \ub85c\ud14c\uc774\uc158",id:"user-agent-\ub85c\ud14c\uc774\uc158",level:3},{value:"\ud504\ub85d\uc2dc \uc0ac\uc6a9",id:"\ud504\ub85d\uc2dc-\uc0ac\uc6a9",level:3},{value:"\uce90\uc2f1 \uad6c\ud604",id:"\uce90\uc2f1-\uad6c\ud604",level:3},{value:"\uacac\uace0\ud55c \uc2a4\ud06c\ub798\ud37c \ud15c\ud50c\ub9bf",id:"\uacac\uace0\ud55c-\uc2a4\ud06c\ub798\ud37c-\ud15c\ud50c\ub9bf",level:3},{value:"1.13 \uccb4\ud06c\ub9ac\uc2a4\ud2b8 \ubc0f \ud301",id:"113-\uccb4\ud06c\ub9ac\uc2a4\ud2b8-\ubc0f-\ud301",level:2},{value:"\uc2a4\ud06c\ub798\ud551 \uc2dc\uc791 \uc804 \uccb4\ud06c\ub9ac\uc2a4\ud2b8",id:"\uc2a4\ud06c\ub798\ud551-\uc2dc\uc791-\uc804-\uccb4\ud06c\ub9ac\uc2a4\ud2b8",level:3},{value:"\uc77c\ubc18\uc801\uc778 \ubb38\uc81c \ud574\uacb0",id:"\uc77c\ubc18\uc801\uc778-\ubb38\uc81c-\ud574\uacb0",level:3},{value:"\ub514\ubc84\uae45 \ud301",id:"\ub514\ubc84\uae45-\ud301",level:3},{value:"\uc5f0\uc2b5 \ubb38\uc81c",id:"\uc5f0\uc2b5-\ubb38\uc81c",level:2},{value:"\ubb38\uc81c 1: \ube14\ub85c\uadf8 \ud3ec\uc2a4\ud2b8 \uc218\uc9d1\uae30",id:"\ubb38\uc81c-1-\ube14\ub85c\uadf8-\ud3ec\uc2a4\ud2b8-\uc218\uc9d1\uae30",level:3},{value:"\ubb38\uc81c 2: \ub3d9\uc801 \ud398\uc774\uc9c0 \uc2a4\ud06c\ub798\ud551",id:"\ubb38\uc81c-2-\ub3d9\uc801-\ud398\uc774\uc9c0-\uc2a4\ud06c\ub798\ud551",level:3},{value:"\ubb38\uc81c 3: \uac00\uaca9 \ube44\uad50 \uc2a4\ud06c\ub798\ud37c",id:"\ubb38\uc81c-3-\uac00\uaca9-\ube44\uad50-\uc2a4\ud06c\ub798\ud37c",level:3},{value:"\ub2e4\uc74c \ub2e8\uacc4",id:"\ub2e4\uc74c-\ub2e8\uacc4",level:2}];function p(e){const n={a:"a",admonition:"admonition",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",input:"input",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,i.R)(),...e.components},{Details:t}=n;return t||function(e,n){throw new Error("Expected "+(n?"component":"object")+" `"+e+"` to be defined: you likely forgot to import, pass, or provide it.")}("Details",!0),(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.header,{children:(0,r.jsx)(n.h1,{id:"1-\uc6f9-\uc2a4\ud06c\ub798\ud551",children:"1. \uc6f9 \uc2a4\ud06c\ub798\ud551"})}),"\n",(0,r.jsx)(n.p,{children:"\uc6f9 \uc2a4\ud06c\ub798\ud551\uc740 \uc6f9\uc0ac\uc774\ud2b8\uc5d0\uc11c \ub370\uc774\ud130\ub97c \uc790\ub3d9\uc73c\ub85c \uc218\uc9d1\ud558\ub294 \uae30\uc220\uc785\ub2c8\ub2e4. \uc774 \ubb38\uc11c\uc5d0\uc11c\ub294 Python\uc744 \uc0ac\uc6a9\ud55c \uc6f9 \uc2a4\ud06c\ub798\ud551\uc758 \uae30\ucd08\ubd80\ud130 \uace0\uae09 \uae30\ubc95\uae4c\uc9c0 \uc2e4\ubb34\uc5d0\uc11c \ud544\uc694\ud55c \ubaa8\ub4e0 \ub0b4\uc6a9\uc744 \ub2e4\ub8f9\ub2c8\ub2e4."}),"\n",(0,r.jsx)(n.h2,{id:"11-\uc6f9-\uc2a4\ud06c\ub798\ud551\uc774\ub780",children:"1.1 \uc6f9 \uc2a4\ud06c\ub798\ud551\uc774\ub780?"}),"\n",(0,r.jsx)(n.p,{children:"\uc6f9 \uc2a4\ud06c\ub798\ud551\uc740 \uc6f9\ud398\uc774\uc9c0\uc758 HTML \uad6c\uc870\ub97c \ubd84\uc11d\ud558\uc5ec \uc6d0\ud558\ub294 \ub370\uc774\ud130\ub97c \ucd94\ucd9c\ud558\ub294 \ud504\ub85c\uc138\uc2a4\uc785\ub2c8\ub2e4."}),"\n",(0,r.jsx)(n.h3,{id:"\uc6f9-\uc2a4\ud06c\ub798\ud551-vs-\uc6f9-\ud06c\ub864\ub9c1",children:"\uc6f9 \uc2a4\ud06c\ub798\ud551 vs \uc6f9 \ud06c\ub864\ub9c1"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"\uc6f9 \uc2a4\ud06c\ub798\ud551"}),": \ud2b9\uc815 \uc6f9\ud398\uc774\uc9c0\uc5d0\uc11c \ub370\uc774\ud130\ub97c \ucd94\ucd9c\ud558\ub294 \uac83"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"\uc6f9 \ud06c\ub864\ub9c1"}),": \uc5ec\ub7ec \uc6f9\ud398\uc774\uc9c0\ub97c \uc790\ub3d9\uc73c\ub85c \ud0d0\uc0c9\ud558\uba70 \ub370\uc774\ud130\ub97c \uc218\uc9d1\ud558\ub294 \uac83"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"\ubc95\uc801-\uace0\ub824\uc0ac\ud56d",children:"\ubc95\uc801 \uace0\ub824\uc0ac\ud56d"}),"\n",(0,r.jsxs)(n.admonition,{title:"\ubc95\uc801 \ucc45\uc784",type:"warning",children:[(0,r.jsx)(n.p,{children:"\uc6f9 \uc2a4\ud06c\ub798\ud551\uc744 \uc218\ud589\ud558\uae30 \uc804\uc5d0 \ubc18\ub4dc\uc2dc \ub2e4\uc74c \uc0ac\ud56d\uc744 \ud655\uc778\ud574\uc57c \ud569\ub2c8\ub2e4:"}),(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"\uc6f9\uc0ac\uc774\ud2b8 \uc774\uc6a9\uc57d\uad00(Terms of Service)"})," \ud655\uc778"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"\uac1c\uc778\uc815\ubcf4\ubcf4\ud638\ubc95"})," \uc900\uc218"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"\uc800\uc791\uad8c\ubc95"})," \uc900\uc218"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"robots.txt"})," \ud30c\uc77c \ud655\uc778"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"\uc11c\ube44\uc2a4 \ubd80\ud558 \uace0\ub824"})," - \uacfc\ub3c4\ud55c \uc694\uccad\uc73c\ub85c \uc11c\ubc84\uc5d0 \ubd80\ub2f4\uc744 \uc8fc\uc9c0 \uc54a\uae30"]}),"\n"]})]}),"\n",(0,r.jsx)(n.h3,{id:"robotstxt-\uc774\ud574\ud558\uae30",children:"robots.txt \uc774\ud574\ud558\uae30"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.code,{children:"robots.txt"})," \ud30c\uc77c\uc740 \uc6f9\uc0ac\uc774\ud2b8\uc758 \ub8e8\ud2b8 \ub514\ub809\ud1a0\ub9ac\uc5d0 \uc704\uce58\ud558\uba70, \ud06c\ub864\ub7ec\uac00 \uc811\uadfc\ud560 \uc218 \uc788\ub294 \uc601\uc5ed\uc744 \uc815\uc758\ud569\ub2c8\ub2e4."]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'import requests\nfrom urllib.robotparser import RobotFileParser\n\ndef check_robots_txt(url):\n    """robots.txt \ud655\uc778"""\n    rp = RobotFileParser()\n    rp.set_url(f"{url}/robots.txt")\n    rp.read()\n\n    # User-agent\ubcc4 \ud5c8\uc6a9 \uc5ec\ubd80 \ud655\uc778\n    can_fetch = rp.can_fetch("*", url)\n    print(f"Can fetch {url}: {can_fetch}")\n\n    # Crawl-delay \ud655\uc778\n    crawl_delay = rp.crawl_delay("*")\n    print(f"Crawl delay: {crawl_delay}")\n\n    return can_fetch\n\n# \uc0ac\uc6a9 \uc608\uc2dc\ncheck_robots_txt("https://www.example.com/page")\n'})}),"\n",(0,r.jsx)(n.admonition,{title:"robots.txt \uc608\uc2dc",type:"tip",children:(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-text",children:"User-agent: *\nDisallow: /admin/\nDisallow: /private/\nAllow: /public/\nCrawl-delay: 2\n\nUser-agent: Googlebot\nDisallow: /temp/\n"})})}),"\n",(0,r.jsx)(n.h2,{id:"12-requests-\ub77c\uc774\ube0c\ub7ec\ub9ac",children:"1.2 requests \ub77c\uc774\ube0c\ub7ec\ub9ac"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.code,{children:"requests"}),"\ub294 Python\uc5d0\uc11c HTTP \uc694\uccad\uc744 \ubcf4\ub0b4\ub294 \uac00\uc7a5 \uc778\uae30 \uc788\ub294 \ub77c\uc774\ube0c\ub7ec\ub9ac\uc785\ub2c8\ub2e4."]}),"\n",(0,r.jsx)(n.h3,{id:"\uc124\uce58",children:"\uc124\uce58"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"pip install requests\n"})}),"\n",(0,r.jsx)(n.h3,{id:"get-\uc694\uccad",children:"GET \uc694\uccad"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"import requests\n\n# \uae30\ubcf8 GET \uc694\uccad\nresponse = requests.get('https://api.github.com')\nprint(f\"Status Code: {response.status_code}\")\nprint(f\"Content-Type: {response.headers['content-type']}\")\nprint(f\"Encoding: {response.encoding}\")\n\n# \uc751\ub2f5 \ub0b4\uc6a9\nprint(response.text)  # \ubb38\uc790\uc5f4\ub85c\nprint(response.json())  # JSON\uc73c\ub85c (JSON \uc751\ub2f5\uc778 \uacbd\uc6b0)\nprint(response.content)  # \ubc14\uc774\ud2b8\ub85c\n\n# URL \ud30c\ub77c\ubbf8\ud130 \uc804\ub2ec\nparams = {\n    'q': 'python',\n    'sort': 'stars',\n    'order': 'desc'\n}\nresponse = requests.get('https://api.github.com/search/repositories', params=params)\nprint(response.url)  # \uc2e4\uc81c \uc694\uccad\ub41c URL \ud655\uc778\n"})}),"\n",(0,r.jsx)(n.h3,{id:"post-\uc694\uccad",children:"POST \uc694\uccad"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"# \ud3fc \ub370\uc774\ud130 \uc804\uc1a1\ndata = {\n    'username': 'user123',\n    'password': 'pass123'\n}\nresponse = requests.post('https://httpbin.org/post', data=data)\n\n# JSON \ub370\uc774\ud130 \uc804\uc1a1\njson_data = {\n    'name': 'John Doe',\n    'email': 'john@example.com'\n}\nresponse = requests.post('https://httpbin.org/post', json=json_data)\n\n# \ud30c\uc77c \uc5c5\ub85c\ub4dc\nfiles = {'file': open('document.pdf', 'rb')}\nresponse = requests.post('https://httpbin.org/post', files=files)\n"})}),"\n",(0,r.jsx)(n.h3,{id:"\ud5e4\ub354headers-\uc124\uc815",children:"\ud5e4\ub354(Headers) \uc124\uc815"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"headers = {\n    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',\n    'Accept': 'text/html,application/xhtml+xml',\n    'Accept-Language': 'ko-KR,ko;q=0.9,en-US;q=0.8',\n    'Referer': 'https://www.google.com/',\n    'DNT': '1'\n}\n\nresponse = requests.get('https://www.example.com', headers=headers)\n"})}),"\n",(0,r.jsx)(n.admonition,{title:"User-Agent\ub97c \uc124\uc815\ud558\ub294 \uc774\uc720",type:"tip",children:(0,r.jsx)(n.p,{children:"\ub9ce\uc740 \uc6f9\uc0ac\uc774\ud2b8\ub294 User-Agent\ub97c \ud655\uc778\ud558\uc5ec \ubd07\uc778\uc9c0 \ud310\ub2e8\ud569\ub2c8\ub2e4. \uc801\uc808\ud55c User-Agent\ub97c \uc124\uc815\ud558\uba74 \uc815\uc0c1\uc801\uc778 \ube0c\ub77c\uc6b0\uc800 \uc811\uc18d\uc73c\ub85c \uc778\uc2dd\ub429\ub2c8\ub2e4."})}),"\n",(0,r.jsx)(n.h3,{id:"\uc138\uc158sessions-\uc0ac\uc6a9",children:"\uc138\uc158(Sessions) \uc0ac\uc6a9"}),"\n",(0,r.jsx)(n.p,{children:"\uc138\uc158\uc744 \uc0ac\uc6a9\ud558\uba74 \ucfe0\ud0a4\ub97c \uc790\ub3d9\uc73c\ub85c \uad00\ub9ac\ud558\uace0 \uc5f0\uacb0\uc744 \uc7ac\uc0ac\uc6a9\ud558\uc5ec \uc131\ub2a5\uc744 \ud5a5\uc0c1\uc2dc\ud0ac \uc218 \uc788\uc2b5\ub2c8\ub2e4."}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"# \uc138\uc158 \uc0dd\uc131\nsession = requests.Session()\n\n# \uc138\uc158 \ud5e4\ub354 \uc124\uc815 (\ubaa8\ub4e0 \uc694\uccad\uc5d0 \uc801\uc6a9)\nsession.headers.update({\n    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'\n})\n\n# \ub85c\uadf8\uc778\nlogin_data = {\n    'username': 'myuser',\n    'password': 'mypass'\n}\nsession.post('https://example.com/login', data=login_data)\n\n# \ub85c\uadf8\uc778 \ud6c4 \uc694\uccad (\ucfe0\ud0a4 \uc790\ub3d9 \ud3ec\ud568)\nresponse = session.get('https://example.com/dashboard')\n\n# \ucfe0\ud0a4 \ud655\uc778\nprint(session.cookies.get_dict())\n\n# \uc138\uc158 \uc885\ub8cc\nsession.close()\n"})}),"\n",(0,r.jsx)(n.h3,{id:"\ud0c0\uc784\uc544\uc6c3-\uc124\uc815",children:"\ud0c0\uc784\uc544\uc6c3 \uc124\uc815"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'try:\n    # \uc5f0\uacb0 \ud0c0\uc784\uc544\uc6c3: 3\ucd08, \uc77d\uae30 \ud0c0\uc784\uc544\uc6c3: 10\ucd08\n    response = requests.get(\'https://example.com\', timeout=(3, 10))\nexcept requests.Timeout:\n    print("\uc694\uccad \uc2dc\uac04 \ucd08\uacfc")\nexcept requests.ConnectionError:\n    print("\uc5f0\uacb0 \uc624\ub958")\n'})}),"\n",(0,r.jsx)(n.h2,{id:"13-beautifulsoup4\ub85c-html-\ud30c\uc2f1",children:"1.3 BeautifulSoup4\ub85c HTML \ud30c\uc2f1"}),"\n",(0,r.jsx)(n.p,{children:"BeautifulSoup\uc740 HTML\uacfc XML\uc744 \ud30c\uc2f1\ud558\uace0 \ud0d0\uc0c9\ud558\ub294 \uac15\ub825\ud55c \ub77c\uc774\ube0c\ub7ec\ub9ac\uc785\ub2c8\ub2e4."}),"\n",(0,r.jsx)(n.h3,{id:"\uc124\uce58-1",children:"\uc124\uce58"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"pip install beautifulsoup4\npip install lxml  # \ube60\ub978 \ud30c\uc11c\n"})}),"\n",(0,r.jsx)(n.h3,{id:"\uae30\ubcf8-\uc0ac\uc6a9\ubc95",children:"\uae30\ubcf8 \uc0ac\uc6a9\ubc95"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"from bs4 import BeautifulSoup\nimport requests\n\n# HTML \uac00\uc838\uc624\uae30\nresponse = requests.get('https://example.com')\nhtml = response.text\n\n# BeautifulSoup \uac1d\uccb4 \uc0dd\uc131\nsoup = BeautifulSoup(html, 'lxml')  # \ub610\ub294 'html.parser'\n\n# \uc804\uccb4 HTML \ucd9c\ub825 (\uc815\ub3c8\ub41c \ud615\ud0dc)\nprint(soup.prettify())\n\n# \uc81c\ubaa9 \uac00\uc838\uc624\uae30\nprint(soup.title)  # <title>...</title>\nprint(soup.title.string)  # \ud14d\uc2a4\ud2b8\ub9cc\nprint(soup.title.parent.name)  # \ubd80\ubaa8 \ud0dc\uadf8 \uc774\ub984\n"})}),"\n",(0,r.jsx)(n.h3,{id:"find\uc640-find_all",children:"find()\uc640 find_all()"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"# \uccab \ubc88\uc9f8 <p> \ud0dc\uadf8 \ucc3e\uae30\nfirst_p = soup.find('p')\nprint(first_p.text)\n\n# \ubaa8\ub4e0 <p> \ud0dc\uadf8 \ucc3e\uae30\nall_p = soup.find_all('p')\nfor p in all_p:\n    print(p.text)\n\n# \ud074\ub798\uc2a4\ub85c \ucc3e\uae30\narticle = soup.find('div', class_='article')\n# \ub610\ub294\narticle = soup.find('div', {'class': 'article'})\n\n# ID\ub85c \ucc3e\uae30\nheader = soup.find('div', id='header')\n# \ub610\ub294\nheader = soup.find(id='header')\n\n# \uc5ec\ub7ec \uc870\uac74\uc73c\ub85c \ucc3e\uae30\nsoup.find('a', {'class': 'link', 'target': '_blank'})\n\n# \uc5ec\ub7ec \ud0dc\uadf8 \ucc3e\uae30\nsoup.find_all(['h1', 'h2', 'h3'])\n\n# \uc815\uaddc\ud45c\ud604\uc2dd \uc0ac\uc6a9\nimport re\nsoup.find_all('a', href=re.compile(r'https://'))\n\n# \uc81c\ud55c\ub41c \uac1c\uc218\ub9cc \ucc3e\uae30\nsoup.find_all('p', limit=5)\n\n# \uc7ac\uadc0 \uac80\uc0c9 \uc81c\uc5b4\nsoup.find_all('title', recursive=False)  # \uc9c1\uc811 \uc790\uc2dd\ub9cc \uac80\uc0c9\n"})}),"\n",(0,r.jsx)(n.h3,{id:"css-\uc120\ud0dd\uc790-\uc0ac\uc6a9",children:"CSS \uc120\ud0dd\uc790 \uc0ac\uc6a9"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"# select(): \ubaa8\ub4e0 \ub9e4\uce6d \uc694\uc18c \ubc18\ud658\n# select_one(): \uccab \ubc88\uc9f8 \ub9e4\uce6d \uc694\uc18c\ub9cc \ubc18\ud658\n\n# \ud0dc\uadf8 \uc120\ud0dd\nsoup.select('p')\n\n# \ud074\ub798\uc2a4 \uc120\ud0dd\nsoup.select('.article')\nsoup.select('div.article')\n\n# ID \uc120\ud0dd\nsoup.select('#header')\n\n# \uc790\uc190 \uc120\ud0dd\uc790\nsoup.select('div p')  # div \uc548\uc758 \ubaa8\ub4e0 p\n\n# \uc790\uc2dd \uc120\ud0dd\uc790\nsoup.select('div > p')  # div\uc758 \uc9c1\uc811 \uc790\uc2dd p\ub9cc\n\n# \uc18d\uc131 \uc120\ud0dd\uc790\nsoup.select('a[href]')  # href \uc18d\uc131\uc774 \uc788\ub294 a\nsoup.select('a[href=\"https://example.com\"]')  # \ud2b9\uc815 \uac12\nsoup.select('a[href^=\"https\"]')  # https\ub85c \uc2dc\uc791\nsoup.select('a[href$=\".pdf\"]')  # .pdf\ub85c \ub05d\ub0a8\nsoup.select('a[href*=\"download\"]')  # download \ud3ec\ud568\n\n# \ubcf5\ud569 \uc120\ud0dd\uc790\nsoup.select('div.article > h2.title')\nsoup.select('ul.menu li a.active')\n\n# \uc758\uc0ac \ud074\ub798\uc2a4\nsoup.select('li:nth-of-type(2)')  # \ub450 \ubc88\uc9f8 li\nsoup.select('tr:nth-child(odd)')  # \ud640\uc218 \ubc88\uc9f8 tr\n"})}),"\n",(0,r.jsx)(n.h3,{id:"\uc694\uc18c-\ud0d0\uc0c9",children:"\uc694\uc18c \ud0d0\uc0c9"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"# \ubd80\ubaa8 \ud0d0\uc0c9\ntag = soup.find('p')\nprint(tag.parent)\nprint(tag.parents)  # \ubaa8\ub4e0 \ubd80\ubaa8 \uc694\uc18c\n\n# \ud615\uc81c \ud0d0\uc0c9\nprint(tag.next_sibling)  # \ub2e4\uc74c \ud615\uc81c\nprint(tag.previous_sibling)  # \uc774\uc804 \ud615\uc81c\nprint(tag.next_siblings)  # \uc774\ud6c4 \ubaa8\ub4e0 \ud615\uc81c\nprint(tag.previous_siblings)  # \uc774\uc804 \ubaa8\ub4e0 \ud615\uc81c\n\n# \uc790\uc2dd \ud0d0\uc0c9\nprint(tag.contents)  # \uc9c1\uc811 \uc790\uc2dd \ub9ac\uc2a4\ud2b8\nprint(tag.children)  # \uc9c1\uc811 \uc790\uc2dd \uc774\ud130\ub808\uc774\ud130\nprint(tag.descendants)  # \ubaa8\ub4e0 \uc790\uc190\n\n# \ub2e4\uc74c/\uc774\uc804 \uc694\uc18c (\uacf5\ubc31 \ud14d\uc2a4\ud2b8 \ud3ec\ud568)\nprint(tag.next_element)\nprint(tag.previous_element)\n"})}),"\n",(0,r.jsx)(n.h3,{id:"\uc18d\uc131\uacfc-\ud14d\uc2a4\ud2b8-\ucd94\ucd9c",children:"\uc18d\uc131\uacfc \ud14d\uc2a4\ud2b8 \ucd94\ucd9c"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"# \uc18d\uc131 \uac00\uc838\uc624\uae30\nlink = soup.find('a')\nprint(link['href'])  # href \uc18d\uc131\nprint(link.get('href'))  # \uc548\uc804\ud55c \ubc29\ubc95\nprint(link.attrs)  # \ubaa8\ub4e0 \uc18d\uc131 \ub515\uc154\ub108\ub9ac\n\n# \uc5ec\ub7ec \ud074\ub798\uc2a4 \ucc98\ub9ac\ndiv = soup.find('div', class_='article featured')\nprint(div['class'])  # ['article', 'featured']\n\n# \ud14d\uc2a4\ud2b8 \ucd94\ucd9c\nprint(tag.text)  # \ubaa8\ub4e0 \ud14d\uc2a4\ud2b8 (\uc790\uc190 \ud3ec\ud568)\nprint(tag.get_text())  # \uad6c\ubd84\uc790 \uc9c0\uc815 \uac00\ub2a5\nprint(tag.get_text(separator=' ', strip=True))\nprint(tag.string)  # \uc9c1\uc811 \ud14d\uc2a4\ud2b8\ub9cc (\uc790\uc2dd\uc774 \ud558\ub098\uc77c \ub54c)\nprint(tag.stripped_strings)  # \uacf5\ubc31 \uc81c\uac70\ub41c \ud14d\uc2a4\ud2b8\ub4e4\n"})}),"\n",(0,r.jsx)(n.h2,{id:"14-css-\uc120\ud0dd\uc790-\uc644\ubcbd-\uac00\uc774\ub4dc",children:"1.4 CSS \uc120\ud0dd\uc790 \uc644\ubcbd \uac00\uc774\ub4dc"}),"\n",(0,r.jsx)(n.h3,{id:"\uae30\ubcf8-\uc120\ud0dd\uc790",children:"\uae30\ubcf8 \uc120\ud0dd\uc790"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'from bs4 import BeautifulSoup\n\nhtml = """\n<div class="container">\n    <h1 id="title">\uc81c\ubaa9</h1>\n    <p class="intro">\uc18c\uac1c \ubb38\ub2e8</p>\n    <ul class="items">\n        <li class="item">\ud56d\ubaa9 1</li>\n        <li class="item active">\ud56d\ubaa9 2</li>\n        <li class="item">\ud56d\ubaa9 3</li>\n    </ul>\n    <a href="https://example.com" target="_blank">\ub9c1\ud06c</a>\n</div>\n"""\n\nsoup = BeautifulSoup(html, \'lxml\')\n\n# \ud0dc\uadf8 \uc120\ud0dd\nsoup.select(\'p\')  # \ubaa8\ub4e0 p \ud0dc\uadf8\n\n# \ud074\ub798\uc2a4 \uc120\ud0dd\nsoup.select(\'.item\')  # class="item"\uc778 \uc694\uc18c\n\n# ID \uc120\ud0dd\nsoup.select(\'#title\')  # id="title"\uc778 \uc694\uc18c\n\n# \uc870\ud569\nsoup.select(\'li.item\')  # class="item"\uc778 li \ud0dc\uadf8\nsoup.select(\'li.item.active\')  # \uc5ec\ub7ec \ud074\ub798\uc2a4\n'})}),"\n",(0,r.jsx)(n.h3,{id:"\uad00\uacc4-\uc120\ud0dd\uc790",children:"\uad00\uacc4 \uc120\ud0dd\uc790"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"# \uc790\uc190 \uc120\ud0dd\uc790 (\uacf5\ubc31)\nsoup.select('div p')  # div \uc548\uc758 \ubaa8\ub4e0 p\n\n# \uc790\uc2dd \uc120\ud0dd\uc790 (>)\nsoup.select('ul > li')  # ul\uc758 \uc9c1\uc811 \uc790\uc2dd li\ub9cc\n\n# \uc778\uc811 \ud615\uc81c \uc120\ud0dd\uc790 (+)\nsoup.select('h1 + p')  # h1 \ubc14\ub85c \ub2e4\uc74c\uc758 p\n\n# \uc77c\ubc18 \ud615\uc81c \uc120\ud0dd\uc790 (~)\nsoup.select('h1 ~ p')  # h1 \uc774\ud6c4\uc758 \ubaa8\ub4e0 p \ud615\uc81c\n"})}),"\n",(0,r.jsx)(n.h3,{id:"\uc18d\uc131-\uc120\ud0dd\uc790",children:"\uc18d\uc131 \uc120\ud0dd\uc790"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'# \uc18d\uc131 \uc874\uc7ac\nsoup.select(\'a[href]\')  # href \uc18d\uc131\uc774 \uc788\ub294 a\n\n# \uc18d\uc131 \uac12 \uc77c\uce58\nsoup.select(\'a[target="_blank"]\')\n\n# \uc18d\uc131 \uac12 \uc2dc\uc791 (^=)\nsoup.select(\'a[href^="https"]\')  # https\ub85c \uc2dc\uc791\n\n# \uc18d\uc131 \uac12 \ub05d ($=)\nsoup.select(\'a[href$=".pdf"]\')  # .pdf\ub85c \ub05d\ub0a8\n\n# \uc18d\uc131 \uac12 \ud3ec\ud568 (*=)\nsoup.select(\'a[href*="example"]\')  # example \ud3ec\ud568\n\n# \uc18d\uc131 \uac12 \uacf5\ubc31 \uad6c\ubd84 \ub2e8\uc5b4 (~=)\nsoup.select(\'div[class~="item"]\')  # class\uc5d0 item \ub2e8\uc5b4 \ud3ec\ud568\n\n# \uc18d\uc131 \uac12 \ud558\uc774\ud508 \uad6c\ubd84 (|=)\nsoup.select(\'div[lang|="en"]\')  # lang="en" \ub610\ub294 "en-US" \ub4f1\n'})}),"\n",(0,r.jsx)(n.h3,{id:"\uc758\uc0ac-\ud074\ub798\uc2a4",children:"\uc758\uc0ac \ud074\ub798\uc2a4"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"# nth-child\nsoup.select('li:nth-child(2)')  # \ub450 \ubc88\uc9f8 \uc790\uc2dd li\nsoup.select('li:nth-child(odd)')  # \ud640\uc218 \ubc88\uc9f8\nsoup.select('li:nth-child(even)')  # \uc9dd\uc218 \ubc88\uc9f8\nsoup.select('li:nth-child(3n)')  # 3\uc758 \ubc30\uc218 \ubc88\uc9f8\n\n# nth-of-type\nsoup.select('p:nth-of-type(1)')  # \uccab \ubc88\uc9f8 p \ud0c0\uc785\n\n# first-child, last-child\nsoup.select('li:first-child')\nsoup.select('li:last-child')\n\n# not\nsoup.select('li:not(.active)')  # active \ud074\ub798\uc2a4\uac00 \uc5c6\ub294 li\n"})}),"\n",(0,r.jsx)(n.h2,{id:"15-\ud398\uc774\uc9c0\ub124\uc774\uc158-\ucc98\ub9ac",children:"1.5 \ud398\uc774\uc9c0\ub124\uc774\uc158 \ucc98\ub9ac"}),"\n",(0,r.jsx)(n.p,{children:"\uc5ec\ub7ec \ud398\uc774\uc9c0\uc5d0 \uac78\uccd0 \uc788\ub294 \ub370\uc774\ud130\ub97c \uc218\uc9d1\ud558\ub294 \ubc29\ubc95\uc785\ub2c8\ub2e4."}),"\n",(0,r.jsx)(n.h3,{id:"url-\ud30c\ub77c\ubbf8\ud130-\ubc29\uc2dd",children:"URL \ud30c\ub77c\ubbf8\ud130 \ubc29\uc2dd"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"import requests\nfrom bs4 import BeautifulSoup\nimport time\n\ndef scrape_multiple_pages(base_url, max_pages=5):\n    \"\"\"\ud398\uc774\uc9c0 \ubc88\ud638\uac00 URL \ud30c\ub77c\ubbf8\ud130\uc778 \uacbd\uc6b0\"\"\"\n    all_data = []\n\n    for page in range(1, max_pages + 1):\n        print(f\"Scraping page {page}...\")\n\n        # URL \uad6c\uc131\n        url = f\"{base_url}?page={page}\"\n\n        try:\n            response = requests.get(url, timeout=10)\n            response.raise_for_status()\n\n            soup = BeautifulSoup(response.text, 'lxml')\n\n            # \ub370\uc774\ud130 \ucd94\ucd9c (\uc608: \uc81c\ud488 \uc815\ubcf4)\n            items = soup.select('.product-item')\n\n            if not items:\n                print(f\"No items found on page {page}\")\n                break\n\n            for item in items:\n                data = {\n                    'title': item.select_one('.title').text.strip(),\n                    'price': item.select_one('.price').text.strip(),\n                    'link': item.select_one('a')['href']\n                }\n                all_data.append(data)\n\n            # \uc608\uc758 \ubc14\ub978 \ud06c\ub864\ub9c1: \uc9c0\uc5f0 \uc2dc\uac04 \ucd94\uac00\n            time.sleep(1)\n\n        except Exception as e:\n            print(f\"Error on page {page}: {e}\")\n            continue\n\n    return all_data\n\n# \uc0ac\uc6a9\ndata = scrape_multiple_pages('https://example.com/products', max_pages=10)\nprint(f\"Total items collected: {len(data)}\")\n"})}),"\n",(0,r.jsx)(n.h3,{id:"\ub2e4\uc74c-\ud398\uc774\uc9c0-\ub9c1\ud06c-\ub530\ub77c\uac00\uae30",children:"\ub2e4\uc74c \ud398\uc774\uc9c0 \ub9c1\ud06c \ub530\ub77c\uac00\uae30"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"def scrape_with_next_button(start_url):\n    \"\"\"'\ub2e4\uc74c' \ubc84\ud2bc\uc744 \ub530\ub77c\uac00\ub294 \ubc29\uc2dd\"\"\"\n    all_data = []\n    current_url = start_url\n    page_count = 0\n\n    while current_url:\n        page_count += 1\n        print(f\"Scraping page {page_count}: {current_url}\")\n\n        try:\n            response = requests.get(current_url, timeout=10)\n            soup = BeautifulSoup(response.text, 'lxml')\n\n            # \ub370\uc774\ud130 \ucd94\ucd9c\n            items = soup.select('.item')\n            for item in items:\n                all_data.append(extract_item_data(item))\n\n            # \ub2e4\uc74c \ud398\uc774\uc9c0 \ub9c1\ud06c \ucc3e\uae30\n            next_button = soup.select_one('a.next-page')\n            if next_button and next_button.get('href'):\n                current_url = next_button['href']\n                # \uc0c1\ub300 URL \ucc98\ub9ac\n                if not current_url.startswith('http'):\n                    from urllib.parse import urljoin\n                    current_url = urljoin(start_url, current_url)\n            else:\n                current_url = None  # \ub354 \uc774\uc0c1 \ud398\uc774\uc9c0 \uc5c6\uc74c\n\n            time.sleep(1)\n\n        except Exception as e:\n            print(f\"Error: {e}\")\n            break\n\n    return all_data\n\ndef extract_item_data(item):\n    \"\"\"\uc544\uc774\ud15c\uc5d0\uc11c \ub370\uc774\ud130 \ucd94\ucd9c\"\"\"\n    return {\n        'title': item.select_one('.title').text.strip(),\n        'description': item.select_one('.desc').text.strip()\n    }\n"})}),"\n",(0,r.jsx)(n.h3,{id:"\ubb34\ud55c-\uc2a4\ud06c\ub864-\ucc98\ub9ac-api-\ud65c\uc6a9",children:"\ubb34\ud55c \uc2a4\ud06c\ub864 \ucc98\ub9ac (API \ud65c\uc6a9)"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'def scrape_infinite_scroll(api_url, max_items=100):\n    """\ubb34\ud55c \uc2a4\ud06c\ub864 \ud398\uc774\uc9c0 (API \uc694\uccad)"""\n    all_items = []\n    offset = 0\n    limit = 20\n\n    while len(all_items) < max_items:\n        params = {\n            \'offset\': offset,\n            \'limit\': limit\n        }\n\n        try:\n            response = requests.get(api_url, params=params, timeout=10)\n            data = response.json()\n\n            items = data.get(\'items\', [])\n            if not items:\n                break  # \ub354 \uc774\uc0c1 \ub370\uc774\ud130 \uc5c6\uc74c\n\n            all_items.extend(items)\n            offset += limit\n\n            print(f"Collected {len(all_items)} items...")\n            time.sleep(0.5)\n\n        except Exception as e:\n            print(f"Error: {e}")\n            break\n\n    return all_items[:max_items]\n'})}),"\n",(0,r.jsx)(n.h2,{id:"16-\ub3d9\uc801-\uc6f9\uc0ac\uc774\ud2b8-\uc2a4\ud06c\ub798\ud551-selenium",children:"1.6 \ub3d9\uc801 \uc6f9\uc0ac\uc774\ud2b8 \uc2a4\ud06c\ub798\ud551 (Selenium)"}),"\n",(0,r.jsx)(n.p,{children:"JavaScript\ub85c \uc0dd\uc131\ub418\ub294 \ub3d9\uc801 \ucf58\ud150\uce20\ub97c \uc2a4\ud06c\ub798\ud551\ud558\ub824\uba74 Selenium\uc774 \ud544\uc694\ud569\ub2c8\ub2e4."}),"\n",(0,r.jsx)(n.h3,{id:"\uc124\uce58-2",children:"\uc124\uce58"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"pip install selenium\n# Chrome WebDriver\ub294 \uc790\ub3d9\uc73c\ub85c \ub2e4\uc6b4\ub85c\ub4dc\ub429\ub2c8\ub2e4 (Selenium 4.6+)\n"})}),"\n",(0,r.jsx)(n.h3,{id:"\uae30\ubcf8-\uc0ac\uc6a9\ubc95-1",children:"\uae30\ubcf8 \uc0ac\uc6a9\ubc95"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"from selenium import webdriver\nfrom selenium.webdriver.common.by import By\nfrom selenium.webdriver.support.ui import WebDriverWait\nfrom selenium.webdriver.support import expected_conditions as EC\nfrom selenium.webdriver.chrome.options import Options\nimport time\n\n# Chrome \uc635\uc158 \uc124\uc815\nchrome_options = Options()\nchrome_options.add_argument('--headless')  # \ube0c\ub77c\uc6b0\uc800 \ucc3d \uc228\uae30\uae30\nchrome_options.add_argument('--no-sandbox')\nchrome_options.add_argument('--disable-dev-shm-usage')\nchrome_options.add_argument('--disable-blink-features=AutomationControlled')\nchrome_options.add_argument('user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36')\n\n# \ub4dc\ub77c\uc774\ubc84 \ucd08\uae30\ud654\ndriver = webdriver.Chrome(options=chrome_options)\n\ntry:\n    # \ud398\uc774\uc9c0 \ub85c\ub4dc\n    driver.get('https://example.com')\n\n    # \ud398\uc774\uc9c0 \ub85c\ub4dc \ub300\uae30\n    wait = WebDriverWait(driver, 10)\n\n    # \uc694\uc18c\uac00 \ub098\ud0c0\ub0a0 \ub54c\uae4c\uc9c0 \ub300\uae30\n    element = wait.until(\n        EC.presence_of_element_located((By.CLASS_NAME, 'product-list'))\n    )\n\n    # \uc694\uc18c \ucc3e\uae30\n    title = driver.find_element(By.TAG_NAME, 'h1')\n    print(title.text)\n\n    # \uc5ec\ub7ec \uc694\uc18c \ucc3e\uae30\n    items = driver.find_elements(By.CLASS_NAME, 'item')\n    for item in items:\n        print(item.text)\n\n    # HTML \uac00\uc838\uc624\uae30\n    html = driver.page_source\n\n    # BeautifulSoup\uacfc \ud568\uaed8 \uc0ac\uc6a9\n    from bs4 import BeautifulSoup\n    soup = BeautifulSoup(html, 'lxml')\n\nfinally:\n    driver.quit()\n"})}),"\n",(0,r.jsx)(n.h3,{id:"\uc694\uc18c-\uc120\ud0dd-\ubc29\ubc95",children:"\uc694\uc18c \uc120\ud0dd \ubc29\ubc95"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"# By.ID\nelement = driver.find_element(By.ID, 'username')\n\n# By.NAME\nelement = driver.find_element(By.NAME, 'email')\n\n# By.CLASS_NAME\nelement = driver.find_element(By.CLASS_NAME, 'btn-primary')\n\n# By.TAG_NAME\nelement = driver.find_element(By.TAG_NAME, 'h1')\n\n# By.CSS_SELECTOR\nelement = driver.find_element(By.CSS_SELECTOR, 'div.article > h2')\n\n# By.XPATH\nelement = driver.find_element(By.XPATH, '//div[@class=\"article\"]/h2')\n\n# By.LINK_TEXT\nelement = driver.find_element(By.LINK_TEXT, '\ub85c\uadf8\uc778')\n\n# By.PARTIAL_LINK_TEXT\nelement = driver.find_element(By.PARTIAL_LINK_TEXT, '\ub354\ubcf4\uae30')\n"})}),"\n",(0,r.jsx)(n.h3,{id:"\uc0c1\ud638\uc791\uc6a9",children:"\uc0c1\ud638\uc791\uc6a9"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"# \ud074\ub9ad\nbutton = driver.find_element(By.ID, 'submit-btn')\nbutton.click()\n\n# \ud14d\uc2a4\ud2b8 \uc785\ub825\ninput_field = driver.find_element(By.NAME, 'search')\ninput_field.clear()  # \uae30\uc874 \ud14d\uc2a4\ud2b8 \uc9c0\uc6b0\uae30\ninput_field.send_keys('Python')\n\n# \uc5d4\ud130 \ud0a4 \uc785\ub825\nfrom selenium.webdriver.common.keys import Keys\ninput_field.send_keys(Keys.RETURN)\n\n# \ub4dc\ub86d\ub2e4\uc6b4 \uc120\ud0dd\nfrom selenium.webdriver.support.ui import Select\nselect = Select(driver.find_element(By.ID, 'country'))\nselect.select_by_visible_text('South Korea')\nselect.select_by_value('KR')\nselect.select_by_index(1)\n\n# \uccb4\ud06c\ubc15\uc2a4/\ub77c\ub514\uc624 \ubc84\ud2bc\ncheckbox = driver.find_element(By.ID, 'agree')\nif not checkbox.is_selected():\n    checkbox.click()\n\n# \uc2a4\ud06c\ub864\ndriver.execute_script('window.scrollTo(0, document.body.scrollHeight);')\n\n# \ud2b9\uc815 \uc694\uc18c\ub85c \uc2a4\ud06c\ub864\nelement = driver.find_element(By.ID, 'footer')\ndriver.execute_script('arguments[0].scrollIntoView();', element)\n"})}),"\n",(0,r.jsx)(n.h3,{id:"\ubb34\ud55c-\uc2a4\ud06c\ub864-\ucc98\ub9ac",children:"\ubb34\ud55c \uc2a4\ud06c\ub864 \ucc98\ub9ac"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"def scrape_infinite_scroll_selenium(url):\n    driver = webdriver.Chrome(options=chrome_options)\n    driver.get(url)\n\n    last_height = driver.execute_script('return document.body.scrollHeight')\n\n    while True:\n        # \ud398\uc774\uc9c0 \ub05d\uae4c\uc9c0 \uc2a4\ud06c\ub864\n        driver.execute_script('window.scrollTo(0, document.body.scrollHeight);')\n\n        # \uc0c8 \ucf58\ud150\uce20 \ub85c\ub4dc \ub300\uae30\n        time.sleep(2)\n\n        # \uc0c8\ub85c\uc6b4 \ub192\uc774 \uacc4\uc0b0\n        new_height = driver.execute_script('return document.body.scrollHeight')\n\n        if new_height == last_height:\n            # \ub354 \uc774\uc0c1 \ub85c\ub4dc\ud560 \ucf58\ud150\uce20 \uc5c6\uc74c\n            break\n\n        last_height = new_height\n\n    # \ubaa8\ub4e0 \ucf58\ud150\uce20\uac00 \ub85c\ub4dc\ub41c \ud6c4 \ub370\uc774\ud130 \ucd94\ucd9c\n    soup = BeautifulSoup(driver.page_source, 'lxml')\n    items = soup.select('.item')\n\n    driver.quit()\n    return items\n"})}),"\n",(0,r.jsx)(n.h3,{id:"\ub300\uae30-\uc804\ub7b5",children:"\ub300\uae30 \uc804\ub7b5"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"from selenium.webdriver.support.ui import WebDriverWait\nfrom selenium.webdriver.support import expected_conditions as EC\n\n# \uba85\uc2dc\uc801 \ub300\uae30 (Explicit Wait)\nwait = WebDriverWait(driver, 10)  # \ucd5c\ub300 10\ucd08 \ub300\uae30\n\n# \uc694\uc18c\uac00 \ub098\ud0c0\ub0a0 \ub54c\uae4c\uc9c0\nelement = wait.until(EC.presence_of_element_located((By.ID, 'myElement')))\n\n# \uc694\uc18c\uac00 \ud074\ub9ad \uac00\ub2a5\ud560 \ub54c\uae4c\uc9c0\nelement = wait.until(EC.element_to_be_clickable((By.ID, 'button')))\n\n# \uc694\uc18c\uac00 \ubcf4\uc77c \ub54c\uae4c\uc9c0\nelement = wait.until(EC.visibility_of_element_located((By.CLASS_NAME, 'content')))\n\n# \ud14d\uc2a4\ud2b8\uac00 \ub098\ud0c0\ub0a0 \ub54c\uae4c\uc9c0\nwait.until(EC.text_to_be_present_in_element((By.ID, 'status'), 'Complete'))\n\n# \uc554\ubb35\uc801 \ub300\uae30 (Implicit Wait)\ndriver.implicitly_wait(10)  # \ubaa8\ub4e0 \uc694\uc18c \uac80\uc0c9\uc5d0 \ucd5c\ub300 10\ucd08 \ub300\uae30\n"})}),"\n",(0,r.jsx)(n.h2,{id:"17-\uc18d\ub3c4-\uc81c\ud55c\uacfc-\uc608\uc758-\ubc14\ub978-\ud06c\ub864\ub9c1",children:"1.7 \uc18d\ub3c4 \uc81c\ud55c\uacfc \uc608\uc758 \ubc14\ub978 \ud06c\ub864\ub9c1"}),"\n",(0,r.jsx)(n.p,{children:"\uc6f9\uc0ac\uc774\ud2b8 \uc11c\ubc84\uc5d0 \ubd80\ub2f4\uc744 \uc8fc\uc9c0 \uc54a\uace0 \ucc28\ub2e8\ub2f9\ud558\uc9c0 \uc54a\uae30 \uc704\ud55c \ubc29\ubc95\uc785\ub2c8\ub2e4."}),"\n",(0,r.jsx)(n.h3,{id:"\uc9c0\uc5f0-\uc2dc\uac04-\ucd94\uac00",children:"\uc9c0\uc5f0 \uc2dc\uac04 \ucd94\uac00"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'import time\nimport random\n\n# \uace0\uc815 \uc9c0\uc5f0\ntime.sleep(1)  # 1\ucd08 \ub300\uae30\n\n# \ub79c\ub364 \uc9c0\uc5f0 (\ub354 \uc790\uc5f0\uc2a4\ub7ec\uc6c0)\ntime.sleep(random.uniform(1, 3))  # 1~3\ucd08 \uc0ac\uc774 \ub79c\ub364\n\n# \uc694\uccad \uac04 \uc9c0\uc5f0\uc774 \uc788\ub294 \uc2a4\ud06c\ub798\ud37c\ndef polite_scraper(urls):\n    for url in urls:\n        response = requests.get(url)\n        # \ub370\uc774\ud130 \ucc98\ub9ac...\n\n        # \ub2e4\uc74c \uc694\uccad \uc804 \ub300\uae30\n        delay = random.uniform(2, 5)\n        print(f"Waiting {delay:.2f} seconds...")\n        time.sleep(delay)\n'})}),"\n",(0,r.jsx)(n.h3,{id:"\uc18d\ub3c4-\uc81c\ud55c-\uad6c\ud604-rate-limiting",children:"\uc18d\ub3c4 \uc81c\ud55c \uad6c\ud604 (Rate Limiting)"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'from time import time, sleep\n\nclass RateLimiter:\n    """\uc694\uccad \uc18d\ub3c4 \uc81c\ud55c"""\n\n    def __init__(self, max_requests, time_window):\n        """\n        max_requests: \uc2dc\uac04 \ucc3d \ub0b4 \ucd5c\ub300 \uc694\uccad \uc218\n        time_window: \uc2dc\uac04 \ucc3d (\ucd08)\n        """\n        self.max_requests = max_requests\n        self.time_window = time_window\n        self.requests = []\n\n    def wait_if_needed(self):\n        """\ud544\uc694\uc2dc \ub300\uae30"""\n        now = time()\n\n        # \uc2dc\uac04 \ucc3d \ubc16\uc758 \uc694\uccad \uc81c\uac70\n        self.requests = [req for req in self.requests\n                        if now - req < self.time_window]\n\n        if len(self.requests) >= self.max_requests:\n            # \uac00\uc7a5 \uc624\ub798\ub41c \uc694\uccad \uc774\ud6c4 \uacbd\uacfc \uc2dc\uac04\n            sleep_time = self.time_window - (now - self.requests[0])\n            if sleep_time > 0:\n                print(f"Rate limit reached. Sleeping for {sleep_time:.2f}s")\n                sleep(sleep_time)\n                self.requests = []\n\n        self.requests.append(now)\n\n# \uc0ac\uc6a9 \uc608\uc2dc: \ubd84\ub2f9 \ucd5c\ub300 10\uac1c \uc694\uccad\nlimiter = RateLimiter(max_requests=10, time_window=60)\n\nurls = [\'https://example.com/page1\', \'https://example.com/page2\', ...]\n\nfor url in urls:\n    limiter.wait_if_needed()\n    response = requests.get(url)\n    # \ucc98\ub9ac...\n'})}),"\n",(0,r.jsx)(n.h3,{id:"\uc7ac\uc2dc\ub3c4-\ub85c\uc9c1-retry-logic",children:"\uc7ac\uc2dc\ub3c4 \ub85c\uc9c1 (Retry Logic)"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'import requests\nfrom requests.adapters import HTTPAdapter\nfrom urllib3.util.retry import Retry\n\ndef create_session_with_retries():\n    """\uc7ac\uc2dc\ub3c4 \uae30\ub2a5\uc774 \uc788\ub294 \uc138\uc158 \uc0dd\uc131"""\n    session = requests.Session()\n\n    # \uc7ac\uc2dc\ub3c4 \uc804\ub7b5 \uc124\uc815\n    retry_strategy = Retry(\n        total=3,  # \ucd5c\ub300 3\ubc88 \uc7ac\uc2dc\ub3c4\n        backoff_factor=1,  # 1, 2, 4\ucd08\ub85c \ub300\uae30 \uc2dc\uac04 \uc99d\uac00\n        status_forcelist=[429, 500, 502, 503, 504],  # \uc7ac\uc2dc\ub3c4\ud560 \uc0c1\ud0dc \ucf54\ub4dc\n        allowed_methods=["HEAD", "GET", "OPTIONS", "POST"]\n    )\n\n    adapter = HTTPAdapter(max_retries=retry_strategy)\n    session.mount("http://", adapter)\n    session.mount("https://", adapter)\n\n    return session\n\n# \uc0ac\uc6a9\nsession = create_session_with_retries()\nresponse = session.get(\'https://example.com\')\n'})}),"\n",(0,r.jsx)(n.h3,{id:"\uc218\ub3d9-\uc7ac\uc2dc\ub3c4-\uad6c\ud604",children:"\uc218\ub3d9 \uc7ac\uc2dc\ub3c4 \uad6c\ud604"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'import time\nfrom requests.exceptions import RequestException\n\ndef fetch_with_retry(url, max_retries=3, backoff_factor=2):\n    """\uc7ac\uc2dc\ub3c4 \ub85c\uc9c1\uc774 \uc788\ub294 \uc694\uccad"""\n    for attempt in range(max_retries):\n        try:\n            response = requests.get(url, timeout=10)\n            response.raise_for_status()\n            return response\n\n        except RequestException as e:\n            if attempt == max_retries - 1:\n                # \ub9c8\uc9c0\ub9c9 \uc2dc\ub3c4\uc600\uc73c\uba74 \uc608\uc678 \ubc1c\uc0dd\n                raise\n\n            # \ub300\uae30 \uc2dc\uac04 \uacc4\uc0b0 (\uc9c0\uc218 \ubc31\uc624\ud504)\n            wait_time = backoff_factor ** attempt\n            print(f"Attempt {attempt + 1} failed: {e}")\n            print(f"Retrying in {wait_time} seconds...")\n            time.sleep(wait_time)\n\n# \uc0ac\uc6a9\ntry:\n    response = fetch_with_retry(\'https://example.com\')\nexcept RequestException as e:\n    print(f"Failed after all retries: {e}")\n'})}),"\n",(0,r.jsx)(n.admonition,{title:"\uc608\uc758 \ubc14\ub978 \ud06c\ub864\ub9c1 \uccb4\ud06c\ub9ac\uc2a4\ud2b8",type:"tip",children:(0,r.jsxs)(n.ul,{className:"contains-task-list",children:["\n",(0,r.jsxs)(n.li,{className:"task-list-item",children:[(0,r.jsx)(n.input,{type:"checkbox",disabled:!0})," ","robots.txt \ud655\uc778"]}),"\n",(0,r.jsxs)(n.li,{className:"task-list-item",children:[(0,r.jsx)(n.input,{type:"checkbox",disabled:!0})," ","User-Agent \uc124\uc815"]}),"\n",(0,r.jsxs)(n.li,{className:"task-list-item",children:[(0,r.jsx)(n.input,{type:"checkbox",disabled:!0})," ","\uc694\uccad \uac04 \uc9c0\uc5f0 \uc2dc\uac04 \ucd94\uac00 (\ucd5c\uc18c 1\ucd08)"]}),"\n",(0,r.jsxs)(n.li,{className:"task-list-item",children:[(0,r.jsx)(n.input,{type:"checkbox",disabled:!0})," ","\uc18d\ub3c4 \uc81c\ud55c \uad6c\ud604 (\ubd84\ub2f9 \uc694\uccad \uc218 \uc81c\ud55c)"]}),"\n",(0,r.jsxs)(n.li,{className:"task-list-item",children:[(0,r.jsx)(n.input,{type:"checkbox",disabled:!0})," ","\uc2e4\ud328 \uc2dc \uc7ac\uc2dc\ub3c4 \ub85c\uc9c1"]}),"\n",(0,r.jsxs)(n.li,{className:"task-list-item",children:[(0,r.jsx)(n.input,{type:"checkbox",disabled:!0})," ","\uc624\ub958 \ucc98\ub9ac \ubc0f \ub85c\uae45"]}),"\n",(0,r.jsxs)(n.li,{className:"task-list-item",children:[(0,r.jsx)(n.input,{type:"checkbox",disabled:!0})," ","\ud53c\ud06c \uc2dc\uac04\ub300 \ud53c\ud558\uae30"]}),"\n",(0,r.jsxs)(n.li,{className:"task-list-item",children:[(0,r.jsx)(n.input,{type:"checkbox",disabled:!0})," ","\ud544\uc694\ud55c \ub370\uc774\ud130\ub9cc \uc694\uccad"]}),"\n"]})}),"\n",(0,r.jsx)(n.h2,{id:"18-\uc624\ub958-\ucc98\ub9ac",children:"1.8 \uc624\ub958 \ucc98\ub9ac"}),"\n",(0,r.jsx)(n.p,{children:"\uacac\uace0\ud55c \uc6f9 \uc2a4\ud06c\ub798\ud37c\ub97c \ub9cc\ub4e4\uae30 \uc704\ud55c \uc624\ub958 \ucc98\ub9ac \ubc29\ubc95\uc785\ub2c8\ub2e4."}),"\n",(0,r.jsx)(n.h3,{id:"\uae30\ubcf8-\uc624\ub958-\ucc98\ub9ac",children:"\uae30\ubcf8 \uc624\ub958 \ucc98\ub9ac"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'import requests\nfrom requests.exceptions import RequestException, Timeout, ConnectionError, HTTPError\n\ndef safe_request(url):\n    """\uc548\uc804\ud55c HTTP \uc694\uccad"""\n    try:\n        response = requests.get(url, timeout=10)\n        response.raise_for_status()  # 4xx, 5xx \uc5d0\ub7ec \uccb4\ud06c\n        return response\n\n    except Timeout:\n        print(f"Request timeout: {url}")\n        return None\n\n    except ConnectionError:\n        print(f"Connection error: {url}")\n        return None\n\n    except HTTPError as e:\n        print(f"HTTP error {e.response.status_code}: {url}")\n        return None\n\n    except RequestException as e:\n        print(f"Request failed: {e}")\n        return None\n'})}),"\n",(0,r.jsx)(n.h3,{id:"beautifulsoup-\uc624\ub958-\ucc98\ub9ac",children:"BeautifulSoup \uc624\ub958 \ucc98\ub9ac"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'from bs4 import BeautifulSoup\n\ndef safe_parse(html):\n    """\uc548\uc804\ud55c HTML \ud30c\uc2f1"""\n    try:\n        soup = BeautifulSoup(html, \'lxml\')\n        return soup\n    except Exception as e:\n        print(f"Parsing error: {e}")\n        return None\n\ndef safe_extract(soup, selector, attribute=None):\n    """\uc548\uc804\ud55c \ub370\uc774\ud130 \ucd94\ucd9c"""\n    try:\n        element = soup.select_one(selector)\n\n        if element is None:\n            return None\n\n        if attribute:\n            return element.get(attribute, \'\').strip()\n        else:\n            return element.text.strip()\n\n    except (AttributeError, TypeError) as e:\n        print(f"Extraction error for selector \'{selector}\': {e}")\n        return None\n\n# \uc0ac\uc6a9 \uc608\uc2dc\nsoup = safe_parse(html)\nif soup:\n    title = safe_extract(soup, \'h1.title\')\n    link = safe_extract(soup, \'a.link\', \'href\')\n    price = safe_extract(soup, \'span.price\')\n'})}),"\n",(0,r.jsx)(n.h3,{id:"\uc885\ud569-\uc624\ub958-\ucc98\ub9ac-\uc2a4\ud06c\ub798\ud37c",children:"\uc885\ud569 \uc624\ub958 \ucc98\ub9ac \uc2a4\ud06c\ub798\ud37c"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'import logging\nfrom typing import Optional, Dict, Any\n\n# \ub85c\uae45 \uc124\uc815\nlogging.basicConfig(\n    level=logging.INFO,\n    format=\'%(asctime)s - %(levelname)s - %(message)s\',\n    handlers=[\n        logging.FileHandler(\'scraper.log\'),\n        logging.StreamHandler()\n    ]\n)\n\nclass WebScraper:\n    """\uacac\uace0\ud55c \uc6f9 \uc2a4\ud06c\ub798\ud37c"""\n\n    def __init__(self, max_retries=3):\n        self.session = create_session_with_retries()\n        self.max_retries = max_retries\n\n    def fetch_page(self, url: str) -> Optional[str]:\n        """\ud398\uc774\uc9c0 \uac00\uc838\uc624\uae30"""\n        try:\n            logging.info(f"Fetching: {url}")\n            response = self.session.get(url, timeout=10)\n            response.raise_for_status()\n            return response.text\n\n        except Exception as e:\n            logging.error(f"Failed to fetch {url}: {e}")\n            return None\n\n    def parse_page(self, html: str) -> Optional[BeautifulSoup]:\n        """HTML \ud30c\uc2f1"""\n        try:\n            return BeautifulSoup(html, \'lxml\')\n        except Exception as e:\n            logging.error(f"Failed to parse HTML: {e}")\n            return None\n\n    def extract_data(self, soup: BeautifulSoup) -> Dict[str, Any]:\n        """\ub370\uc774\ud130 \ucd94\ucd9c"""\n        data = {}\n\n        # \uac01 \ud544\ub4dc\ub97c \uc548\uc804\ud558\uac8c \ucd94\ucd9c\n        extractors = {\n            \'title\': (\'h1.title\', None),\n            \'price\': (\'span.price\', None),\n            \'image\': (\'img.product-image\', \'src\'),\n            \'description\': (\'div.description\', None)\n        }\n\n        for key, (selector, attr) in extractors.items():\n            try:\n                element = soup.select_one(selector)\n                if element:\n                    data[key] = element.get(attr) if attr else element.text.strip()\n                else:\n                    data[key] = None\n                    logging.warning(f"Element not found: {selector}")\n\n            except Exception as e:\n                logging.error(f"Error extracting {key}: {e}")\n                data[key] = None\n\n        return data\n\n    def scrape(self, url: str) -> Optional[Dict[str, Any]]:\n        """\uc804\uccb4 \uc2a4\ud06c\ub798\ud551 \ud504\ub85c\uc138\uc2a4"""\n        html = self.fetch_page(url)\n        if not html:\n            return None\n\n        soup = self.parse_page(html)\n        if not soup:\n            return None\n\n        data = self.extract_data(soup)\n\n        # \ud544\uc218 \ud544\ub4dc \uac80\uc99d\n        if not data.get(\'title\'):\n            logging.warning(f"Missing required field \'title\' for {url}")\n            return None\n\n        return data\n\n# \uc0ac\uc6a9\nscraper = WebScraper()\ndata = scraper.scrape(\'https://example.com/product/123\')\nif data:\n    print(data)\n'})}),"\n",(0,r.jsx)(n.h2,{id:"19-\ub370\uc774\ud130-\ucd94\ucd9c-\ud328\ud134",children:"1.9 \ub370\uc774\ud130 \ucd94\ucd9c \ud328\ud134"}),"\n",(0,r.jsx)(n.p,{children:"\uc77c\ubc18\uc801\uc778 \ub370\uc774\ud130 \ucd94\ucd9c \ud328\ud134\uacfc \uae30\ubc95\uc785\ub2c8\ub2e4."}),"\n",(0,r.jsx)(n.h3,{id:"\ud14c\uc774\ube14-\ub370\uc774\ud130-\ucd94\ucd9c",children:"\ud14c\uc774\ube14 \ub370\uc774\ud130 \ucd94\ucd9c"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"def extract_table(soup, table_selector='table'):\n    \"\"\"HTML \ud14c\uc774\ube14\uc744 \ub9ac\uc2a4\ud2b8\uc758 \ub515\uc154\ub108\ub9ac\ub85c \ubcc0\ud658\"\"\"\n    table = soup.select_one(table_selector)\n    if not table:\n        return []\n\n    # \ud5e4\ub354 \ucd94\ucd9c\n    headers = []\n    header_row = table.select_one('thead tr')\n    if header_row:\n        headers = [th.text.strip() for th in header_row.select('th')]\n    else:\n        # thead\uac00 \uc5c6\uc73c\uba74 \uccab \ubc88\uc9f8 \ud589\uc744 \ud5e4\ub354\ub85c\n        first_row = table.select_one('tr')\n        headers = [th.text.strip() for th in first_row.select('th, td')]\n\n    # \ub370\uc774\ud130 \ud589 \ucd94\ucd9c\n    rows = []\n    for tr in table.select('tbody tr'):\n        cells = [td.text.strip() for td in tr.select('td')]\n        if cells:\n            row_dict = dict(zip(headers, cells))\n            rows.append(row_dict)\n\n    return rows\n\n# \uc0ac\uc6a9 \uc608\uc2dc\nhtml = \"\"\"\n<table class=\"data-table\">\n    <thead>\n        <tr><th>\uc774\ub984</th><th>\ub098\uc774</th><th>\uc9c1\uc5c5</th></tr>\n    </thead>\n    <tbody>\n        <tr><td>\ud64d\uae38\ub3d9</td><td>30</td><td>\uac1c\ubc1c\uc790</td></tr>\n        <tr><td>\uae40\ucca0\uc218</td><td>25</td><td>\ub514\uc790\uc774\ub108</td></tr>\n    </tbody>\n</table>\n\"\"\"\n\nsoup = BeautifulSoup(html, 'lxml')\ndata = extract_table(soup, '.data-table')\nprint(data)\n# [{'\uc774\ub984': '\ud64d\uae38\ub3d9', '\ub098\uc774': '30', '\uc9c1\uc5c5': '\uac1c\ubc1c\uc790'},\n#  {'\uc774\ub984': '\uae40\ucca0\uc218', '\ub098\uc774': '25', '\uc9c1\uc5c5': '\ub514\uc790\uc774\ub108'}]\n"})}),"\n",(0,r.jsx)(n.h3,{id:"\ub9ac\uc2a4\ud2b8-\ub370\uc774\ud130-\ucd94\ucd9c",children:"\ub9ac\uc2a4\ud2b8 \ub370\uc774\ud130 \ucd94\ucd9c"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"def extract_product_list(soup):\n    \"\"\"\uc81c\ud488 \ubaa9\ub85d \ucd94\ucd9c\"\"\"\n    products = []\n\n    for item in soup.select('.product-item'):\n        product = {\n            'id': item.get('data-product-id'),\n            'title': safe_text(item, '.product-title'),\n            'price': parse_price(safe_text(item, '.price')),\n            'rating': parse_rating(item),\n            'image': safe_attr(item, 'img.product-image', 'src'),\n            'link': safe_attr(item, 'a.product-link', 'href'),\n            'in_stock': 'out-of-stock' not in item.get('class', [])\n        }\n        products.append(product)\n\n    return products\n\ndef safe_text(parent, selector):\n    \"\"\"\uc548\uc804\ud558\uac8c \ud14d\uc2a4\ud2b8 \ucd94\ucd9c\"\"\"\n    element = parent.select_one(selector)\n    return element.text.strip() if element else None\n\ndef safe_attr(parent, selector, attr):\n    \"\"\"\uc548\uc804\ud558\uac8c \uc18d\uc131 \ucd94\ucd9c\"\"\"\n    element = parent.select_one(selector)\n    return element.get(attr) if element else None\n\ndef parse_price(price_str):\n    \"\"\"\uac00\uaca9 \ubb38\uc790\uc5f4\uc744 \uc22b\uc790\ub85c \ubcc0\ud658\"\"\"\n    if not price_str:\n        return None\n\n    import re\n    # \uc22b\uc790\uc640 \uc18c\uc218\uc810\ub9cc \ucd94\ucd9c\n    price = re.sub(r'[^\\d.]', '', price_str)\n    try:\n        return float(price)\n    except ValueError:\n        return None\n\ndef parse_rating(item):\n    \"\"\"\ubcc4\uc810 \ucd94\ucd9c\"\"\"\n    rating_elem = item.select_one('.rating')\n    if not rating_elem:\n        return None\n\n    # \ubc29\ubc95 1: data \uc18d\uc131\uc5d0\uc11c\n    rating = rating_elem.get('data-rating')\n    if rating:\n        return float(rating)\n\n    # \ubc29\ubc95 2: \ubcc4 \uac1c\uc218 \uc138\uae30\n    filled_stars = len(rating_elem.select('.star.filled'))\n    return float(filled_stars)\n"})}),"\n",(0,r.jsx)(n.h3,{id:"json-\ub370\uc774\ud130-\ucd94\ucd9c-\uc2a4\ud06c\ub9bd\ud2b8-\ud0dc\uadf8-\ub0b4",children:"JSON \ub370\uc774\ud130 \ucd94\ucd9c (\uc2a4\ud06c\ub9bd\ud2b8 \ud0dc\uadf8 \ub0b4)"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"import json\nimport re\n\ndef extract_json_from_script(soup, pattern):\n    \"\"\"\uc2a4\ud06c\ub9bd\ud2b8 \ud0dc\uadf8\uc5d0\uc11c JSON \ucd94\ucd9c\"\"\"\n    scripts = soup.find_all('script', type='application/ld+json')\n\n    for script in scripts:\n        try:\n            data = json.loads(script.string)\n            return data\n        except (json.JSONDecodeError, AttributeError):\n            continue\n\n    # \uc77c\ubc18 \uc2a4\ud06c\ub9bd\ud2b8 \ud0dc\uadf8\uc5d0\uc11c \ud328\ud134\uc73c\ub85c \ucc3e\uae30\n    for script in soup.find_all('script'):\n        if not script.string:\n            continue\n\n        match = re.search(pattern, script.string)\n        if match:\n            try:\n                json_str = match.group(1)\n                return json.loads(json_str)\n            except json.JSONDecodeError:\n                continue\n\n    return None\n\n# \uc0ac\uc6a9 \uc608\uc2dc\n# window.__INITIAL_STATE__ = {...}; \ud328\ud134\ndata = extract_json_from_script(\n    soup,\n    r'window\\.__INITIAL_STATE__\\s*=\\s*({.*?});'\n)\n"})}),"\n",(0,r.jsx)(n.h3,{id:"\uc0c1\ub300-url\uc744-\uc808\ub300-url\ub85c-\ubcc0\ud658",children:"\uc0c1\ub300 URL\uc744 \uc808\ub300 URL\ub85c \ubcc0\ud658"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"from urllib.parse import urljoin, urlparse\n\ndef make_absolute_url(base_url, relative_url):\n    \"\"\"\uc0c1\ub300 URL\uc744 \uc808\ub300 URL\ub85c \ubcc0\ud658\"\"\"\n    if not relative_url:\n        return None\n\n    # \uc774\ubbf8 \uc808\ub300 URL\uc774\uba74 \uadf8\ub300\ub85c \ubc18\ud658\n    if urlparse(relative_url).netloc:\n        return relative_url\n\n    return urljoin(base_url, relative_url)\n\n# \uc0ac\uc6a9\nbase = 'https://example.com/products/page1'\nlinks = soup.select('a')\n\nfor link in links:\n    href = link.get('href')\n    absolute_url = make_absolute_url(base, href)\n    print(absolute_url)\n"})}),"\n",(0,r.jsx)(n.h2,{id:"110-\uc2e4\uc804-\uc608\uc81c",children:"1.10 \uc2e4\uc804 \uc608\uc81c"}),"\n",(0,r.jsx)(n.h3,{id:"\uc608\uc81c-1-\ub274\uc2a4-\uae30\uc0ac-\uc2a4\ud06c\ub798\ud551",children:"\uc608\uc81c 1: \ub274\uc2a4 \uae30\uc0ac \uc2a4\ud06c\ub798\ud551"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"import requests\nfrom bs4 import BeautifulSoup\nfrom datetime import datetime\nimport time\n\nclass NewsScraper:\n    \"\"\"\ub274\uc2a4 \uae30\uc0ac \uc2a4\ud06c\ub798\ud37c\"\"\"\n\n    def __init__(self):\n        self.session = requests.Session()\n        self.session.headers.update({\n            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'\n        })\n\n    def scrape_article(self, url):\n        \"\"\"\uac1c\ubcc4 \uae30\uc0ac \uc2a4\ud06c\ub798\ud551\"\"\"\n        try:\n            response = self.session.get(url, timeout=10)\n            response.raise_for_status()\n            soup = BeautifulSoup(response.text, 'lxml')\n\n            article = {\n                'url': url,\n                'title': self._extract_title(soup),\n                'author': self._extract_author(soup),\n                'date': self._extract_date(soup),\n                'content': self._extract_content(soup),\n                'tags': self._extract_tags(soup),\n                'scraped_at': datetime.now().isoformat()\n            }\n\n            return article\n\n        except Exception as e:\n            print(f\"Error scraping {url}: {e}\")\n            return None\n\n    def _extract_title(self, soup):\n        \"\"\"\uc81c\ubaa9 \ucd94\ucd9c\"\"\"\n        # \uc5ec\ub7ec \uc120\ud0dd\uc790 \uc2dc\ub3c4\n        selectors = ['h1.article-title', 'h1.post-title', 'h1', 'title']\n        for selector in selectors:\n            element = soup.select_one(selector)\n            if element:\n                return element.text.strip()\n        return None\n\n    def _extract_author(self, soup):\n        \"\"\"\uc791\uc131\uc790 \ucd94\ucd9c\"\"\"\n        selectors = [\n            'span.author',\n            'a.author',\n            'meta[name=\"author\"]',\n            '.byline'\n        ]\n        for selector in selectors:\n            if selector.startswith('meta'):\n                element = soup.select_one(selector)\n                if element:\n                    return element.get('content')\n            else:\n                element = soup.select_one(selector)\n                if element:\n                    return element.text.strip()\n        return None\n\n    def _extract_date(self, soup):\n        \"\"\"\ub0a0\uc9dc \ucd94\ucd9c\"\"\"\n        # meta \ud0dc\uadf8\uc5d0\uc11c\n        meta_date = soup.select_one('meta[property=\"article:published_time\"]')\n        if meta_date:\n            return meta_date.get('content')\n\n        # time \ud0dc\uadf8\uc5d0\uc11c\n        time_tag = soup.select_one('time')\n        if time_tag:\n            return time_tag.get('datetime') or time_tag.text.strip()\n\n        return None\n\n    def _extract_content(self, soup):\n        \"\"\"\ubcf8\ubb38 \ucd94\ucd9c\"\"\"\n        selectors = [\n            'div.article-content',\n            'div.post-content',\n            'article',\n            'div.entry-content'\n        ]\n\n        for selector in selectors:\n            content_div = soup.select_one(selector)\n            if content_div:\n                # \ubd88\ud544\uc694\ud55c \uc694\uc18c \uc81c\uac70\n                for tag in content_div.select('script, style, aside, .ad'):\n                    tag.decompose()\n\n                # \ubb38\ub2e8\ub4e4\uc744 \ubaa8\uc544\uc11c \ubc18\ud658\n                paragraphs = [p.text.strip() for p in content_div.find_all('p')]\n                return '\\n\\n'.join(paragraphs)\n\n        return None\n\n    def _extract_tags(self, soup):\n        \"\"\"\ud0dc\uadf8 \ucd94\ucd9c\"\"\"\n        tags = []\n        tag_elements = soup.select('a.tag, a.post-tag, .tags a')\n        for tag in tag_elements:\n            tags.append(tag.text.strip())\n        return tags\n\n    def scrape_article_list(self, list_url, max_articles=10):\n        \"\"\"\uae30\uc0ac \ubaa9\ub85d \ud398\uc774\uc9c0\uc5d0\uc11c \uc5ec\ub7ec \uae30\uc0ac \uc2a4\ud06c\ub798\ud551\"\"\"\n        try:\n            response = self.session.get(list_url, timeout=10)\n            soup = BeautifulSoup(response.text, 'lxml')\n\n            # \uae30\uc0ac \ub9c1\ud06c \ucd94\ucd9c\n            article_links = []\n            for link in soup.select('a.article-link, h2.article-title a')[:max_articles]:\n                href = link.get('href')\n                if href:\n                    article_links.append(urljoin(list_url, href))\n\n            # \uac01 \uae30\uc0ac \uc2a4\ud06c\ub798\ud551\n            articles = []\n            for i, link in enumerate(article_links, 1):\n                print(f\"Scraping article {i}/{len(article_links)}: {link}\")\n                article = self.scrape_article(link)\n                if article:\n                    articles.append(article)\n                time.sleep(1)  # \uc608\uc758 \ubc14\ub978 \ud06c\ub864\ub9c1\n\n            return articles\n\n        except Exception as e:\n            print(f\"Error scraping article list: {e}\")\n            return []\n\n# \uc0ac\uc6a9 \uc608\uc2dc\nscraper = NewsScraper()\narticles = scraper.scrape_article_list('https://news.example.com/', max_articles=5)\n\nfor article in articles:\n    print(f\"\\n\uc81c\ubaa9: {article['title']}\")\n    print(f\"\uc791\uc131\uc790: {article['author']}\")\n    print(f\"\ub0a0\uc9dc: {article['date']}\")\n    print(f\"\ud0dc\uadf8: {', '.join(article['tags'])}\")\n    print(f\"\ub0b4\uc6a9 \ubbf8\ub9ac\ubcf4\uae30: {article['content'][:100]}...\")\n"})}),"\n",(0,r.jsx)(n.h3,{id:"\uc608\uc81c-2-\uc81c\ud488-\uac00\uaca9-\ubaa8\ub2c8\ud130\ub9c1",children:"\uc608\uc81c 2: \uc81c\ud488 \uac00\uaca9 \ubaa8\ub2c8\ud130\ub9c1"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"import csv\nfrom datetime import datetime\n\nclass PriceMonitor:\n    \"\"\"\uc81c\ud488 \uac00\uaca9 \ubaa8\ub2c8\ud130\ub9c1\"\"\"\n\n    def __init__(self, csv_file='price_history.csv'):\n        self.csv_file = csv_file\n        self.session = requests.Session()\n        self.session.headers.update({\n            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'\n        })\n\n    def get_product_price(self, url):\n        \"\"\"\uc81c\ud488 \uac00\uaca9 \uac00\uc838\uc624\uae30\"\"\"\n        try:\n            response = self.session.get(url, timeout=10)\n            soup = BeautifulSoup(response.text, 'lxml')\n\n            # \uac00\uaca9 \ucd94\ucd9c (\uc5ec\ub7ec \ud328\ud134 \uc2dc\ub3c4)\n            price = None\n            price_selectors = [\n                'span.price',\n                'span.product-price',\n                'div.price-box span.price',\n                'meta[property=\"product:price:amount\"]'\n            ]\n\n            for selector in price_selectors:\n                if selector.startswith('meta'):\n                    elem = soup.select_one(selector)\n                    if elem:\n                        price = elem.get('content')\n                        break\n                else:\n                    elem = soup.select_one(selector)\n                    if elem:\n                        price = elem.text.strip()\n                        break\n\n            # \uac00\uaca9\uc744 \uc22b\uc790\ub85c \ubcc0\ud658\n            if price:\n                import re\n                price_num = re.sub(r'[^\\d.]', '', price)\n                return float(price_num)\n\n            return None\n\n        except Exception as e:\n            print(f\"Error getting price from {url}: {e}\")\n            return None\n\n    def save_price(self, product_name, url, price):\n        \"\"\"\uac00\uaca9 \uc774\ub825 \uc800\uc7a5\"\"\"\n        timestamp = datetime.now().isoformat()\n\n        # CSV \ud30c\uc77c\uc5d0 \ucd94\uac00\n        with open(self.csv_file, 'a', newline='', encoding='utf-8') as f:\n            writer = csv.writer(f)\n            writer.writerow([timestamp, product_name, url, price])\n\n    def monitor_products(self, products):\n        \"\"\"\uc5ec\ub7ec \uc81c\ud488 \ubaa8\ub2c8\ud130\ub9c1\n\n        products: [{'name': '\uc81c\ud488\uba85', 'url': 'URL'}, ...]\n        \"\"\"\n        print(f\"Monitoring {len(products)} products...\")\n\n        for product in products:\n            name = product['name']\n            url = product['url']\n\n            print(f\"Checking {name}...\")\n            price = self.get_product_price(url)\n\n            if price:\n                print(f\"  Price: {price:,.0f}\uc6d0\")\n                self.save_price(name, url, price)\n            else:\n                print(f\"  Failed to get price\")\n\n            time.sleep(2)  # \uc9c0\uc5f0\n\n    def get_price_history(self, product_name):\n        \"\"\"\ud2b9\uc815 \uc81c\ud488\uc758 \uac00\uaca9 \uc774\ub825 \uc870\ud68c\"\"\"\n        history = []\n\n        try:\n            with open(self.csv_file, 'r', encoding='utf-8') as f:\n                reader = csv.reader(f)\n                for row in reader:\n                    if len(row) >= 4 and row[1] == product_name:\n                        history.append({\n                            'timestamp': row[0],\n                            'price': float(row[3])\n                        })\n        except FileNotFoundError:\n            pass\n\n        return history\n\n# \uc0ac\uc6a9 \uc608\uc2dc\nmonitor = PriceMonitor('prices.csv')\n\nproducts = [\n    {'name': '\ub178\ud2b8\ubd81 A', 'url': 'https://shop.example.com/laptop-a'},\n    {'name': '\ub9c8\uc6b0\uc2a4 B', 'url': 'https://shop.example.com/mouse-b'},\n]\n\n# \uac00\uaca9 \ubaa8\ub2c8\ud130\ub9c1 (\uc815\uae30\uc801\uc73c\ub85c \uc2e4\ud589)\nmonitor.monitor_products(products)\n\n# \uac00\uaca9 \uc774\ub825 \ud655\uc778\nhistory = monitor.get_price_history('\ub178\ud2b8\ubd81 A')\nfor record in history:\n    print(f\"{record['timestamp']}: {record['price']:,.0f}\uc6d0\")\n"})}),"\n",(0,r.jsx)(n.h3,{id:"\uc608\uc81c-3-\ub0a0\uc528-\ub370\uc774\ud130-\uc218\uc9d1",children:"\uc608\uc81c 3: \ub0a0\uc528 \ub370\uc774\ud130 \uc218\uc9d1"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'class WeatherScraper:\n    """\ub0a0\uc528 \ub370\uc774\ud130 \uc2a4\ud06c\ub798\ud37c"""\n\n    def __init__(self):\n        self.session = requests.Session()\n        self.session.headers.update({\n            \'User-Agent\': \'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36\'\n        })\n\n    def get_weather(self, city_url):\n        """\ub0a0\uc528 \uc815\ubcf4 \uac00\uc838\uc624\uae30"""\n        try:\n            response = self.session.get(city_url, timeout=10)\n            soup = BeautifulSoup(response.text, \'lxml\')\n\n            weather = {\n                \'timestamp\': datetime.now().isoformat(),\n                \'temperature\': self._extract_temperature(soup),\n                \'condition\': self._extract_condition(soup),\n                \'humidity\': self._extract_humidity(soup),\n                \'wind_speed\': self._extract_wind_speed(soup),\n                \'precipitation\': self._extract_precipitation(soup)\n            }\n\n            return weather\n\n        except Exception as e:\n            print(f"Error getting weather: {e}")\n            return None\n\n    def _extract_temperature(self, soup):\n        """\uc628\ub3c4 \ucd94\ucd9c"""\n        temp_elem = soup.select_one(\'.temperature, .temp, [class*="temp"]\')\n        if temp_elem:\n            import re\n            temp_text = temp_elem.text\n            match = re.search(r\'-?\\d+\', temp_text)\n            if match:\n                return int(match.group())\n        return None\n\n    def _extract_condition(self, soup):\n        """\ub0a0\uc528 \uc0c1\ud0dc \ucd94\ucd9c (\ub9d1\uc74c, \ud750\ub9bc \ub4f1)"""\n        condition_elem = soup.select_one(\'.condition, .weather-condition, .sky\')\n        if condition_elem:\n            return condition_elem.text.strip()\n        return None\n\n    def _extract_humidity(self, soup):\n        """\uc2b5\ub3c4 \ucd94\ucd9c"""\n        humidity_elem = soup.select_one(\'.humidity, [class*="humid"]\')\n        if humidity_elem:\n            import re\n            match = re.search(r\'\\d+\', humidity_elem.text)\n            if match:\n                return int(match.group())\n        return None\n\n    def _extract_wind_speed(self, soup):\n        """\ud48d\uc18d \ucd94\ucd9c"""\n        wind_elem = soup.select_one(\'.wind-speed, [class*="wind"]\')\n        if wind_elem:\n            import re\n            match = re.search(r\'\\d+\\.?\\d*\', wind_elem.text)\n            if match:\n                return float(match.group())\n        return None\n\n    def _extract_precipitation(self, soup):\n        """\uac15\uc218\ub7c9 \ucd94\ucd9c"""\n        precip_elem = soup.select_one(\'.precipitation, .rainfall, [class*="rain"]\')\n        if precip_elem:\n            import re\n            match = re.search(r\'\\d+\\.?\\d*\', precip_elem.text)\n            if match:\n                return float(match.group())\n        return None\n\n    def get_forecast(self, city_url, days=7):\n        """\uc77c\uae30 \uc608\ubcf4 \uac00\uc838\uc624\uae30"""\n        try:\n            response = self.session.get(city_url, timeout=10)\n            soup = BeautifulSoup(response.text, \'lxml\')\n\n            forecast = []\n            forecast_items = soup.select(\'.forecast-day, .day-forecast\')[:days]\n\n            for item in forecast_items:\n                day = {\n                    \'date\': self._safe_text(item, \'.date, .forecast-date\'),\n                    \'high\': self._extract_temp_from_elem(item, \'.high, .temp-high\'),\n                    \'low\': self._extract_temp_from_elem(item, \'.low, .temp-low\'),\n                    \'condition\': self._safe_text(item, \'.condition, .weather\')\n                }\n                forecast.append(day)\n\n            return forecast\n\n        except Exception as e:\n            print(f"Error getting forecast: {e}")\n            return []\n\n    def _safe_text(self, parent, selector):\n        """\uc548\uc804\ud558\uac8c \ud14d\uc2a4\ud2b8 \ucd94\ucd9c"""\n        elem = parent.select_one(selector)\n        return elem.text.strip() if elem else None\n\n    def _extract_temp_from_elem(self, parent, selector):\n        """\uc694\uc18c\uc5d0\uc11c \uc628\ub3c4 \ucd94\ucd9c"""\n        elem = parent.select_one(selector)\n        if elem:\n            import re\n            match = re.search(r\'-?\\d+\', elem.text)\n            if match:\n                return int(match.group())\n        return None\n\n# \uc0ac\uc6a9 \uc608\uc2dc\nweather_scraper = WeatherScraper()\ncurrent = weather_scraper.get_weather(\'https://weather.example.com/seoul\')\n\nif current:\n    print(f"\ud604\uc7ac \ub0a0\uc528:")\n    print(f"  \uc628\ub3c4: {current[\'temperature\']}\xb0C")\n    print(f"  \uc0c1\ud0dc: {current[\'condition\']}")\n    print(f"  \uc2b5\ub3c4: {current[\'humidity\']}%")\n    print(f"  \ud48d\uc18d: {current[\'wind_speed\']}m/s")\n'})}),"\n",(0,r.jsx)(n.h3,{id:"\uc608\uc81c-4-\ucc44\uc6a9-\uacf5\uace0-\uc218\uc9d1",children:"\uc608\uc81c 4: \ucc44\uc6a9 \uacf5\uace0 \uc218\uc9d1"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"class JobScraper:\n    \"\"\"\ucc44\uc6a9 \uacf5\uace0 \uc2a4\ud06c\ub798\ud37c\"\"\"\n\n    def __init__(self):\n        self.session = requests.Session()\n        self.session.headers.update({\n            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'\n        })\n\n    def search_jobs(self, keyword, location=None, max_pages=3):\n        \"\"\"\ucc44\uc6a9 \uacf5\uace0 \uac80\uc0c9\"\"\"\n        all_jobs = []\n\n        for page in range(1, max_pages + 1):\n            print(f\"Scraping page {page}...\")\n\n            # \uac80\uc0c9 URL \uad6c\uc131\n            params = {'q': keyword, 'page': page}\n            if location:\n                params['location'] = location\n\n            url = 'https://jobs.example.com/search'\n\n            try:\n                response = self.session.get(url, params=params, timeout=10)\n                soup = BeautifulSoup(response.text, 'lxml')\n\n                # \ucc44\uc6a9 \uacf5\uace0 \ucd94\ucd9c\n                job_items = soup.select('.job-item, .job-card')\n\n                if not job_items:\n                    break  # \ub354 \uc774\uc0c1 \uacf5\uace0 \uc5c6\uc74c\n\n                for item in job_items:\n                    job = self._extract_job_info(item)\n                    if job:\n                        all_jobs.append(job)\n\n                time.sleep(1)\n\n            except Exception as e:\n                print(f\"Error on page {page}: {e}\")\n                continue\n\n        return all_jobs\n\n    def _extract_job_info(self, item):\n        \"\"\"\ucc44\uc6a9 \uacf5\uace0 \uc815\ubcf4 \ucd94\ucd9c\"\"\"\n        try:\n            job = {\n                'title': self._safe_text(item, '.job-title, h2.title'),\n                'company': self._safe_text(item, '.company-name, .company'),\n                'location': self._safe_text(item, '.location, .job-location'),\n                'salary': self._safe_text(item, '.salary, .compensation'),\n                'experience': self._safe_text(item, '.experience, .career'),\n                'education': self._safe_text(item, '.education, .degree'),\n                'employment_type': self._safe_text(item, '.employment-type, .job-type'),\n                'posted_date': self._safe_text(item, '.posted-date, .date'),\n                'deadline': self._safe_text(item, '.deadline, .closing-date'),\n                'link': self._safe_attr(item, 'a.job-link, h2 a', 'href'),\n                'tags': self._extract_tags(item),\n                'scraped_at': datetime.now().isoformat()\n            }\n            return job\n\n        except Exception as e:\n            print(f\"Error extracting job info: {e}\")\n            return None\n\n    def _safe_text(self, parent, selector):\n        \"\"\"\uc548\uc804\ud558\uac8c \ud14d\uc2a4\ud2b8 \ucd94\ucd9c\"\"\"\n        elem = parent.select_one(selector)\n        return elem.text.strip() if elem else None\n\n    def _safe_attr(self, parent, selector, attr):\n        \"\"\"\uc548\uc804\ud558\uac8c \uc18d\uc131 \ucd94\ucd9c\"\"\"\n        elem = parent.select_one(selector)\n        return elem.get(attr) if elem else None\n\n    def _extract_tags(self, item):\n        \"\"\"\ud0dc\uadf8 \ucd94\ucd9c (\uae30\uc220 \uc2a4\ud0dd, \ubcf5\uc9c0 \ub4f1)\"\"\"\n        tags = []\n        tag_elems = item.select('.tag, .skill, .tech-stack span')\n        for tag in tag_elems:\n            tags.append(tag.text.strip())\n        return tags\n\n    def get_job_details(self, job_url):\n        \"\"\"\uc0c1\uc138 \ucc44\uc6a9 \uc815\ubcf4 \uac00\uc838\uc624\uae30\"\"\"\n        try:\n            response = self.session.get(job_url, timeout=10)\n            soup = BeautifulSoup(response.text, 'lxml')\n\n            details = {\n                'description': self._extract_description(soup),\n                'requirements': self._extract_requirements(soup),\n                'benefits': self._extract_benefits(soup),\n                'application_process': self._extract_process(soup)\n            }\n\n            return details\n\n        except Exception as e:\n            print(f\"Error getting job details: {e}\")\n            return None\n\n    def _extract_description(self, soup):\n        \"\"\"\uc9c1\ubb34 \uc124\uba85 \ucd94\ucd9c\"\"\"\n        desc_elem = soup.select_one('.job-description, .description')\n        if desc_elem:\n            paragraphs = [p.text.strip() for p in desc_elem.find_all('p')]\n            return '\\n\\n'.join(paragraphs)\n        return None\n\n    def _extract_requirements(self, soup):\n        \"\"\"\uc790\uaca9 \uc694\uac74 \ucd94\ucd9c\"\"\"\n        req_section = soup.select_one('.requirements, .qualifications')\n        if req_section:\n            items = [li.text.strip() for li in req_section.find_all('li')]\n            return items\n        return []\n\n    def _extract_benefits(self, soup):\n        \"\"\"\ubcf5\uc9c0 \ud61c\ud0dd \ucd94\ucd9c\"\"\"\n        benefits_section = soup.select_one('.benefits, .perks')\n        if benefits_section:\n            items = [li.text.strip() for li in benefits_section.find_all('li')]\n            return items\n        return []\n\n    def _extract_process(self, soup):\n        \"\"\"\ucc44\uc6a9 \uc808\ucc28 \ucd94\ucd9c\"\"\"\n        process_section = soup.select_one('.process, .application-process')\n        if process_section:\n            steps = [li.text.strip() for li in process_section.find_all('li')]\n            return steps\n        return []\n\n# \uc0ac\uc6a9 \uc608\uc2dc\njob_scraper = JobScraper()\n\n# Python \uac1c\ubc1c\uc790 \ucc44\uc6a9 \uacf5\uace0 \uac80\uc0c9\njobs = job_scraper.search_jobs('Python', location='\uc11c\uc6b8', max_pages=3)\n\nprint(f\"Found {len(jobs)} jobs\\n\")\n\nfor job in jobs[:5]:  # \uc0c1\uc704 5\uac1c\ub9cc \ucd9c\ub825\n    print(f\"\uc81c\ubaa9: {job['title']}\")\n    print(f\"\ud68c\uc0ac: {job['company']}\")\n    print(f\"\uc704\uce58: {job['location']}\")\n    print(f\"\uae09\uc5ec: {job['salary']}\")\n    print(f\"\uacbd\ub825: {job['experience']}\")\n    print(f\"\ud0dc\uadf8: {', '.join(job['tags'])}\")\n    print(f\"\ub9c1\ud06c: {job['link']}\")\n    print(\"-\" * 80)\n"})}),"\n",(0,r.jsx)(n.h2,{id:"111-\ub370\uc774\ud130-\uc800\uc7a5",children:"1.11 \ub370\uc774\ud130 \uc800\uc7a5"}),"\n",(0,r.jsx)(n.p,{children:"\uc2a4\ud06c\ub798\ud551\ud55c \ub370\uc774\ud130\ub97c \ub2e4\uc591\ud55c \ud615\uc2dd\uc73c\ub85c \uc800\uc7a5\ud558\ub294 \ubc29\ubc95\uc785\ub2c8\ub2e4."}),"\n",(0,r.jsx)(n.h3,{id:"csv-\ud30c\uc77c\ub85c-\uc800\uc7a5",children:"CSV \ud30c\uc77c\ub85c \uc800\uc7a5"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"import csv\n\ndef save_to_csv(data, filename):\n    \"\"\"\ub9ac\uc2a4\ud2b8\uc758 \ub515\uc154\ub108\ub9ac\ub97c CSV\ub85c \uc800\uc7a5\"\"\"\n    if not data:\n        print(\"No data to save\")\n        return\n\n    # \ud544\ub4dc\uba85\uc740 \uccab \ubc88\uc9f8 \ud56d\ubaa9\uc758 \ud0a4\ub85c\n    fieldnames = data[0].keys()\n\n    with open(filename, 'w', newline='', encoding='utf-8-sig') as f:\n        writer = csv.DictWriter(f, fieldnames=fieldnames)\n        writer.writeheader()\n        writer.writerows(data)\n\n    print(f\"Saved {len(data)} items to {filename}\")\n\n# \uc0ac\uc6a9 \uc608\uc2dc\nproducts = [\n    {'name': '\uc81c\ud488 A', 'price': 10000, 'stock': 50},\n    {'name': '\uc81c\ud488 B', 'price': 20000, 'stock': 30},\n]\n\nsave_to_csv(products, 'products.csv')\n"})}),"\n",(0,r.jsx)(n.h3,{id:"json-\ud30c\uc77c\ub85c-\uc800\uc7a5",children:"JSON \ud30c\uc77c\ub85c \uc800\uc7a5"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'import json\n\ndef save_to_json(data, filename, indent=2):\n    """\ub370\uc774\ud130\ub97c JSON \ud30c\uc77c\ub85c \uc800\uc7a5"""\n    with open(filename, \'w\', encoding=\'utf-8\') as f:\n        json.dump(data, f, ensure_ascii=False, indent=indent)\n\n    print(f"Saved data to {filename}")\n\ndef load_from_json(filename):\n    """JSON \ud30c\uc77c\uc5d0\uc11c \ub370\uc774\ud130 \ub85c\ub4dc"""\n    with open(filename, \'r\', encoding=\'utf-8\') as f:\n        return json.load(f)\n\n# \uc0ac\uc6a9 \uc608\uc2dc\nsave_to_json(products, \'products.json\')\nloaded_data = load_from_json(\'products.json\')\n'})}),"\n",(0,r.jsx)(n.h3,{id:"sqlite-\ub370\uc774\ud130\ubca0\uc774\uc2a4\uc5d0-\uc800\uc7a5",children:"SQLite \ub370\uc774\ud130\ubca0\uc774\uc2a4\uc5d0 \uc800\uc7a5"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"import sqlite3\nfrom datetime import datetime\n\nclass DatabaseStorage:\n    \"\"\"SQLite \ub370\uc774\ud130\ubca0\uc774\uc2a4 \uc800\uc7a5\uc18c\"\"\"\n\n    def __init__(self, db_file='scraper.db'):\n        self.db_file = db_file\n        self.conn = sqlite3.connect(db_file)\n        self.create_tables()\n\n    def create_tables(self):\n        \"\"\"\ud14c\uc774\ube14 \uc0dd\uc131\"\"\"\n        cursor = self.conn.cursor()\n\n        # \uc81c\ud488 \ud14c\uc774\ube14\n        cursor.execute('''\n            CREATE TABLE IF NOT EXISTS products (\n                id INTEGER PRIMARY KEY AUTOINCREMENT,\n                title TEXT NOT NULL,\n                price REAL,\n                url TEXT UNIQUE,\n                description TEXT,\n                image_url TEXT,\n                category TEXT,\n                in_stock BOOLEAN,\n                scraped_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n            )\n        ''')\n\n        # \uac00\uaca9 \uc774\ub825 \ud14c\uc774\ube14\n        cursor.execute('''\n            CREATE TABLE IF NOT EXISTS price_history (\n                id INTEGER PRIMARY KEY AUTOINCREMENT,\n                product_id INTEGER,\n                price REAL,\n                recorded_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n                FOREIGN KEY (product_id) REFERENCES products (id)\n            )\n        ''')\n\n        self.conn.commit()\n\n    def save_product(self, product):\n        \"\"\"\uc81c\ud488 \uc800\uc7a5 \ub610\ub294 \uc5c5\ub370\uc774\ud2b8\"\"\"\n        cursor = self.conn.cursor()\n\n        # \uae30\uc874 \uc81c\ud488 \ud655\uc778\n        cursor.execute('SELECT id, price FROM products WHERE url = ?',\n                      (product['url'],))\n        existing = cursor.fetchone()\n\n        if existing:\n            # \uc5c5\ub370\uc774\ud2b8\n            product_id, old_price = existing\n            cursor.execute('''\n                UPDATE products\n                SET title=?, price=?, description=?, image_url=?,\n                    category=?, in_stock=?, scraped_at=?\n                WHERE id=?\n            ''', (\n                product['title'], product['price'], product.get('description'),\n                product.get('image_url'), product.get('category'),\n                product.get('in_stock', True), datetime.now(), product_id\n            ))\n\n            # \uac00\uaca9\uc774 \ubcc0\uacbd\ub418\uc5c8\uc73c\uba74 \uc774\ub825 \uc800\uc7a5\n            if old_price != product['price']:\n                cursor.execute('''\n                    INSERT INTO price_history (product_id, price)\n                    VALUES (?, ?)\n                ''', (product_id, product['price']))\n\n        else:\n            # \uc0c8\ub85c \uc0bd\uc785\n            cursor.execute('''\n                INSERT INTO products\n                (title, price, url, description, image_url, category, in_stock)\n                VALUES (?, ?, ?, ?, ?, ?, ?)\n            ''', (\n                product['title'], product['price'], product['url'],\n                product.get('description'), product.get('image_url'),\n                product.get('category'), product.get('in_stock', True)\n            ))\n\n            product_id = cursor.lastrowid\n\n            # \ucd08\uae30 \uac00\uaca9 \uc774\ub825\n            cursor.execute('''\n                INSERT INTO price_history (product_id, price)\n                VALUES (?, ?)\n            ''', (product_id, product['price']))\n\n        self.conn.commit()\n        return product_id\n\n    def save_products(self, products):\n        \"\"\"\uc5ec\ub7ec \uc81c\ud488 \uc800\uc7a5\"\"\"\n        for product in products:\n            self.save_product(product)\n        print(f\"Saved {len(products)} products to database\")\n\n    def get_products(self, category=None):\n        \"\"\"\uc81c\ud488 \uc870\ud68c\"\"\"\n        cursor = self.conn.cursor()\n\n        if category:\n            cursor.execute('''\n                SELECT * FROM products WHERE category = ?\n                ORDER BY scraped_at DESC\n            ''', (category,))\n        else:\n            cursor.execute('SELECT * FROM products ORDER BY scraped_at DESC')\n\n        columns = [desc[0] for desc in cursor.description]\n        results = cursor.fetchall()\n\n        return [dict(zip(columns, row)) for row in results]\n\n    def get_price_history(self, product_id):\n        \"\"\"\uc81c\ud488 \uac00\uaca9 \uc774\ub825 \uc870\ud68c\"\"\"\n        cursor = self.conn.cursor()\n        cursor.execute('''\n            SELECT price, recorded_at FROM price_history\n            WHERE product_id = ?\n            ORDER BY recorded_at DESC\n        ''', (product_id,))\n\n        return cursor.fetchall()\n\n    def close(self):\n        \"\"\"\uc5f0\uacb0 \uc885\ub8cc\"\"\"\n        self.conn.close()\n\n# \uc0ac\uc6a9 \uc608\uc2dc\ndb = DatabaseStorage('products.db')\n\n# \uc81c\ud488 \uc800\uc7a5\nproducts = [\n    {\n        'title': '\ub178\ud2b8\ubd81 A',\n        'price': 1200000,\n        'url': 'https://shop.example.com/laptop-a',\n        'category': '\uc804\uc790\uc81c\ud488',\n        'in_stock': True\n    },\n    {\n        'title': '\ub9c8\uc6b0\uc2a4 B',\n        'price': 35000,\n        'url': 'https://shop.example.com/mouse-b',\n        'category': '\uc804\uc790\uc81c\ud488',\n        'in_stock': True\n    }\n]\n\ndb.save_products(products)\n\n# \uc81c\ud488 \uc870\ud68c\nall_products = db.get_products()\nfor p in all_products:\n    print(f\"{p['title']}: {p['price']}\uc6d0\")\n\ndb.close()\n"})}),"\n",(0,r.jsx)(n.h3,{id:"pandas\ub85c-\ub370\uc774\ud130-\ucc98\ub9ac-\ubc0f-\uc800\uc7a5",children:"Pandas\ub85c \ub370\uc774\ud130 \ucc98\ub9ac \ubc0f \uc800\uc7a5"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"import pandas as pd\n\ndef save_with_pandas(data, filename):\n    \"\"\"Pandas\ub97c \uc0ac\uc6a9\ud55c \ub370\uc774\ud130 \uc800\uc7a5\"\"\"\n    df = pd.DataFrame(data)\n\n    # CSV\ub85c \uc800\uc7a5\n    df.to_csv(f'{filename}.csv', index=False, encoding='utf-8-sig')\n\n    # Excel\ub85c \uc800\uc7a5\n    df.to_excel(f'{filename}.xlsx', index=False, engine='openpyxl')\n\n    # JSON\uc73c\ub85c \uc800\uc7a5\n    df.to_json(f'{filename}.json', orient='records', force_ascii=False, indent=2)\n\n    print(f\"Saved {len(df)} records\")\n\n    # \uae30\ubcf8 \ud1b5\uacc4\n    print(\"\\n\uae30\ubcf8 \ud1b5\uacc4:\")\n    print(df.describe())\n\n    return df\n\n# \uc0ac\uc6a9 \uc608\uc2dc\nproducts = [\n    {'name': '\uc81c\ud488 A', 'price': 10000, 'rating': 4.5, 'reviews': 120},\n    {'name': '\uc81c\ud488 B', 'price': 20000, 'rating': 4.2, 'reviews': 85},\n    {'name': '\uc81c\ud488 C', 'price': 15000, 'rating': 4.8, 'reviews': 200},\n]\n\ndf = save_with_pandas(products, 'products_analysis')\n\n# \ub370\uc774\ud130 \ubd84\uc11d\nprint(f\"\\n\ud3c9\uade0 \uac00\uaca9: {df['price'].mean():,.0f}\uc6d0\")\nprint(f\"\ucd5c\uace0 \ud3c9\uc810: {df['rating'].max()}\")\nprint(f\"\ucd1d \ub9ac\ubdf0 \uc218: {df['reviews'].sum()}\")\n"})}),"\n",(0,r.jsx)(n.h2,{id:"112-\ubaa8\ubc94-\uc0ac\ub840",children:"1.12 \ubaa8\ubc94 \uc0ac\ub840"}),"\n",(0,r.jsx)(n.h3,{id:"user-agent-\ub85c\ud14c\uc774\uc158",children:"User-Agent \ub85c\ud14c\uc774\uc158"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"import random\n\nclass UserAgentRotator:\n    \"\"\"User-Agent \ub85c\ud14c\uc774\ud130\"\"\"\n\n    USER_AGENTS = [\n        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',\n        'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',\n        'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:121.0) Gecko/20100101 Firefox/121.0',\n        'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.1 Safari/605.1.15',\n        'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'\n    ]\n\n    @classmethod\n    def get_random(cls):\n        \"\"\"\ub79c\ub364 User-Agent \ubc18\ud658\"\"\"\n        return random.choice(cls.USER_AGENTS)\n\n    @classmethod\n    def get_headers(cls):\n        \"\"\"\ub79c\ub364 User-Agent\uac00 \ud3ec\ud568\ub41c \ud5e4\ub354 \ubc18\ud658\"\"\"\n        return {\n            'User-Agent': cls.get_random(),\n            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n            'Accept-Language': 'ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7',\n            'Accept-Encoding': 'gzip, deflate, br',\n            'DNT': '1',\n            'Connection': 'keep-alive',\n            'Upgrade-Insecure-Requests': '1'\n        }\n\n# \uc0ac\uc6a9\nresponse = requests.get(url, headers=UserAgentRotator.get_headers())\n"})}),"\n",(0,r.jsx)(n.h3,{id:"\ud504\ub85d\uc2dc-\uc0ac\uc6a9",children:"\ud504\ub85d\uc2dc \uc0ac\uc6a9"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'class ProxyRotator:\n    """\ud504\ub85d\uc2dc \ub85c\ud14c\uc774\ud130"""\n\n    def __init__(self, proxy_list):\n        """\n        proxy_list: [\'http://ip:port\', \'http://ip:port\', ...]\n        """\n        self.proxies = proxy_list\n        self.current_index = 0\n\n    def get_next(self):\n        """\ub2e4\uc74c \ud504\ub85d\uc2dc \ubc18\ud658"""\n        proxy = self.proxies[self.current_index]\n        self.current_index = (self.current_index + 1) % len(self.proxies)\n        return {\'http\': proxy, \'https\': proxy}\n\n    def get_random(self):\n        """\ub79c\ub364 \ud504\ub85d\uc2dc \ubc18\ud658"""\n        proxy = random.choice(self.proxies)\n        return {\'http\': proxy, \'https\': proxy}\n\n# \uc0ac\uc6a9\nproxies = [\n    \'http://proxy1.example.com:8080\',\n    \'http://proxy2.example.com:8080\',\n]\n\nproxy_rotator = ProxyRotator(proxies)\nresponse = requests.get(url, proxies=proxy_rotator.get_next())\n'})}),"\n",(0,r.jsx)(n.h3,{id:"\uce90\uc2f1-\uad6c\ud604",children:"\uce90\uc2f1 \uad6c\ud604"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'import pickle\nimport hashlib\nfrom pathlib import Path\nfrom datetime import datetime, timedelta\n\nclass ResponseCache:\n    """\uc751\ub2f5 \uce90\uc2dc"""\n\n    def __init__(self, cache_dir=\'cache\', expiry_hours=24):\n        self.cache_dir = Path(cache_dir)\n        self.cache_dir.mkdir(exist_ok=True)\n        self.expiry_hours = expiry_hours\n\n    def _get_cache_path(self, url):\n        """URL\uc5d0 \ub300\ud55c \uce90\uc2dc \ud30c\uc77c \uacbd\ub85c"""\n        url_hash = hashlib.md5(url.encode()).hexdigest()\n        return self.cache_dir / f"{url_hash}.pkl"\n\n    def get(self, url):\n        """\uce90\uc2dc\uc5d0\uc11c \uc751\ub2f5 \uac00\uc838\uc624\uae30"""\n        cache_path = self._get_cache_path(url)\n\n        if not cache_path.exists():\n            return None\n\n        # \uce90\uc2dc \ub9cc\ub8cc \ud655\uc778\n        cache_time = datetime.fromtimestamp(cache_path.stat().st_mtime)\n        if datetime.now() - cache_time > timedelta(hours=self.expiry_hours):\n            cache_path.unlink()  # \ub9cc\ub8cc\ub41c \uce90\uc2dc \uc0ad\uc81c\n            return None\n\n        # \uce90\uc2dc \ub85c\ub4dc\n        with open(cache_path, \'rb\') as f:\n            return pickle.load(f)\n\n    def set(self, url, response_text):\n        """\uc751\ub2f5 \uce90\uc2dc \uc800\uc7a5"""\n        cache_path = self._get_cache_path(url)\n        with open(cache_path, \'wb\') as f:\n            pickle.dump(response_text, f)\n\n    def clear(self):\n        """\ubaa8\ub4e0 \uce90\uc2dc \uc0ad\uc81c"""\n        for cache_file in self.cache_dir.glob(\'*.pkl\'):\n            cache_file.unlink()\n\n# \uc0ac\uc6a9\ncache = ResponseCache(expiry_hours=24)\n\ndef fetch_with_cache(url):\n    """\uce90\uc2dc\ub97c \uc0ac\uc6a9\ud55c \uc694\uccad"""\n    # \uce90\uc2dc \ud655\uc778\n    cached = cache.get(url)\n    if cached:\n        print(f"Using cached response for {url}")\n        return cached\n\n    # \uc0c8\ub85c \uc694\uccad\n    print(f"Fetching {url}")\n    response = requests.get(url)\n\n    # \uce90\uc2dc \uc800\uc7a5\n    cache.set(url, response.text)\n\n    return response.text\n'})}),"\n",(0,r.jsx)(n.h3,{id:"\uacac\uace0\ud55c-\uc2a4\ud06c\ub798\ud37c-\ud15c\ud50c\ub9bf",children:"\uacac\uace0\ud55c \uc2a4\ud06c\ub798\ud37c \ud15c\ud50c\ub9bf"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'import logging\nfrom typing import List, Dict, Optional\nimport time\nimport random\n\nclass RobustScraper:\n    """\uacac\uace0\ud55c \uc6f9 \uc2a4\ud06c\ub798\ud37c \ud15c\ud50c\ub9bf"""\n\n    def __init__(self,\n                 max_retries=3,\n                 delay_range=(1, 3),\n                 timeout=10,\n                 cache_expiry_hours=24):\n\n        # \ub85c\uae45 \uc124\uc815\n        self.logger = self._setup_logging()\n\n        # \uc138\uc158 \uc124\uc815\n        self.session = self._create_session(max_retries)\n\n        # \uc124\uc815\n        self.delay_range = delay_range\n        self.timeout = timeout\n\n        # \uce90\uc2dc\n        self.cache = ResponseCache(expiry_hours=cache_expiry_hours)\n\n        # User-Agent \ub85c\ud14c\uc774\ud130\n        self.ua_rotator = UserAgentRotator()\n\n        # \ud1b5\uacc4\n        self.stats = {\n            \'requests\': 0,\n            \'successes\': 0,\n            \'failures\': 0,\n            \'cache_hits\': 0\n        }\n\n    def _setup_logging(self):\n        """\ub85c\uae45 \uc124\uc815"""\n        logger = logging.getLogger(self.__class__.__name__)\n        logger.setLevel(logging.INFO)\n\n        # \ud30c\uc77c \ud578\ub4e4\ub7ec\n        fh = logging.FileHandler(f\'{self.__class__.__name__}.log\')\n        fh.setLevel(logging.INFO)\n\n        # \ucf58\uc194 \ud578\ub4e4\ub7ec\n        ch = logging.StreamHandler()\n        ch.setLevel(logging.INFO)\n\n        # \ud3ec\ub9f7\ud130\n        formatter = logging.Formatter(\n            \'%(asctime)s - %(name)s - %(levelname)s - %(message)s\'\n        )\n        fh.setFormatter(formatter)\n        ch.setFormatter(formatter)\n\n        logger.addHandler(fh)\n        logger.addHandler(ch)\n\n        return logger\n\n    def _create_session(self, max_retries):\n        """\uc7ac\uc2dc\ub3c4 \uae30\ub2a5\uc774 \uc788\ub294 \uc138\uc158 \uc0dd\uc131"""\n        session = requests.Session()\n\n        retry_strategy = Retry(\n            total=max_retries,\n            backoff_factor=1,\n            status_forcelist=[429, 500, 502, 503, 504]\n        )\n\n        adapter = HTTPAdapter(max_retries=retry_strategy)\n        session.mount("http://", adapter)\n        session.mount("https://", adapter)\n\n        return session\n\n    def fetch(self, url: str, use_cache=True) -> Optional[str]:\n        """URL\uc5d0\uc11c HTML \uac00\uc838\uc624\uae30"""\n        self.stats[\'requests\'] += 1\n\n        # \uce90\uc2dc \ud655\uc778\n        if use_cache:\n            cached = self.cache.get(url)\n            if cached:\n                self.logger.info(f"Cache hit: {url}")\n                self.stats[\'cache_hits\'] += 1\n                return cached\n\n        try:\n            # \ud5e4\ub354 \uc124\uc815\n            headers = self.ua_rotator.get_headers()\n\n            # \uc694\uccad\n            self.logger.info(f"Fetching: {url}")\n            response = self.session.get(\n                url,\n                headers=headers,\n                timeout=self.timeout\n            )\n            response.raise_for_status()\n\n            # \uce90\uc2dc \uc800\uc7a5\n            if use_cache:\n                self.cache.set(url, response.text)\n\n            self.stats[\'successes\'] += 1\n\n            # \uc608\uc758 \ubc14\ub978 \uc9c0\uc5f0\n            self._polite_delay()\n\n            return response.text\n\n        except Exception as e:\n            self.logger.error(f"Failed to fetch {url}: {e}")\n            self.stats[\'failures\'] += 1\n            return None\n\n    def _polite_delay(self):\n        """\ub79c\ub364 \uc9c0\uc5f0"""\n        delay = random.uniform(*self.delay_range)\n        time.sleep(delay)\n\n    def parse(self, html: str) -> Optional[BeautifulSoup]:\n        """HTML \ud30c\uc2f1"""\n        try:\n            return BeautifulSoup(html, \'lxml\')\n        except Exception as e:\n            self.logger.error(f"Failed to parse HTML: {e}")\n            return None\n\n    def extract(self, soup: BeautifulSoup) -> Dict:\n        """\ub370\uc774\ud130 \ucd94\ucd9c (\uc11c\ube0c\ud074\ub798\uc2a4\uc5d0\uc11c \uad6c\ud604)"""\n        raise NotImplementedError("Subclass must implement extract()")\n\n    def scrape(self, url: str) -> Optional[Dict]:\n        """\uc804\uccb4 \uc2a4\ud06c\ub798\ud551 \ud504\ub85c\uc138\uc2a4"""\n        html = self.fetch(url)\n        if not html:\n            return None\n\n        soup = self.parse(html)\n        if not soup:\n            return None\n\n        data = self.extract(soup)\n        return data\n\n    def scrape_multiple(self, urls: List[str]) -> List[Dict]:\n        """\uc5ec\ub7ec URL \uc2a4\ud06c\ub798\ud551"""\n        results = []\n\n        for i, url in enumerate(urls, 1):\n            self.logger.info(f"Scraping {i}/{len(urls)}: {url}")\n\n            data = self.scrape(url)\n            if data:\n                results.append(data)\n\n        return results\n\n    def print_stats(self):\n        """\ud1b5\uacc4 \ucd9c\ub825"""\n        self.logger.info("=== Scraping Statistics ===")\n        self.logger.info(f"Total requests: {self.stats[\'requests\']}")\n        self.logger.info(f"Successes: {self.stats[\'successes\']}")\n        self.logger.info(f"Failures: {self.stats[\'failures\']}")\n        self.logger.info(f"Cache hits: {self.stats[\'cache_hits\']}")\n\n        if self.stats[\'requests\'] > 0:\n            success_rate = (self.stats[\'successes\'] / self.stats[\'requests\']) * 100\n            self.logger.info(f"Success rate: {success_rate:.1f}%")\n\n# \uc11c\ube0c\ud074\ub798\uc2a4 \uc608\uc2dc\nclass ProductScraper(RobustScraper):\n    """\uc81c\ud488 \uc2a4\ud06c\ub798\ud37c \uad6c\ud604"""\n\n    def extract(self, soup: BeautifulSoup) -> Dict:\n        """\uc81c\ud488 \uc815\ubcf4 \ucd94\ucd9c"""\n        return {\n            \'title\': self._safe_text(soup, \'h1.product-title\'),\n            \'price\': self._safe_text(soup, \'span.price\'),\n            \'description\': self._safe_text(soup, \'div.description\'),\n            \'rating\': self._safe_text(soup, \'span.rating\'),\n        }\n\n    def _safe_text(self, soup, selector):\n        """\uc548\uc804\ud558\uac8c \ud14d\uc2a4\ud2b8 \ucd94\ucd9c"""\n        elem = soup.select_one(selector)\n        return elem.text.strip() if elem else None\n\n# \uc0ac\uc6a9\nscraper = ProductScraper()\nurls = [\'https://shop.example.com/product/1\', \'https://shop.example.com/product/2\']\nresults = scraper.scrape_multiple(urls)\nscraper.print_stats()\n'})}),"\n",(0,r.jsx)(n.h2,{id:"113-\uccb4\ud06c\ub9ac\uc2a4\ud2b8-\ubc0f-\ud301",children:"1.13 \uccb4\ud06c\ub9ac\uc2a4\ud2b8 \ubc0f \ud301"}),"\n",(0,r.jsx)(n.h3,{id:"\uc2a4\ud06c\ub798\ud551-\uc2dc\uc791-\uc804-\uccb4\ud06c\ub9ac\uc2a4\ud2b8",children:"\uc2a4\ud06c\ub798\ud551 \uc2dc\uc791 \uc804 \uccb4\ud06c\ub9ac\uc2a4\ud2b8"}),"\n",(0,r.jsx)(n.admonition,{title:"\uc2dc\uc791 \uc804 \ud655\uc778\uc0ac\ud56d",type:"tip",children:(0,r.jsxs)(n.ul,{className:"contains-task-list",children:["\n",(0,r.jsxs)(n.li,{className:"task-list-item",children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.input,{type:"checkbox",disabled:!0})," ",(0,r.jsx)(n.strong,{children:"\ubc95\uc801 \uac80\ud1a0"})]}),"\n",(0,r.jsxs)(n.ul,{className:"contains-task-list",children:["\n",(0,r.jsxs)(n.li,{className:"task-list-item",children:[(0,r.jsx)(n.input,{type:"checkbox",disabled:!0})," ","\uc6f9\uc0ac\uc774\ud2b8 \uc774\uc6a9\uc57d\uad00 \ud655\uc778"]}),"\n",(0,r.jsxs)(n.li,{className:"task-list-item",children:[(0,r.jsx)(n.input,{type:"checkbox",disabled:!0})," ","robots.txt \ud655\uc778"]}),"\n",(0,r.jsxs)(n.li,{className:"task-list-item",children:[(0,r.jsx)(n.input,{type:"checkbox",disabled:!0})," ","\uac1c\uc778\uc815\ubcf4 \uc218\uc9d1 \uc5ec\ubd80 \ud655\uc778"]}),"\n",(0,r.jsxs)(n.li,{className:"task-list-item",children:[(0,r.jsx)(n.input,{type:"checkbox",disabled:!0})," ","\uc800\uc791\uad8c \ubb38\uc81c \uc5c6\ub294\uc9c0 \ud655\uc778"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{className:"task-list-item",children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.input,{type:"checkbox",disabled:!0})," ",(0,r.jsx)(n.strong,{children:"\uae30\uc220\uc801 \uc900\ube44"})]}),"\n",(0,r.jsxs)(n.ul,{className:"contains-task-list",children:["\n",(0,r.jsxs)(n.li,{className:"task-list-item",children:[(0,r.jsx)(n.input,{type:"checkbox",disabled:!0})," ","\uc6f9\uc0ac\uc774\ud2b8 \uad6c\uc870 \ubd84\uc11d (\uac1c\ubc1c\uc790 \ub3c4\uad6c \ud65c\uc6a9)"]}),"\n",(0,r.jsxs)(n.li,{className:"task-list-item",children:[(0,r.jsx)(n.input,{type:"checkbox",disabled:!0})," ","\ud544\uc694\ud55c \ub370\uc774\ud130 \uc120\ud0dd\uc790 \ud30c\uc545"]}),"\n",(0,r.jsxs)(n.li,{className:"task-list-item",children:[(0,r.jsx)(n.input,{type:"checkbox",disabled:!0})," ","\ub3d9\uc801 \ucf58\ud150\uce20 \uc5ec\ubd80 \ud655\uc778 (Selenium \ud544\uc694 \uc5ec\ubd80)"]}),"\n",(0,r.jsxs)(n.li,{className:"task-list-item",children:[(0,r.jsx)(n.input,{type:"checkbox",disabled:!0})," ","\ud398\uc774\uc9c0\ub124\uc774\uc158 \ubc29\uc2dd \ud655\uc778"]}),"\n",(0,r.jsxs)(n.li,{className:"task-list-item",children:[(0,r.jsx)(n.input,{type:"checkbox",disabled:!0})," ","API \uc874\uc7ac \uc5ec\ubd80 \ud655\uc778 (API\uac00 \uc788\uc73c\uba74 \uc2a4\ud06c\ub798\ud551\ubcf4\ub2e4 \uc6b0\uc120)"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{className:"task-list-item",children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.input,{type:"checkbox",disabled:!0})," ",(0,r.jsx)(n.strong,{children:"\uc124\uacc4"})]}),"\n",(0,r.jsxs)(n.ul,{className:"contains-task-list",children:["\n",(0,r.jsxs)(n.li,{className:"task-list-item",children:[(0,r.jsx)(n.input,{type:"checkbox",disabled:!0})," ","\ub370\uc774\ud130 \uc2a4\ud0a4\ub9c8 \uc815\uc758"]}),"\n",(0,r.jsxs)(n.li,{className:"task-list-item",children:[(0,r.jsx)(n.input,{type:"checkbox",disabled:!0})," ","\uc800\uc7a5 \ubc29\uc2dd \uacb0\uc815 (CSV, JSON, DB)"]}),"\n",(0,r.jsxs)(n.li,{className:"task-list-item",children:[(0,r.jsx)(n.input,{type:"checkbox",disabled:!0})," ","\uc624\ub958 \ucc98\ub9ac \uc804\ub7b5 \uc218\ub9bd"]}),"\n",(0,r.jsxs)(n.li,{className:"task-list-item",children:[(0,r.jsx)(n.input,{type:"checkbox",disabled:!0})," ","\uc2a4\ucf00\uc904\ub9c1 \ud544\uc694 \uc5ec\ubd80 \uacb0\uc815"]}),"\n"]}),"\n"]}),"\n"]})}),"\n",(0,r.jsx)(n.h3,{id:"\uc77c\ubc18\uc801\uc778-\ubb38\uc81c-\ud574\uacb0",children:"\uc77c\ubc18\uc801\uc778 \ubb38\uc81c \ud574\uacb0"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"# \ubb38\uc81c 1: \uc694\uc18c\ub97c \ucc3e\uc744 \uc218 \uc5c6\uc74c\n# \ud574\uacb0: \uc5ec\ub7ec \uc120\ud0dd\uc790 \uc2dc\ub3c4, \ub3d9\uc801 \ucf58\ud150\uce20 \ud655\uc778\n\ndef find_with_fallback(soup, selectors):\n    \"\"\"\uc5ec\ub7ec \uc120\ud0dd\uc790\ub85c \uc2dc\ub3c4\"\"\"\n    for selector in selectors:\n        elem = soup.select_one(selector)\n        if elem:\n            return elem\n    return None\n\n# \uc0ac\uc6a9\ntitle = find_with_fallback(soup, [\n    'h1.title',\n    'h1.post-title',\n    'div.header h1',\n    'h1'\n])\n\n# \ubb38\uc81c 2: \uc778\ucf54\ub529 \uc624\ub958\n# \ud574\uacb0: \uc62c\ubc14\ub978 \uc778\ucf54\ub529 \uc9c0\uc815\n\nresponse = requests.get(url)\nresponse.encoding = 'utf-8'  # \ub610\ub294 'euc-kr', 'cp949'\nhtml = response.text\n\n# \ubb38\uc81c 3: \ub3d9\uc801 \ucf58\ud150\uce20\uac00 \ub85c\ub4dc\ub418\uc9c0 \uc54a\uc74c\n# \ud574\uacb0: Selenium \ub300\uae30 \uc0ac\uc6a9\n\nfrom selenium.webdriver.support.ui import WebDriverWait\nfrom selenium.webdriver.support import expected_conditions as EC\n\nwait = WebDriverWait(driver, 10)\nelement = wait.until(\n    EC.presence_of_element_located((By.CLASS_NAME, 'dynamic-content'))\n)\n\n# \ubb38\uc81c 4: \ucc28\ub2e8\ub2f9\ud568 (403, 429 \uc624\ub958)\n# \ud574\uacb0: User-Agent, \uc9c0\uc5f0 \uc2dc\uac04, \ud504\ub85d\uc2dc\n\nheaders = {\n    'User-Agent': 'Mozilla/5.0 ...',\n    'Referer': 'https://www.google.com/'\n}\ntime.sleep(random.uniform(2, 5))\n\n# \ubb38\uc81c 5: \uba54\ubaa8\ub9ac \ubd80\uc871 (\ub300\ub7c9 \ub370\uc774\ud130)\n# \ud574\uacb0: \ubc30\uce58 \ucc98\ub9ac, \uc81c\ub108\ub808\uc774\ud130 \uc0ac\uc6a9\n\ndef scrape_in_batches(urls, batch_size=100):\n    \"\"\"\ubc30\uce58\ub85c \uc2a4\ud06c\ub798\ud551\"\"\"\n    for i in range(0, len(urls), batch_size):\n        batch = urls[i:i+batch_size]\n        results = scrape_multiple(batch)\n        save_to_db(results)  # \uc989\uc2dc \uc800\uc7a5\n        time.sleep(5)  # \ubc30\uce58 \uac04 \ud734\uc2dd\n"})}),"\n",(0,r.jsx)(n.h3,{id:"\ub514\ubc84\uae45-\ud301",children:"\ub514\ubc84\uae45 \ud301"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'# HTML \uc800\uc7a5\ud558\uc5ec \uc624\ud504\ub77c\uc778 \ubd84\uc11d\ndef save_html_for_debug(url, filename=\'debug.html\'):\n    """HTML\uc744 \ud30c\uc77c\ub85c \uc800\uc7a5"""\n    response = requests.get(url)\n    with open(filename, \'w\', encoding=\'utf-8\') as f:\n        f.write(response.text)\n    print(f"HTML saved to {filename}")\n\n# BeautifulSoup prettify\ub85c \uad6c\uc870 \ud655\uc778\nsoup = BeautifulSoup(html, \'lxml\')\nprint(soup.prettify()[:1000])  # \ucc98\uc74c 1000\uc790\n\n# \uc120\ud0dd\uc790 \ud14c\uc2a4\ud2b8\ndef test_selector(soup, selector):\n    """\uc120\ud0dd\uc790\uac00 \uc81c\ub300\ub85c \uc791\ub3d9\ud558\ub294\uc9c0 \ud14c\uc2a4\ud2b8"""\n    elements = soup.select(selector)\n    print(f"Found {len(elements)} elements with selector: {selector}")\n    for i, elem in enumerate(elements[:3], 1):\n        print(f"  {i}. {elem.text.strip()[:50]}")\n\n# \uc694\uccad/\uc751\ub2f5 \ub85c\uae45\nimport http.client as http_client\nhttp_client.HTTPConnection.debuglevel = 1\n\nlogging.basicConfig()\nlogging.getLogger().setLevel(logging.DEBUG)\nrequests_log = logging.getLogger("requests.packages.urllib3")\nrequests_log.setLevel(logging.DEBUG)\nrequests_log.propagate = True\n'})}),"\n",(0,r.jsx)(n.h2,{id:"\uc5f0\uc2b5-\ubb38\uc81c",children:"\uc5f0\uc2b5 \ubb38\uc81c"}),"\n",(0,r.jsx)(n.h3,{id:"\ubb38\uc81c-1-\ube14\ub85c\uadf8-\ud3ec\uc2a4\ud2b8-\uc218\uc9d1\uae30",children:"\ubb38\uc81c 1: \ube14\ub85c\uadf8 \ud3ec\uc2a4\ud2b8 \uc218\uc9d1\uae30"}),"\n",(0,r.jsx)(n.p,{children:"\ube14\ub85c\uadf8\uc758 \ucd5c\uadfc \uac8c\uc2dc\ubb3c 10\uac1c\ub97c \uc218\uc9d1\ud558\uc5ec CSV \ud30c\uc77c\ub85c \uc800\uc7a5\ud558\ub294 \uc2a4\ud06c\ub798\ud37c\ub97c \uc791\uc131\ud558\uc138\uc694."}),"\n",(0,r.jsx)(n.p,{children:"\ud544\uc694\ud55c \uc815\ubcf4:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"\uc81c\ubaa9"}),"\n",(0,r.jsx)(n.li,{children:"\uc791\uc131\uc790"}),"\n",(0,r.jsx)(n.li,{children:"\uc791\uc131\uc77c"}),"\n",(0,r.jsx)(n.li,{children:"\uce74\ud14c\uace0\ub9ac"}),"\n",(0,r.jsx)(n.li,{children:"\ubcf8\ubb38 \ubbf8\ub9ac\ubcf4\uae30 (\uccab 100\uc790)"}),"\n",(0,r.jsx)(n.li,{children:"URL"}),"\n"]}),"\n",(0,r.jsxs)(t,{children:[(0,r.jsx)("summary",{children:"\ud574\ub2f5 \ubcf4\uae30"}),(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"import requests\nfrom bs4 import BeautifulSoup\nimport csv\nfrom datetime import datetime\n\nclass BlogScraper:\n    def __init__(self, blog_url):\n        self.blog_url = blog_url\n        self.session = requests.Session()\n        self.session.headers.update({\n            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'\n        })\n\n    def scrape_posts(self, max_posts=10):\n        \"\"\"\ube14\ub85c\uadf8 \ud3ec\uc2a4\ud2b8 \uc218\uc9d1\"\"\"\n        posts = []\n\n        try:\n            response = self.session.get(self.blog_url, timeout=10)\n            soup = BeautifulSoup(response.text, 'lxml')\n\n            # \ud3ec\uc2a4\ud2b8 \ubaa9\ub85d \ucd94\ucd9c (\uc2e4\uc81c \ube14\ub85c\uadf8 \uad6c\uc870\uc5d0 \ub9de\uac8c \uc218\uc815 \ud544\uc694)\n            post_items = soup.select('.post-item')[:max_posts]\n\n            for item in post_items:\n                post = {\n                    'title': self._safe_text(item, '.post-title'),\n                    'author': self._safe_text(item, '.post-author'),\n                    'date': self._safe_text(item, '.post-date'),\n                    'category': self._safe_text(item, '.post-category'),\n                    'preview': self._get_preview(item),\n                    'url': self._safe_attr(item, 'a.post-link', 'href')\n                }\n                posts.append(post)\n\n        except Exception as e:\n            print(f\"Error: {e}\")\n\n        return posts\n\n    def _safe_text(self, parent, selector):\n        elem = parent.select_one(selector)\n        return elem.text.strip() if elem else ''\n\n    def _safe_attr(self, parent, selector, attr):\n        elem = parent.select_one(selector)\n        return elem.get(attr, '') if elem else ''\n\n    def _get_preview(self, item):\n        preview_elem = item.select_one('.post-preview, .post-excerpt')\n        if preview_elem:\n            text = preview_elem.text.strip()\n            return text[:100] + '...' if len(text) > 100 else text\n        return ''\n\n    def save_to_csv(self, posts, filename='blog_posts.csv'):\n        \"\"\"CSV\ub85c \uc800\uc7a5\"\"\"\n        if not posts:\n            print(\"No posts to save\")\n            return\n\n        with open(filename, 'w', newline='', encoding='utf-8-sig') as f:\n            fieldnames = ['title', 'author', 'date', 'category', 'preview', 'url']\n            writer = csv.DictWriter(f, fieldnames=fieldnames)\n            writer.writeheader()\n            writer.writerows(posts)\n\n        print(f\"Saved {len(posts)} posts to {filename}\")\n\n# \uc0ac\uc6a9\nscraper = BlogScraper('https://blog.example.com')\nposts = scraper.scrape_posts(max_posts=10)\nscraper.save_to_csv(posts)\n"})})]}),"\n",(0,r.jsx)(n.h3,{id:"\ubb38\uc81c-2-\ub3d9\uc801-\ud398\uc774\uc9c0-\uc2a4\ud06c\ub798\ud551",children:"\ubb38\uc81c 2: \ub3d9\uc801 \ud398\uc774\uc9c0 \uc2a4\ud06c\ub798\ud551"}),"\n",(0,r.jsx)(n.p,{children:"Selenium\uc744 \uc0ac\uc6a9\ud558\uc5ec \ubb34\ud55c \uc2a4\ud06c\ub864 \ud398\uc774\uc9c0\uc5d0\uc11c \uc774\ubbf8\uc9c0 URL\uc744 \uc218\uc9d1\ud558\uc138\uc694. \ucd5c\uc18c 50\uac1c\uc758 \uc774\ubbf8\uc9c0\ub97c \uc218\uc9d1\ud574\uc57c \ud569\ub2c8\ub2e4."}),"\n",(0,r.jsxs)(t,{children:[(0,r.jsx)("summary",{children:"\ud574\ub2f5 \ubcf4\uae30"}),(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"from selenium import webdriver\nfrom selenium.webdriver.common.by import By\nfrom selenium.webdriver.chrome.options import Options\nimport time\n\nclass InfiniteScrollScraper:\n    def __init__(self, url):\n        self.url = url\n\n        # Chrome \uc635\uc158\n        chrome_options = Options()\n        chrome_options.add_argument('--headless')\n        chrome_options.add_argument('--no-sandbox')\n        chrome_options.add_argument('--disable-dev-shm-usage')\n\n        self.driver = webdriver.Chrome(options=chrome_options)\n\n    def scrape_images(self, min_images=50):\n        \"\"\"\ubb34\ud55c \uc2a4\ud06c\ub864\uc5d0\uc11c \uc774\ubbf8\uc9c0 \uc218\uc9d1\"\"\"\n        self.driver.get(self.url)\n\n        image_urls = set()  # \uc911\ubcf5 \uc81c\uac70\uc6a9\n        scroll_pause_time = 2\n        last_height = self.driver.execute_script(\"return document.body.scrollHeight\")\n\n        while len(image_urls) < min_images:\n            # \ud604\uc7ac \ud398\uc774\uc9c0\uc758 \uc774\ubbf8\uc9c0 \uc218\uc9d1\n            images = self.driver.find_elements(By.TAG_NAME, 'img')\n\n            for img in images:\n                src = img.get_attribute('src')\n                if src and src.startswith('http'):\n                    image_urls.add(src)\n\n            print(f\"Collected {len(image_urls)} images...\")\n\n            # \uc2a4\ud06c\ub864\n            self.driver.execute_script(\n                \"window.scrollTo(0, document.body.scrollHeight);\"\n            )\n\n            # \ub85c\ub529 \ub300\uae30\n            time.sleep(scroll_pause_time)\n\n            # \uc0c8\ub85c\uc6b4 \ub192\uc774\n            new_height = self.driver.execute_script(\n                \"return document.body.scrollHeight\"\n            )\n\n            if new_height == last_height:\n                # \ub354 \uc774\uc0c1 \ucf58\ud150\uce20 \uc5c6\uc74c\n                break\n\n            last_height = new_height\n\n        self.driver.quit()\n        return list(image_urls)\n\n    def save_urls(self, urls, filename='images.txt'):\n        \"\"\"URL \uc800\uc7a5\"\"\"\n        with open(filename, 'w', encoding='utf-8') as f:\n            for url in urls:\n                f.write(url + '\\n')\n\n        print(f\"Saved {len(urls)} image URLs to {filename}\")\n\n# \uc0ac\uc6a9\nscraper = InfiniteScrollScraper('https://example.com/gallery')\nimage_urls = scraper.scrape_images(min_images=50)\nscraper.save_urls(image_urls)\n"})})]}),"\n",(0,r.jsx)(n.h3,{id:"\ubb38\uc81c-3-\uac00\uaca9-\ube44\uad50-\uc2a4\ud06c\ub798\ud37c",children:"\ubb38\uc81c 3: \uac00\uaca9 \ube44\uad50 \uc2a4\ud06c\ub798\ud37c"}),"\n",(0,r.jsx)(n.p,{children:"\uc5ec\ub7ec \uc1fc\ud551\ubab0\uc5d0\uc11c \ub3d9\uc77c \uc81c\ud488\uc758 \uac00\uaca9\uc744 \uc218\uc9d1\ud558\uc5ec \ube44\uad50\ud558\ub294 \uc2a4\ud06c\ub798\ud37c\ub97c \uc791\uc131\ud558\uc138\uc694."}),"\n",(0,r.jsxs)(t,{children:[(0,r.jsx)("summary",{children:"\ud574\ub2f5 \ubcf4\uae30"}),(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"import requests\nfrom bs4 import BeautifulSoup\nimport json\n\nclass PriceComparator:\n    def __init__(self):\n        self.session = requests.Session()\n        self.session.headers.update({\n            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'\n        })\n\n        # \uac01 \uc1fc\ud551\ubab0\uc758 \uc2a4\ud06c\ub798\ud551 \uc124\uc815\n        self.shops = {\n            'Shop A': {\n                'url_template': 'https://shopa.example.com/search?q={}',\n                'selectors': {\n                    'item': '.product-card',\n                    'title': '.product-title',\n                    'price': '.price',\n                    'link': 'a.product-link'\n                }\n            },\n            'Shop B': {\n                'url_template': 'https://shopb.example.com/products?search={}',\n                'selectors': {\n                    'item': '.item',\n                    'title': 'h3.title',\n                    'price': 'span.cost',\n                    'link': 'a.item-link'\n                }\n            }\n        }\n\n    def search_product(self, product_name):\n        \"\"\"\uc5ec\ub7ec \uc1fc\ud551\ubab0\uc5d0\uc11c \uc81c\ud488 \uac80\uc0c9\"\"\"\n        results = {}\n\n        for shop_name, config in self.shops.items():\n            print(f\"Searching in {shop_name}...\")\n\n            try:\n                url = config['url_template'].format(product_name)\n                response = self.session.get(url, timeout=10)\n                soup = BeautifulSoup(response.text, 'lxml')\n\n                items = soup.select(config['selectors']['item'])\n\n                if items:\n                    first_item = items[0]\n\n                    results[shop_name] = {\n                        'title': self._safe_text(\n                            first_item,\n                            config['selectors']['title']\n                        ),\n                        'price': self._parse_price(\n                            self._safe_text(\n                                first_item,\n                                config['selectors']['price']\n                            )\n                        ),\n                        'link': self._safe_attr(\n                            first_item,\n                            config['selectors']['link'],\n                            'href'\n                        )\n                    }\n\n            except Exception as e:\n                print(f\"Error searching {shop_name}: {e}\")\n                continue\n\n        return results\n\n    def _safe_text(self, parent, selector):\n        elem = parent.select_one(selector)\n        return elem.text.strip() if elem else ''\n\n    def _safe_attr(self, parent, selector, attr):\n        elem = parent.select_one(selector)\n        return elem.get(attr, '') if elem else ''\n\n    def _parse_price(self, price_str):\n        \"\"\"\uac00\uaca9 \ubb38\uc790\uc5f4\uc744 \uc22b\uc790\ub85c \ubcc0\ud658\"\"\"\n        import re\n        if not price_str:\n            return None\n\n        numbers = re.sub(r'[^\\d]', '', price_str)\n        try:\n            return int(numbers)\n        except ValueError:\n            return None\n\n    def compare_prices(self, results):\n        \"\"\"\uac00\uaca9 \ube44\uad50\"\"\"\n        print(\"\\n=== \uac00\uaca9 \ube44\uad50 ===\")\n\n        # \uac00\uaca9\uc21c \uc815\ub82c\n        sorted_results = sorted(\n            results.items(),\n            key=lambda x: x[1]['price'] if x[1]['price'] else float('inf')\n        )\n\n        for shop_name, info in sorted_results:\n            if info['price']:\n                print(f\"{shop_name}: {info['price']:,}\uc6d0\")\n                print(f\"  \uc81c\ud488: {info['title']}\")\n                print(f\"  \ub9c1\ud06c: {info['link']}\")\n                print()\n\n        # \ucd5c\uc800\uac00\n        if sorted_results and sorted_results[0][1]['price']:\n            lowest = sorted_results[0]\n            print(f\"\ucd5c\uc800\uac00: {lowest[0]} - {lowest[1]['price']:,}\uc6d0\")\n\n        return sorted_results\n\n    def save_results(self, product_name, results, filename='price_comparison.json'):\n        \"\"\"\uacb0\uacfc \uc800\uc7a5\"\"\"\n        data = {\n            'product': product_name,\n            'search_date': datetime.now().isoformat(),\n            'results': results\n        }\n\n        with open(filename, 'w', encoding='utf-8') as f:\n            json.dump(data, f, ensure_ascii=False, indent=2)\n\n        print(f\"Results saved to {filename}\")\n\n# \uc0ac\uc6a9\ncomparator = PriceComparator()\nresults = comparator.search_product('\ubb34\uc120 \ub9c8\uc6b0\uc2a4')\ncomparator.compare_prices(results)\ncomparator.save_results('\ubb34\uc120 \ub9c8\uc6b0\uc2a4', results)\n"})})]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"\ub2e4\uc74c-\ub2e8\uacc4",children:"\ub2e4\uc74c \ub2e8\uacc4"}),"\n",(0,r.jsx)(n.p,{children:"\uc6f9 \uc2a4\ud06c\ub798\ud551\uc744 \ub9c8\uc2a4\ud130\ud588\ub2e4\uba74, \uc774\uc81c \uc218\uc9d1\ud55c \ub370\uc774\ud130\ub97c \uc11c\ube44\uc2a4\ub85c \uc81c\uacf5\ud558\ub294 \ubc29\ubc95\uc744 \ubc30\uc6cc\ubcf4\uc138\uc694:"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"\ub2e4\uc74c:"})," ",(0,r.jsx)(n.a,{href:"./apis",children:"REST API \uac1c\ubc1c"})]}),"\n",(0,r.jsx)(n.admonition,{title:"\uc8fc\uc758\uc0ac\ud56d",type:"warning",children:(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"\uc6f9 \uc2a4\ud06c\ub798\ud551\uc740 \ud56d\uc0c1 \ubc95\uc801, \uc724\ub9ac\uc801 \uace0\ub824\uc0ac\ud56d\uc744 \uc5fc\ub450\uc5d0 \ub450\uace0 \uc218\ud589\ud574\uc57c \ud569\ub2c8\ub2e4"}),"\n",(0,r.jsx)(n.li,{children:"\uc6f9\uc0ac\uc774\ud2b8\uc758 \uad6c\uc870\ub294 \uc5b8\uc81c\ub4e0 \ubcc0\uacbd\ub420 \uc218 \uc788\uc73c\ubbc0\ub85c \uc815\uae30\uc801\uc778 \uc720\uc9c0\ubcf4\uc218\uac00 \ud544\uc694\ud569\ub2c8\ub2e4"}),"\n",(0,r.jsx)(n.li,{children:"\uac00\ub2a5\ud558\ub2e4\uba74 \uacf5\uc2dd API \uc0ac\uc6a9\uc744 \uc6b0\uc120\uc801\uc73c\ub85c \uace0\ub824\ud558\uc138\uc694"}),"\n"]})})]})}function d(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(p,{...e})}):p(e)}},8453:(e,n,t)=>{t.d(n,{R:()=>l,x:()=>o});var s=t(6540);const r={},i=s.createContext(r);function l(e){const n=s.useContext(i);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:l(e.components),s.createElement(i.Provider,{value:n},e.children)}}}]);