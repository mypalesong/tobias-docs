"use strict";(globalThis.webpackChunkgithub_docs=globalThis.webpackChunkgithub_docs||[]).push([[8340],{5709:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>i,default:()=>u,frontMatter:()=>o,metadata:()=>s,toc:()=>d});const s=JSON.parse('{"id":"dagster/integrations","title":"10. \ud1b5\ud569","description":"Dagster\ub97c dbt, Airbyte, AWS, GCP, Snowflake \ub4f1 \ub2e4\uc591\ud55c \ub3c4\uad6c \ubc0f \ud50c\ub7ab\ud3fc\uacfc \ud1b5\ud569\ud558\ub294 \ubc29\ubc95\uc744 \ud559\uc2b5\ud569\ub2c8\ub2e4.","source":"@site/docs/dagster/integrations.md","sourceDirName":"dagster","slug":"/dagster/integrations","permalink":"/tobias-docs/comp-2/docs/dagster/integrations","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/dagster/integrations.md","tags":[],"version":"current","sidebarPosition":11,"frontMatter":{"sidebar_position":11},"sidebar":"dagsterSidebar","previous":{"title":"9. \ubaa8\ub2c8\ud130\ub9c1","permalink":"/tobias-docs/comp-2/docs/dagster/monitoring"},"next":{"title":"11. \uc2e4\uc804 \uc608\uc81c","permalink":"/tobias-docs/comp-2/docs/dagster/examples"}}');var r=t(4848),a=t(8453);const o={sidebar_position:11},i="10. \ud1b5\ud569",l={},d=[{value:"10.1 dbt \ud1b5\ud569",id:"101-dbt-\ud1b5\ud569",level:2},{value:"10.1.1 dbt \ud504\ub85c\uc81d\ud2b8 \uc124\uc815",id:"1011-dbt-\ud504\ub85c\uc81d\ud2b8-\uc124\uc815",level:3},{value:"10.1.2 Dagster + dbt \ud1b5\ud569",id:"1012-dagster--dbt-\ud1b5\ud569",level:3},{value:"10.1.3 \uc120\ud0dd\uc801 dbt \ubaa8\ub378 \uc2e4\ud589",id:"1013-\uc120\ud0dd\uc801-dbt-\ubaa8\ub378-\uc2e4\ud589",level:3},{value:"10.1.4 dbt \ud14c\uc2a4\ud2b8\uc640 Dagster \ud1b5\ud569",id:"1014-dbt-\ud14c\uc2a4\ud2b8\uc640-dagster-\ud1b5\ud569",level:3},{value:"10.1.5 Upstream Dagster Assets \u2192 dbt",id:"1015-upstream-dagster-assets--dbt",level:3},{value:"10.2 Airbyte \ud1b5\ud569",id:"102-airbyte-\ud1b5\ud569",level:2},{value:"10.2.1 Airbyte \uc124\uc815",id:"1021-airbyte-\uc124\uc815",level:3},{value:"10.2.2 Dagster + Airbyte \ud1b5\ud569",id:"1022-dagster--airbyte-\ud1b5\ud569",level:3},{value:"10.2.3 Airbyte + dbt \ud30c\uc774\ud504\ub77c\uc778",id:"1023-airbyte--dbt-\ud30c\uc774\ud504\ub77c\uc778",level:3},{value:"10.3 AWS \ud1b5\ud569",id:"103-aws-\ud1b5\ud569",level:2},{value:"10.3.1 S3 \ud1b5\ud569",id:"1031-s3-\ud1b5\ud569",level:3},{value:"10.3.2 Glue \ud1b5\ud569",id:"1032-glue-\ud1b5\ud569",level:3},{value:"10.3.3 Athena \ud1b5\ud569",id:"1033-athena-\ud1b5\ud569",level:3},{value:"10.4 GCP \ud1b5\ud569",id:"104-gcp-\ud1b5\ud569",level:2},{value:"10.4.1 BigQuery \ud1b5\ud569",id:"1041-bigquery-\ud1b5\ud569",level:3},{value:"10.4.2 GCS (Google Cloud Storage) \ud1b5\ud569",id:"1042-gcs-google-cloud-storage-\ud1b5\ud569",level:3},{value:"10.5 Snowflake \ud1b5\ud569",id:"105-snowflake-\ud1b5\ud569",level:2},{value:"10.5.1 Snowflake \uc5f0\uacb0",id:"1051-snowflake-\uc5f0\uacb0",level:3},{value:"10.5.2 Snowflake + dbt \ud1b5\ud569",id:"1052-snowflake--dbt-\ud1b5\ud569",level:3},{value:"10.6 \uc2e4\uc2b5 \ud504\ub85c\uc81d\ud2b8",id:"106-\uc2e4\uc2b5-\ud504\ub85c\uc81d\ud2b8",level:2},{value:"10.6.1 End-to-End \ub370\uc774\ud130 \ud30c\uc774\ud504\ub77c\uc778",id:"1061-end-to-end-\ub370\uc774\ud130-\ud30c\uc774\ud504\ub77c\uc778",level:3},{value:"10.6.2 \ud1b5\ud569 \ubaa8\ubc94 \uc0ac\ub840",id:"1062-\ud1b5\ud569-\ubaa8\ubc94-\uc0ac\ub840",level:3},{value:"10.7 \uc694\uc57d",id:"107-\uc694\uc57d",level:2},{value:"\ud575\uc2ec \uac1c\ub150",id:"\ud575\uc2ec-\uac1c\ub150",level:3},{value:"Best Practices",id:"best-practices",level:3},{value:"\ub9c8\ubb34\ub9ac",id:"\ub9c8\ubb34\ub9ac",level:3}];function c(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,a.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.header,{children:(0,r.jsx)(n.h1,{id:"10-\ud1b5\ud569",children:"10. \ud1b5\ud569"})}),"\n",(0,r.jsx)(n.p,{children:"Dagster\ub97c dbt, Airbyte, AWS, GCP, Snowflake \ub4f1 \ub2e4\uc591\ud55c \ub3c4\uad6c \ubc0f \ud50c\ub7ab\ud3fc\uacfc \ud1b5\ud569\ud558\ub294 \ubc29\ubc95\uc744 \ud559\uc2b5\ud569\ub2c8\ub2e4."}),"\n",(0,r.jsx)(n.h2,{id:"101-dbt-\ud1b5\ud569",children:"10.1 dbt \ud1b5\ud569"}),"\n",(0,r.jsx)(n.h3,{id:"1011-dbt-\ud504\ub85c\uc81d\ud2b8-\uc124\uc815",children:"10.1.1 dbt \ud504\ub85c\uc81d\ud2b8 \uc124\uc815"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"# dbt \ud504\ub85c\uc81d\ud2b8 \uad6c\uc870\nmy_dbt_project/\n\u251c\u2500\u2500 dbt_project.yml\n\u251c\u2500\u2500 profiles.yml\n\u251c\u2500\u2500 models/\n\u2502   \u251c\u2500\u2500 staging/\n\u2502   \u2502   \u251c\u2500\u2500 stg_customers.sql\n\u2502   \u2502   \u2514\u2500\u2500 stg_orders.sql\n\u2502   \u251c\u2500\u2500 marts/\n\u2502   \u2502   \u251c\u2500\u2500 dim_customers.sql\n\u2502   \u2502   \u2514\u2500\u2500 fct_orders.sql\n\u2502   \u2514\u2500\u2500 schema.yml\n\u2514\u2500\u2500 macros/\n"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-yaml",children:'# dbt_project.yml\nname: \'my_dbt_project\'\nversion: \'1.0.0\'\nconfig-version: 2\n\nprofile: \'my_profile\'\n\nmodel-paths: ["models"]\nanalysis-paths: ["analyses"]\ntest-paths: ["tests"]\nseed-paths: ["seeds"]\nmacro-paths: ["macros"]\n\ntarget-path: "target"\nclean-targets:\n  - "target"\n  - "dbt_packages"\n\nmodels:\n  my_dbt_project:\n    staging:\n      +materialized: view\n    marts:\n      +materialized: table\n'})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-yaml",children:"# profiles.yml\nmy_profile:\n  outputs:\n    dev:\n      type: postgres\n      host: localhost\n      port: 5432\n      user: dbt_user\n      password: dbt_password\n      dbname: analytics\n      schema: dbt_dev\n      threads: 4\n\n    prod:\n      type: postgres\n      host: prod-db.example.com\n      port: 5432\n      user: dbt_user\n      password: \"{{ env_var('DBT_PASSWORD') }}\"\n      dbname: analytics\n      schema: dbt_prod\n      threads: 8\n\n  target: dev\n"})}),"\n",(0,r.jsx)(n.h3,{id:"1012-dagster--dbt-\ud1b5\ud569",children:"10.1.2 Dagster + dbt \ud1b5\ud569"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"# \ud544\uc694\ud55c \ud328\ud0a4\uc9c0 \uc124\uce58\npip install dagster-dbt\n"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'# dbt_assets.py\n"""\nDagster\uc640 dbt \ud1b5\ud569\n"""\nfrom dagster import AssetExecutionContext, Definitions\nfrom dagster_dbt import (\n    DbtCliResource,\n    dbt_assets,\n    DbtProject,\n    build_dbt_asset_selection\n)\nfrom pathlib import Path\n\n# dbt \ud504\ub85c\uc81d\ud2b8 \uacbd\ub85c\nDBT_PROJECT_DIR = Path(__file__).parent / "my_dbt_project"\nDBT_PROFILES_DIR = DBT_PROJECT_DIR\n\n# dbt \ud504\ub85c\uc81d\ud2b8 \ub85c\ub4dc\ndbt_project = DbtProject(\n    project_dir=DBT_PROJECT_DIR,\n    packaged_project_dir=DBT_PROJECT_DIR,\n)\n\n# dbt manifest \ub85c\ub4dc\ndbt_project.prepare_if_dev()\n\n@dbt_assets(\n    manifest=dbt_project.manifest_path,\n    project=dbt_project,\n)\ndef my_dbt_assets(context: AssetExecutionContext, dbt: DbtCliResource):\n    """dbt \ubaa8\ub378\uc744 Dagster Asset\uc73c\ub85c \ub178\ucd9c"""\n    yield from dbt.cli(["build"], context=context).stream()\n\n# Definitions\ndefs = Definitions(\n    assets=[my_dbt_assets],\n    resources={\n        "dbt": DbtCliResource(\n            project_dir=DBT_PROJECT_DIR,\n            profiles_dir=DBT_PROFILES_DIR,\n        )\n    }\n)\n'})}),"\n",(0,r.jsx)(n.h3,{id:"1013-\uc120\ud0dd\uc801-dbt-\ubaa8\ub378-\uc2e4\ud589",children:"10.1.3 \uc120\ud0dd\uc801 dbt \ubaa8\ub378 \uc2e4\ud589"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'# selective_dbt_assets.py\n"""\n\ud2b9\uc815 dbt \ubaa8\ub378\ub9cc \uc120\ud0dd\uc801\uc73c\ub85c \uc2e4\ud589\n"""\nfrom dagster import (\n    asset,\n    AssetExecutionContext,\n    AssetSelection,\n    define_asset_job\n)\nfrom dagster_dbt import (\n    DbtCliResource,\n    dbt_assets,\n    DbtProject,\n    build_dbt_asset_selection\n)\n\n# dbt \ud504\ub85c\uc81d\ud2b8 \uc124\uc815\ndbt_project = DbtProject(\n    project_dir="my_dbt_project",\n)\ndbt_project.prepare_if_dev()\n\n@dbt_assets(\n    manifest=dbt_project.manifest_path,\n    project=dbt_project,\n    select="tag:daily"  # "daily" \ud0dc\uadf8\uac00 \uc788\ub294 \ubaa8\ub378\ub9cc\n)\ndef daily_dbt_models(context: AssetExecutionContext, dbt: DbtCliResource):\n    """\uc77c\uc77c \uc2e4\ud589\ub418\ub294 dbt \ubaa8\ub378"""\n    yield from dbt.cli(["build", "--select", "tag:daily"], context=context).stream()\n\n@dbt_assets(\n    manifest=dbt_project.manifest_path,\n    project=dbt_project,\n    select="marts"  # marts \ub514\ub809\ud1a0\ub9ac\uc758 \ubaa8\ub378\ub9cc\n)\ndef mart_dbt_models(context: AssetExecutionContext, dbt: DbtCliResource):\n    """Mart dbt \ubaa8\ub378"""\n    yield from dbt.cli(["build", "--select", "marts"], context=context).stream()\n\n# \ud2b9\uc815 \ubaa8\ub378\ub9cc \uc2e4\ud589\ud558\ub294 Job\nstaging_job = define_asset_job(\n    name="staging_models",\n    selection=AssetSelection.groups("staging")\n)\n\nmarts_job = define_asset_job(\n    name="marts_models",\n    selection=AssetSelection.groups("marts")\n)\n'})}),"\n",(0,r.jsx)(n.h3,{id:"1014-dbt-\ud14c\uc2a4\ud2b8\uc640-dagster-\ud1b5\ud569",children:"10.1.4 dbt \ud14c\uc2a4\ud2b8\uc640 Dagster \ud1b5\ud569"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'# dbt_with_tests.py\n"""\ndbt \ud14c\uc2a4\ud2b8\ub97c Dagster Asset Check\ub85c \ud1b5\ud569\n"""\nfrom dagster import (\n    AssetExecutionContext,\n    AssetCheckResult,\n    AssetCheckSeverity,\n    Output\n)\nfrom dagster_dbt import DbtCliResource, dbt_assets, DbtProject\nfrom typing import Iterator, Union\n\ndbt_project = DbtProject(project_dir="my_dbt_project")\ndbt_project.prepare_if_dev()\n\n@dbt_assets(\n    manifest=dbt_project.manifest_path,\n    project=dbt_project,\n)\ndef dbt_models_with_tests(\n    context: AssetExecutionContext,\n    dbt: DbtCliResource\n) -> Iterator[Union[Output, AssetCheckResult]]:\n    """dbt \ubaa8\ub378\uacfc \ud14c\uc2a4\ud2b8\ub97c \ud568\uaed8 \uc2e4\ud589"""\n    # 1. \ubaa8\ub378 \ube4c\ub4dc\n    yield from dbt.cli(["build"], context=context).stream()\n\n    # 2. \ud14c\uc2a4\ud2b8 \uc2e4\ud589\n    test_results = dbt.cli(["test"], context=context).stream()\n\n    for result in test_results:\n        # \ud14c\uc2a4\ud2b8 \uacb0\uacfc\ub97c AssetCheckResult\ub85c \ubcc0\ud658\n        if hasattr(result, \'test_results\'):\n            for test in result.test_results:\n                yield AssetCheckResult(\n                    passed=test.status == "pass",\n                    asset_key=test.node.asset_key,\n                    check_name=test.node.name,\n                    metadata={\n                        "test_type": test.node.test_metadata.name,\n                        "failures": test.failures if test.failures else 0\n                    },\n                    severity=(\n                        AssetCheckSeverity.ERROR\n                        if test.status == "fail"\n                        else None\n                    )\n                )\n'})}),"\n",(0,r.jsx)(n.h3,{id:"1015-upstream-dagster-assets--dbt",children:"10.1.5 Upstream Dagster Assets \u2192 dbt"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'# upstream_to_dbt.py\n"""\nDagster Asset\uc744 dbt\uc758 upstream\uc73c\ub85c \uc0ac\uc6a9\n"""\nfrom dagster import asset, AssetExecutionContext\nfrom dagster_dbt import DbtCliResource, dbt_assets, DbtProject\nimport pandas as pd\n\n@asset\ndef raw_customers(context: AssetExecutionContext) -> pd.DataFrame:\n    """\uc678\ubd80 \uc18c\uc2a4\uc5d0\uc11c \uc6d0\uc2dc \uace0\uac1d \ub370\uc774\ud130 \ub85c\ub4dc"""\n    context.log.info("Fetching raw customer data from API")\n\n    # API\uc5d0\uc11c \ub370\uc774\ud130 \uac00\uc838\uc624\uae30 (\uc2dc\ubbac\ub808\uc774\uc158)\n    df = pd.DataFrame({\n        "customer_id": range(1, 101),\n        "name": [f"Customer {i}" for i in range(1, 101)],\n        "email": [f"customer{i}@example.com" for i in range(1, 101)],\n        "signup_date": pd.date_range("2024-01-01", periods=100)\n    })\n\n    # \ub370\uc774\ud130\ubca0\uc774\uc2a4\uc5d0 \uc800\uc7a5 (dbt\uac00 \uc77d\uc744 \uc218 \uc788\ub3c4\ub85d)\n    # df.to_sql(\'raw_customers\', con=engine, if_exists=\'replace\')\n\n    context.log.info(f"Loaded {len(df)} customers")\n    return df\n\n@asset\ndef raw_orders(context: AssetExecutionContext) -> pd.DataFrame:\n    """\uc6d0\uc2dc \uc8fc\ubb38 \ub370\uc774\ud130"""\n    df = pd.DataFrame({\n        "order_id": range(1, 501),\n        "customer_id": [i % 100 + 1 for i in range(501)],\n        "order_date": pd.date_range("2024-01-01", periods=501, freq=\'H\'),\n        "amount": [100 + (i % 500) for i in range(501)]\n    })\n\n    # df.to_sql(\'raw_orders\', con=engine, if_exists=\'replace\')\n\n    return df\n\n# dbt \ud504\ub85c\uc81d\ud2b8 \uc124\uc815\ndbt_project = DbtProject(project_dir="my_dbt_project")\ndbt_project.prepare_if_dev()\n\n@dbt_assets(\n    manifest=dbt_project.manifest_path,\n    project=dbt_project,\n)\ndef dbt_analytics_models(context: AssetExecutionContext, dbt: DbtCliResource):\n    """dbt \ubd84\uc11d \ubaa8\ub378 (raw_customers, raw_orders\ub97c \uc0ac\uc6a9)"""\n    # raw_customers\uc640 raw_orders\uac00 \uba3c\uc800 \uc2e4\ud589\ub428\n    yield from dbt.cli(["build"], context=context).stream()\n'})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-sql",children:"-- models/staging/stg_customers.sql\n-- Dagster\uc758 raw_customers \ud14c\uc774\ube14\uc744 \ucc38\uc870\n\nselect\n    customer_id,\n    name,\n    email,\n    signup_date,\n    current_timestamp() as loaded_at\nfrom {{ source('raw', 'raw_customers') }}\n"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-sql",children:"-- models/staging/stg_orders.sql\nselect\n    order_id,\n    customer_id,\n    order_date,\n    amount,\n    current_timestamp() as loaded_at\nfrom {{ source('raw', 'raw_orders') }}\n"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-sql",children:"-- models/marts/fct_orders.sql\n-- \uace0\uac1d\uacfc \uc8fc\ubb38 \uc870\uc778\nselect\n    o.order_id,\n    o.customer_id,\n    c.name as customer_name,\n    c.email as customer_email,\n    o.order_date,\n    o.amount\nfrom {{ ref('stg_orders') }} o\nleft join {{ ref('stg_customers') }} c\n    on o.customer_id = c.customer_id\n"})}),"\n",(0,r.jsx)(n.h2,{id:"102-airbyte-\ud1b5\ud569",children:"10.2 Airbyte \ud1b5\ud569"}),"\n",(0,r.jsx)(n.h3,{id:"1021-airbyte-\uc124\uc815",children:"10.2.1 Airbyte \uc124\uc815"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"# Airbyte \uc124\uce58 (Docker Compose)\ngit clone https://github.com/airbytehq/airbyte.git\ncd airbyte\ndocker-compose up -d\n"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"# dagster-airbyte \uc124\uce58\npip install dagster-airbyte\n"})}),"\n",(0,r.jsx)(n.h3,{id:"1022-dagster--airbyte-\ud1b5\ud569",children:"10.2.2 Dagster + Airbyte \ud1b5\ud569"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'# airbyte_assets.py\n"""\nDagster\uc640 Airbyte \ud1b5\ud569\n"""\nfrom dagster import (\n    asset,\n    AssetExecutionContext,\n    Output,\n    MetadataValue,\n    Definitions\n)\nfrom dagster_airbyte import (\n    AirbyteResource,\n    load_assets_from_airbyte_instance,\n    build_airbyte_assets\n)\n\n# Airbyte \uc5f0\uacb0 \uc124\uc815\nairbyte_instance = AirbyteResource(\n    host="localhost",\n    port="8000",\n    username="airbyte",\n    password="password"\n)\n\n# Airbyte Connection\uc744 Dagster Asset\uc73c\ub85c \ub85c\ub4dc\nairbyte_assets = load_assets_from_airbyte_instance(\n    airbyte_instance,\n    connection_filter=lambda meta: "postgres" in meta.name.lower(),\n    connection_to_asset_key_fn=lambda conn, name: [\n        "airbyte",\n        conn.source.name,\n        name\n    ]\n)\n\n# \ud2b9\uc815 Connection\ub9cc Asset\uc73c\ub85c \uc815\uc758\n@asset(\n    required_resource_keys={"airbyte"}\n)\ndef sync_postgres_to_bigquery(context: AssetExecutionContext) -> Output[dict]:\n    """Postgres\uc5d0\uc11c BigQuery\ub85c \ub370\uc774\ud130 \ub3d9\uae30\ud654"""\n    airbyte = context.resources.airbyte\n\n    # Connection ID (Airbyte UI\uc5d0\uc11c \ud655\uc778)\n    connection_id = "your-connection-id"\n\n    # \ub3d9\uae30\ud654 \uc2e4\ud589\n    sync_result = airbyte.sync_and_poll(connection_id)\n\n    return Output(\n        sync_result,\n        metadata={\n            "records_synced": MetadataValue.int(\n                sync_result.records_synced\n            ),\n            "bytes_synced": MetadataValue.int(\n                sync_result.bytes_synced\n            ),\n            "job_status": MetadataValue.text(\n                sync_result.job_status\n            )\n        }\n    )\n\ndefs = Definitions(\n    assets=[*airbyte_assets, sync_postgres_to_bigquery],\n    resources={\n        "airbyte": airbyte_instance\n    }\n)\n'})}),"\n",(0,r.jsx)(n.h3,{id:"1023-airbyte--dbt-\ud30c\uc774\ud504\ub77c\uc778",children:"10.2.3 Airbyte + dbt \ud30c\uc774\ud504\ub77c\uc778"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'# airbyte_dbt_pipeline.py\n"""\nAirbyte\ub85c \ub370\uc774\ud130 \ucd94\ucd9c \u2192 dbt\ub85c \ubcc0\ud658\n"""\nfrom dagster import (\n    asset,\n    AssetExecutionContext,\n    Definitions,\n    define_asset_job,\n    ScheduleDefinition\n)\nfrom dagster_airbyte import AirbyteResource, build_airbyte_assets\nfrom dagster_dbt import DbtCliResource, dbt_assets, DbtProject\n\n# Airbyte \uc124\uc815\nairbyte_resource = AirbyteResource(\n    host="localhost",\n    port="8000",\n)\n\n# Airbyte Connection\uc744 Asset\uc73c\ub85c \ubcc0\ud658\nairbyte_connection_id = "your-airbyte-connection-id"\nairbyte_assets = build_airbyte_assets(\n    connection_id=airbyte_connection_id,\n    destination_tables=["raw_customers", "raw_orders"],\n    asset_key_prefix=["airbyte", "postgres"]\n)\n\n# dbt \ud504\ub85c\uc81d\ud2b8\ndbt_project = DbtProject(project_dir="my_dbt_project")\ndbt_project.prepare_if_dev()\n\n@dbt_assets(\n    manifest=dbt_project.manifest_path,\n    project=dbt_project,\n)\ndef analytics_models(context: AssetExecutionContext, dbt: DbtCliResource):\n    """dbt \ubd84\uc11d \ubaa8\ub378 (Airbyte \ub370\uc774\ud130 \uc0ac\uc6a9)"""\n    yield from dbt.cli(["build"], context=context).stream()\n\n# ELT \ud30c\uc774\ud504\ub77c\uc778 Job\nelt_pipeline = define_asset_job(\n    name="daily_elt_pipeline",\n    selection=[airbyte_assets, analytics_models]\n)\n\n# \ub9e4\uc77c \uc2e4\ud589 \uc2a4\ucf00\uc904\ndaily_schedule = ScheduleDefinition(\n    name="daily_elt_schedule",\n    job=elt_pipeline,\n    cron_schedule="0 2 * * *"  # \ub9e4\uc77c \uc624\uc804 2\uc2dc\n)\n\ndefs = Definitions(\n    assets=[airbyte_assets, analytics_models],\n    jobs=[elt_pipeline],\n    schedules=[daily_schedule],\n    resources={\n        "airbyte": airbyte_resource,\n        "dbt": DbtCliResource(project_dir="my_dbt_project")\n    }\n)\n'})}),"\n",(0,r.jsx)(n.h2,{id:"103-aws-\ud1b5\ud569",children:"10.3 AWS \ud1b5\ud569"}),"\n",(0,r.jsx)(n.h3,{id:"1031-s3-\ud1b5\ud569",children:"10.3.1 S3 \ud1b5\ud569"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"# AWS \uad00\ub828 \ud328\ud0a4\uc9c0 \uc124\uce58\npip install dagster-aws boto3\n"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'# aws/s3_assets.py\n"""\nAWS S3 \ud1b5\ud569\n"""\nfrom dagster import (\n    asset,\n    AssetExecutionContext,\n    Output,\n    MetadataValue,\n    ConfigurableResource\n)\nimport pandas as pd\nimport boto3\nfrom io import StringIO, BytesIO\nfrom typing import Optional\n\nclass S3Resource(ConfigurableResource):\n    """S3 \ub9ac\uc18c\uc2a4"""\n    bucket_name: str\n    region_name: str = "us-east-1"\n    aws_access_key_id: Optional[str] = None\n    aws_secret_access_key: Optional[str] = None\n\n    def get_client(self):\n        """S3 \ud074\ub77c\uc774\uc5b8\ud2b8 \uc0dd\uc131"""\n        return boto3.client(\n            \'s3\',\n            region_name=self.region_name,\n            aws_access_key_id=self.aws_access_key_id,\n            aws_secret_access_key=self.aws_secret_access_key\n        )\n\n    def upload_dataframe(\n        self,\n        df: pd.DataFrame,\n        key: str,\n        format: str = "parquet"\n    ):\n        """DataFrame\uc744 S3\uc5d0 \uc5c5\ub85c\ub4dc"""\n        s3 = self.get_client()\n\n        if format == "parquet":\n            buffer = BytesIO()\n            df.to_parquet(buffer, index=False)\n            s3.put_object(\n                Bucket=self.bucket_name,\n                Key=key,\n                Body=buffer.getvalue()\n            )\n        elif format == "csv":\n            csv_buffer = StringIO()\n            df.to_csv(csv_buffer, index=False)\n            s3.put_object(\n                Bucket=self.bucket_name,\n                Key=key,\n                Body=csv_buffer.getvalue()\n            )\n\n    def download_dataframe(\n        self,\n        key: str,\n        format: str = "parquet"\n    ) -> pd.DataFrame:\n        """S3\uc5d0\uc11c DataFrame \ub2e4\uc6b4\ub85c\ub4dc"""\n        s3 = self.get_client()\n\n        obj = s3.get_object(Bucket=self.bucket_name, Key=key)\n\n        if format == "parquet":\n            return pd.read_parquet(BytesIO(obj[\'Body\'].read()))\n        elif format == "csv":\n            return pd.read_csv(StringIO(obj[\'Body\'].read().decode(\'utf-8\')))\n\n    def list_objects(self, prefix: str = "") -> list:\n        """S3 \uac1d\uccb4 \ubaa9\ub85d \uc870\ud68c"""\n        s3 = self.get_client()\n\n        response = s3.list_objects_v2(\n            Bucket=self.bucket_name,\n            Prefix=prefix\n        )\n\n        return [obj[\'Key\'] for obj in response.get(\'Contents\', [])]\n\n@asset\ndef upload_to_s3(\n    context: AssetExecutionContext,\n    s3: S3Resource\n) -> Output[str]:\n    """\ub370\uc774\ud130\ub97c S3\uc5d0 \uc5c5\ub85c\ub4dc"""\n    # \ub370\uc774\ud130 \uc0dd\uc131\n    df = pd.DataFrame({\n        "id": range(1, 1001),\n        "value": range(1000, 2000)\n    })\n\n    # S3\uc5d0 \uc5c5\ub85c\ub4dc\n    key = f"data/processed/data_{context.run_id}.parquet"\n    s3.upload_dataframe(df, key, format="parquet")\n\n    context.log.info(f"Uploaded {len(df)} rows to s3://{s3.bucket_name}/{key}")\n\n    return Output(\n        key,\n        metadata={\n            "s3_uri": MetadataValue.text(f"s3://{s3.bucket_name}/{key}"),\n            "row_count": MetadataValue.int(len(df)),\n            "file_size_mb": MetadataValue.float(\n                df.memory_usage(deep=True).sum() / 1024 / 1024\n            )\n        }\n    )\n\n@asset\ndef read_from_s3(\n    context: AssetExecutionContext,\n    s3: S3Resource,\n    upload_to_s3: str\n) -> pd.DataFrame:\n    """S3\uc5d0\uc11c \ub370\uc774\ud130 \uc77d\uae30"""\n    # S3\uc5d0\uc11c \ub2e4\uc6b4\ub85c\ub4dc\n    df = s3.download_dataframe(upload_to_s3, format="parquet")\n\n    context.log.info(f"Downloaded {len(df)} rows from S3")\n\n    return df\n'})}),"\n",(0,r.jsx)(n.h3,{id:"1032-glue-\ud1b5\ud569",children:"10.3.2 Glue \ud1b5\ud569"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'# aws/glue_integration.py\n"""\nAWS Glue \ud1b5\ud569\n"""\nfrom dagster import asset, AssetExecutionContext, ConfigurableResource\nimport boto3\nfrom typing import Dict, Any, Optional\n\nclass GlueResource(ConfigurableResource):\n    """AWS Glue \ub9ac\uc18c\uc2a4"""\n    region_name: str = "us-east-1"\n    aws_access_key_id: Optional[str] = None\n    aws_secret_access_key: Optional[str] = None\n\n    def get_client(self):\n        """Glue \ud074\ub77c\uc774\uc5b8\ud2b8 \uc0dd\uc131"""\n        return boto3.client(\n            \'glue\',\n            region_name=self.region_name,\n            aws_access_key_id=self.aws_access_key_id,\n            aws_secret_access_key=self.aws_secret_access_key\n        )\n\n    def start_crawler(self, crawler_name: str) -> Dict[str, Any]:\n        """Glue Crawler \uc2e4\ud589"""\n        glue = self.get_client()\n        response = glue.start_crawler(Name=crawler_name)\n        return response\n\n    def get_crawler_status(self, crawler_name: str) -> str:\n        """Crawler \uc0c1\ud0dc \uc870\ud68c"""\n        glue = self.get_client()\n        response = glue.get_crawler(Name=crawler_name)\n        return response[\'Crawler\'][\'State\']\n\n    def start_job(\n        self,\n        job_name: str,\n        arguments: Dict[str, str] = None\n    ) -> str:\n        """Glue Job \uc2e4\ud589"""\n        glue = self.get_client()\n        response = glue.start_job_run(\n            JobName=job_name,\n            Arguments=arguments or {}\n        )\n        return response[\'JobRunId\']\n\n    def get_job_status(self, job_name: str, run_id: str) -> str:\n        """Job \uc0c1\ud0dc \uc870\ud68c"""\n        glue = self.get_client()\n        response = glue.get_job_run(JobName=job_name, RunId=run_id)\n        return response[\'JobRun\'][\'JobRunState\']\n\n@asset\ndef run_glue_crawler(\n    context: AssetExecutionContext,\n    glue: GlueResource\n) -> str:\n    """Glue Crawler \uc2e4\ud589"""\n    crawler_name = "my-data-crawler"\n\n    # Crawler \uc2dc\uc791\n    glue.start_crawler(crawler_name)\n    context.log.info(f"Started Glue Crawler: {crawler_name}")\n\n    # \uc0c1\ud0dc \ud655\uc778 (\uc2e4\uc81c\ub85c\ub294 polling \ud544\uc694)\n    status = glue.get_crawler_status(crawler_name)\n    context.log.info(f"Crawler status: {status}")\n\n    return crawler_name\n\n@asset\ndef run_glue_job(\n    context: AssetExecutionContext,\n    glue: GlueResource,\n    run_glue_crawler: str\n) -> str:\n    """Glue Job \uc2e4\ud589"""\n    job_name = "my-etl-job"\n\n    # Job \uc2dc\uc791\n    run_id = glue.start_job(\n        job_name=job_name,\n        arguments={\n            "--source_path": "s3://my-bucket/input/",\n            "--target_path": "s3://my-bucket/output/"\n        }\n    )\n\n    context.log.info(f"Started Glue Job: {job_name}, Run ID: {run_id}")\n\n    return run_id\n'})}),"\n",(0,r.jsx)(n.h3,{id:"1033-athena-\ud1b5\ud569",children:"10.3.3 Athena \ud1b5\ud569"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'# aws/athena_integration.py\n"""\nAWS Athena \ud1b5\ud569\n"""\nfrom dagster import asset, AssetExecutionContext, ConfigurableResource\nimport boto3\nimport pandas as pd\nimport time\nfrom typing import Optional\n\nclass AthenaResource(ConfigurableResource):\n    """AWS Athena \ub9ac\uc18c\uc2a4"""\n    database: str\n    output_location: str\n    region_name: str = "us-east-1"\n    aws_access_key_id: Optional[str] = None\n    aws_secret_access_key: Optional[str] = None\n\n    def get_client(self):\n        """Athena \ud074\ub77c\uc774\uc5b8\ud2b8 \uc0dd\uc131"""\n        return boto3.client(\n            \'athena\',\n            region_name=self.region_name,\n            aws_access_key_id=self.aws_access_key_id,\n            aws_secret_access_key=self.aws_secret_access_key\n        )\n\n    def execute_query(\n        self,\n        query: str,\n        wait: bool = True\n    ) -> str:\n        """Athena \ucffc\ub9ac \uc2e4\ud589"""\n        athena = self.get_client()\n\n        # \ucffc\ub9ac \uc2e4\ud589\n        response = athena.start_query_execution(\n            QueryString=query,\n            QueryExecutionContext={\'Database\': self.database},\n            ResultConfiguration={\'OutputLocation\': self.output_location}\n        )\n\n        execution_id = response[\'QueryExecutionId\']\n\n        # \ucffc\ub9ac \uc644\ub8cc \ub300\uae30\n        if wait:\n            self._wait_for_query(execution_id)\n\n        return execution_id\n\n    def _wait_for_query(self, execution_id: str, max_wait: int = 300):\n        """\ucffc\ub9ac \uc644\ub8cc \ub300\uae30"""\n        athena = self.get_client()\n        elapsed = 0\n\n        while elapsed < max_wait:\n            response = athena.get_query_execution(\n                QueryExecutionId=execution_id\n            )\n            status = response[\'QueryExecution\'][\'Status\'][\'State\']\n\n            if status in [\'SUCCEEDED\', \'FAILED\', \'CANCELLED\']:\n                return status\n\n            time.sleep(5)\n            elapsed += 5\n\n        raise TimeoutError(f"Query did not complete within {max_wait} seconds")\n\n    def get_query_results(self, execution_id: str) -> pd.DataFrame:\n        """\ucffc\ub9ac \uacb0\uacfc \uc870\ud68c"""\n        athena = self.get_client()\n\n        # \uacb0\uacfc \uac00\uc838\uc624\uae30\n        results = []\n        next_token = None\n\n        while True:\n            if next_token:\n                response = athena.get_query_results(\n                    QueryExecutionId=execution_id,\n                    NextToken=next_token\n                )\n            else:\n                response = athena.get_query_results(\n                    QueryExecutionId=execution_id\n                )\n\n            # \uccab \ubc88\uc9f8 \ud589\uc740 \ud5e4\ub354\n            if not results:\n                columns = [\n                    col[\'Name\']\n                    for col in response[\'ResultSet\'][\'ResultSetMetadata\'][\'ColumnInfo\']\n                ]\n            else:\n                # \ub370\uc774\ud130 \ud589 \ucd94\uac00\n                for row in response[\'ResultSet\'][\'Rows\'][1:]:\n                    results.append([\n                        field.get(\'VarCharValue\', None)\n                        for field in row[\'Data\']\n                    ])\n\n            next_token = response.get(\'NextToken\')\n            if not next_token:\n                break\n\n        # DataFrame \uc0dd\uc131\n        return pd.DataFrame(results, columns=columns)\n\n@asset\ndef query_athena(\n    context: AssetExecutionContext,\n    athena: AthenaResource\n) -> pd.DataFrame:\n    """Athena \ucffc\ub9ac \uc2e4\ud589 \ubc0f \uacb0\uacfc \uc870\ud68c"""\n    query = """\n    SELECT\n        customer_id,\n        COUNT(*) as order_count,\n        SUM(amount) as total_amount\n    FROM orders\n    WHERE order_date >= DATE \'2024-01-01\'\n    GROUP BY customer_id\n    ORDER BY total_amount DESC\n    LIMIT 100\n    """\n\n    context.log.info("Executing Athena query...")\n\n    # \ucffc\ub9ac \uc2e4\ud589\n    execution_id = athena.execute_query(query, wait=True)\n\n    context.log.info(f"Query completed: {execution_id}")\n\n    # \uacb0\uacfc \uc870\ud68c\n    df = athena.get_query_results(execution_id)\n\n    context.log.info(f"Retrieved {len(df)} rows")\n\n    return df\n'})}),"\n",(0,r.jsx)(n.h2,{id:"104-gcp-\ud1b5\ud569",children:"10.4 GCP \ud1b5\ud569"}),"\n",(0,r.jsx)(n.h3,{id:"1041-bigquery-\ud1b5\ud569",children:"10.4.1 BigQuery \ud1b5\ud569"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"# GCP \uad00\ub828 \ud328\ud0a4\uc9c0 \uc124\uce58\npip install dagster-gcp google-cloud-bigquery\n"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'# gcp/bigquery_integration.py\n"""\nGCP BigQuery \ud1b5\ud569\n"""\nfrom dagster import (\n    asset,\n    AssetExecutionContext,\n    Output,\n    MetadataValue,\n    ConfigurableResource\n)\nfrom google.cloud import bigquery\nfrom google.oauth2 import service_account\nimport pandas as pd\nfrom typing import Optional\n\nclass BigQueryResource(ConfigurableResource):\n    """BigQuery \ub9ac\uc18c\uc2a4"""\n    project_id: str\n    credentials_path: Optional[str] = None\n\n    def get_client(self) -> bigquery.Client:\n        """BigQuery \ud074\ub77c\uc774\uc5b8\ud2b8 \uc0dd\uc131"""\n        if self.credentials_path:\n            credentials = service_account.Credentials.from_service_account_file(\n                self.credentials_path\n            )\n            return bigquery.Client(\n                project=self.project_id,\n                credentials=credentials\n            )\n        else:\n            return bigquery.Client(project=self.project_id)\n\n    def execute_query(self, query: str) -> pd.DataFrame:\n        """\ucffc\ub9ac \uc2e4\ud589 \ubc0f \uacb0\uacfc \ubc18\ud658"""\n        client = self.get_client()\n        query_job = client.query(query)\n        return query_job.to_dataframe()\n\n    def upload_dataframe(\n        self,\n        df: pd.DataFrame,\n        dataset_id: str,\n        table_id: str,\n        write_disposition: str = "WRITE_TRUNCATE"\n    ):\n        """DataFrame\uc744 BigQuery \ud14c\uc774\ube14\uc5d0 \uc5c5\ub85c\ub4dc"""\n        client = self.get_client()\n        table_ref = f"{self.project_id}.{dataset_id}.{table_id}"\n\n        job_config = bigquery.LoadJobConfig(\n            write_disposition=write_disposition\n        )\n\n        job = client.load_table_from_dataframe(\n            df,\n            table_ref,\n            job_config=job_config\n        )\n        job.result()  # \uc644\ub8cc \ub300\uae30\n\n@asset\ndef load_from_bigquery(\n    context: AssetExecutionContext,\n    bigquery_resource: BigQueryResource\n) -> pd.DataFrame:\n    """BigQuery\uc5d0\uc11c \ub370\uc774\ud130 \ub85c\ub4dc"""\n    query = """\n    SELECT\n        customer_id,\n        name,\n        email,\n        DATE(created_at) as signup_date\n    FROM `my-project.analytics.customers`\n    WHERE created_at >= TIMESTAMP_SUB(CURRENT_TIMESTAMP(), INTERVAL 30 DAY)\n    """\n\n    context.log.info("Executing BigQuery query...")\n\n    df = bigquery_resource.execute_query(query)\n\n    context.log.info(f"Loaded {len(df)} rows from BigQuery")\n\n    return df\n\n@asset\ndef write_to_bigquery(\n    context: AssetExecutionContext,\n    bigquery_resource: BigQueryResource,\n    load_from_bigquery: pd.DataFrame\n) -> Output[str]:\n    """\ucc98\ub9ac\ub41c \ub370\uc774\ud130\ub97c BigQuery\uc5d0 \uc800\uc7a5"""\n    # \ub370\uc774\ud130 \ucc98\ub9ac\n    df = load_from_bigquery.copy()\n    df["processed_at"] = pd.Timestamp.now()\n\n    # BigQuery\uc5d0 \uc800\uc7a5\n    dataset_id = "analytics"\n    table_id = "processed_customers"\n\n    bigquery_resource.upload_dataframe(\n        df,\n        dataset_id=dataset_id,\n        table_id=table_id\n    )\n\n    table_ref = f"{bigquery_resource.project_id}.{dataset_id}.{table_id}"\n\n    context.log.info(f"Uploaded {len(df)} rows to {table_ref}")\n\n    return Output(\n        table_ref,\n        metadata={\n            "table": MetadataValue.text(table_ref),\n            "row_count": MetadataValue.int(len(df)),\n            "column_count": MetadataValue.int(len(df.columns))\n        }\n    )\n\n@asset\ndef bigquery_aggregation(\n    context: AssetExecutionContext,\n    bigquery_resource: BigQueryResource\n) -> pd.DataFrame:\n    """BigQuery\uc5d0\uc11c \uc9d1\uacc4 \ucffc\ub9ac \uc2e4\ud589"""\n    query = """\n    SELECT\n        DATE_TRUNC(signup_date, MONTH) as month,\n        COUNT(*) as new_customers,\n        COUNT(DISTINCT email) as unique_emails\n    FROM `my-project.analytics.processed_customers`\n    GROUP BY month\n    ORDER BY month DESC\n    LIMIT 12\n    """\n\n    df = bigquery_resource.execute_query(query)\n\n    return df\n'})}),"\n",(0,r.jsx)(n.h3,{id:"1042-gcs-google-cloud-storage-\ud1b5\ud569",children:"10.4.2 GCS (Google Cloud Storage) \ud1b5\ud569"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'# gcp/gcs_integration.py\n"""\nGCP Cloud Storage \ud1b5\ud569\n"""\nfrom dagster import asset, AssetExecutionContext, ConfigurableResource\nfrom google.cloud import storage\nfrom google.oauth2 import service_account\nimport pandas as pd\nfrom io import BytesIO\nfrom typing import Optional\n\nclass GCSResource(ConfigurableResource):\n    """GCS \ub9ac\uc18c\uc2a4"""\n    bucket_name: str\n    credentials_path: Optional[str] = None\n\n    def get_client(self) -> storage.Client:\n        """GCS \ud074\ub77c\uc774\uc5b8\ud2b8 \uc0dd\uc131"""\n        if self.credentials_path:\n            credentials = service_account.Credentials.from_service_account_file(\n                self.credentials_path\n            )\n            return storage.Client(credentials=credentials)\n        else:\n            return storage.Client()\n\n    def upload_dataframe(\n        self,\n        df: pd.DataFrame,\n        blob_name: str,\n        format: str = "parquet"\n    ):\n        """DataFrame\uc744 GCS\uc5d0 \uc5c5\ub85c\ub4dc"""\n        client = self.get_client()\n        bucket = client.bucket(self.bucket_name)\n        blob = bucket.blob(blob_name)\n\n        buffer = BytesIO()\n\n        if format == "parquet":\n            df.to_parquet(buffer, index=False)\n        elif format == "csv":\n            df.to_csv(buffer, index=False)\n\n        blob.upload_from_string(buffer.getvalue())\n\n    def download_dataframe(\n        self,\n        blob_name: str,\n        format: str = "parquet"\n    ) -> pd.DataFrame:\n        """GCS\uc5d0\uc11c DataFrame \ub2e4\uc6b4\ub85c\ub4dc"""\n        client = self.get_client()\n        bucket = client.bucket(self.bucket_name)\n        blob = bucket.blob(blob_name)\n\n        content = blob.download_as_bytes()\n\n        if format == "parquet":\n            return pd.read_parquet(BytesIO(content))\n        elif format == "csv":\n            return pd.read_csv(BytesIO(content))\n\n@asset\ndef upload_to_gcs(\n    context: AssetExecutionContext,\n    gcs: GCSResource\n) -> str:\n    """\ub370\uc774\ud130\ub97c GCS\uc5d0 \uc5c5\ub85c\ub4dc"""\n    df = pd.DataFrame({\n        "id": range(1, 101),\n        "value": range(100, 200)\n    })\n\n    blob_name = f"data/processed/data_{context.run_id}.parquet"\n    gcs.upload_dataframe(df, blob_name, format="parquet")\n\n    uri = f"gs://{gcs.bucket_name}/{blob_name}"\n    context.log.info(f"Uploaded to {uri}")\n\n    return blob_name\n\n@asset\ndef read_from_gcs(\n    context: AssetExecutionContext,\n    gcs: GCSResource,\n    upload_to_gcs: str\n) -> pd.DataFrame:\n    """GCS\uc5d0\uc11c \ub370\uc774\ud130 \uc77d\uae30"""\n    df = gcs.download_dataframe(upload_to_gcs, format="parquet")\n\n    context.log.info(f"Downloaded {len(df)} rows from GCS")\n\n    return df\n'})}),"\n",(0,r.jsx)(n.h2,{id:"105-snowflake-\ud1b5\ud569",children:"10.5 Snowflake \ud1b5\ud569"}),"\n",(0,r.jsx)(n.h3,{id:"1051-snowflake-\uc5f0\uacb0",children:"10.5.1 Snowflake \uc5f0\uacb0"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"# Snowflake \ud328\ud0a4\uc9c0 \uc124\uce58\npip install dagster-snowflake snowflake-connector-python\n"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'# snowflake_integration.py\n"""\nSnowflake \ud1b5\ud569\n"""\nfrom dagster import (\n    asset,\n    AssetExecutionContext,\n    Output,\n    MetadataValue,\n    ConfigurableResource\n)\nimport snowflake.connector\nimport pandas as pd\nfrom typing import Optional, Dict, Any\n\nclass SnowflakeResource(ConfigurableResource):\n    """Snowflake \ub9ac\uc18c\uc2a4"""\n    account: str\n    user: str\n    password: str\n    warehouse: str\n    database: str\n    schema: str\n    role: Optional[str] = None\n\n    def get_connection(self):\n        """Snowflake \uc5f0\uacb0 \uc0dd\uc131"""\n        return snowflake.connector.connect(\n            account=self.account,\n            user=self.user,\n            password=self.password,\n            warehouse=self.warehouse,\n            database=self.database,\n            schema=self.schema,\n            role=self.role\n        )\n\n    def execute_query(self, query: str) -> pd.DataFrame:\n        """\ucffc\ub9ac \uc2e4\ud589 \ubc0f \uacb0\uacfc \ubc18\ud658"""\n        conn = self.get_connection()\n        try:\n            cursor = conn.cursor()\n            cursor.execute(query)\n            df = cursor.fetch_pandas_all()\n            return df\n        finally:\n            conn.close()\n\n    def execute_statement(self, statement: str) -> Dict[str, Any]:\n        """SQL \ubb38 \uc2e4\ud589 (INSERT, UPDATE \ub4f1)"""\n        conn = self.get_connection()\n        try:\n            cursor = conn.cursor()\n            cursor.execute(statement)\n            return {\n                "rowcount": cursor.rowcount,\n                "sfqid": cursor.sfqid\n            }\n        finally:\n            conn.close()\n\n    def upload_dataframe(\n        self,\n        df: pd.DataFrame,\n        table_name: str,\n        if_exists: str = "replace"\n    ):\n        """DataFrame\uc744 Snowflake \ud14c\uc774\ube14\uc5d0 \uc5c5\ub85c\ub4dc"""\n        from snowflake.connector.pandas_tools import write_pandas\n\n        conn = self.get_connection()\n        try:\n            success, nchunks, nrows, _ = write_pandas(\n                conn=conn,\n                df=df,\n                table_name=table_name,\n                database=self.database,\n                schema=self.schema,\n                auto_create_table=True,\n                overwrite=(if_exists == "replace")\n            )\n\n            return {\n                "success": success,\n                "nchunks": nchunks,\n                "nrows": nrows\n            }\n        finally:\n            conn.close()\n\n@asset\ndef load_from_snowflake(\n    context: AssetExecutionContext,\n    snowflake: SnowflakeResource\n) -> pd.DataFrame:\n    """Snowflake\uc5d0\uc11c \ub370\uc774\ud130 \ub85c\ub4dc"""\n    query = """\n    SELECT\n        customer_id,\n        customer_name,\n        email,\n        signup_date,\n        total_purchases\n    FROM customers\n    WHERE signup_date >= DATEADD(day, -30, CURRENT_DATE())\n    ORDER BY total_purchases DESC\n    LIMIT 1000\n    """\n\n    context.log.info("Executing Snowflake query...")\n\n    df = snowflake.execute_query(query)\n\n    context.log.info(f"Loaded {len(df)} rows from Snowflake")\n\n    return df\n\n@asset\ndef write_to_snowflake(\n    context: AssetExecutionContext,\n    snowflake: SnowflakeResource,\n    load_from_snowflake: pd.DataFrame\n) -> Output[Dict[str, Any]]:\n    """\ucc98\ub9ac\ub41c \ub370\uc774\ud130\ub97c Snowflake\uc5d0 \uc800\uc7a5"""\n    # \ub370\uc774\ud130 \ucc98\ub9ac\n    df = load_from_snowflake.copy()\n    df["processed_at"] = pd.Timestamp.now()\n    df["customer_segment"] = pd.cut(\n        df["total_purchases"],\n        bins=[0, 1000, 5000, float(\'inf\')],\n        labels=["Basic", "Standard", "Premium"]\n    )\n\n    # Snowflake\uc5d0 \uc800\uc7a5\n    table_name = "processed_customers"\n    result = snowflake.upload_dataframe(\n        df,\n        table_name=table_name,\n        if_exists="replace"\n    )\n\n    context.log.info(\n        f"Uploaded {result[\'nrows\']} rows to {snowflake.schema}.{table_name}"\n    )\n\n    return Output(\n        result,\n        metadata={\n            "table": MetadataValue.text(f"{snowflake.schema}.{table_name}"),\n            "rows_written": MetadataValue.int(result["nrows"]),\n            "success": MetadataValue.bool(result["success"])\n        }\n    )\n\n@asset\ndef snowflake_aggregation(\n    context: AssetExecutionContext,\n    snowflake: SnowflakeResource\n) -> pd.DataFrame:\n    """Snowflake\uc5d0\uc11c \uc9d1\uacc4 \uc218\ud589"""\n    query = """\n    SELECT\n        customer_segment,\n        COUNT(*) as customer_count,\n        AVG(total_purchases) as avg_purchases,\n        SUM(total_purchases) as total_revenue\n    FROM processed_customers\n    GROUP BY customer_segment\n    ORDER BY total_revenue DESC\n    """\n\n    df = snowflake.execute_query(query)\n\n    context.log.info("Aggregation completed")\n\n    return df\n'})}),"\n",(0,r.jsx)(n.h3,{id:"1052-snowflake--dbt-\ud1b5\ud569",children:"10.5.2 Snowflake + dbt \ud1b5\ud569"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'# snowflake_dbt_pipeline.py\n"""\nSnowflake + dbt \ud1b5\ud569 \ud30c\uc774\ud504\ub77c\uc778\n"""\nfrom dagster import (\n    asset,\n    AssetExecutionContext,\n    Definitions,\n    define_asset_job\n)\nfrom dagster_dbt import DbtCliResource, dbt_assets, DbtProject\nimport pandas as pd\n\n# Snowflake \ub9ac\uc18c\uc2a4 (\uc704\uc5d0\uc11c \uc815\uc758\ud55c \uac83 \uc7ac\uc0ac\uc6a9)\nfrom snowflake_integration import SnowflakeResource\n\n@asset\ndef raw_sales_data(\n    context: AssetExecutionContext,\n    snowflake: SnowflakeResource\n) -> Dict[str, Any]:\n    """\uc6d0\uc2dc \ud310\ub9e4 \ub370\uc774\ud130\ub97c Snowflake\uc5d0 \ub85c\ub4dc"""\n    # \uc678\ubd80 \uc18c\uc2a4\uc5d0\uc11c \ub370\uc774\ud130 \uac00\uc838\uc624\uae30\n    df = pd.DataFrame({\n        "sale_id": range(1, 1001),\n        "customer_id": [i % 100 + 1 for i in range(1001)],\n        "product_id": [i % 50 + 1 for i in range(1001)],\n        "sale_date": pd.date_range("2024-01-01", periods=1001, freq=\'H\'),\n        "amount": [100 + (i % 500) for i in range(1001)]\n    })\n\n    # Snowflake\uc5d0 \uc5c5\ub85c\ub4dc\n    result = snowflake.upload_dataframe(\n        df,\n        table_name="raw_sales",\n        if_exists="replace"\n    )\n\n    context.log.info(f"Loaded {result[\'nrows\']} sales records")\n\n    return result\n\n# dbt \ud504\ub85c\uc81d\ud2b8 \uc124\uc815\ndbt_project = DbtProject(project_dir="my_dbt_project")\ndbt_project.prepare_if_dev()\n\n@dbt_assets(\n    manifest=dbt_project.manifest_path,\n    project=dbt_project,\n)\ndef snowflake_analytics_models(\n    context: AssetExecutionContext,\n    dbt: DbtCliResource\n):\n    """dbt \ubd84\uc11d \ubaa8\ub378 (Snowflake \ub370\uc774\ud130 \uc0ac\uc6a9)"""\n    yield from dbt.cli(["build"], context=context).stream()\n\n# \uc804\uccb4 \ud30c\uc774\ud504\ub77c\uc778\nanalytics_pipeline = define_asset_job(\n    name="snowflake_analytics_pipeline",\n    selection=[raw_sales_data, snowflake_analytics_models]\n)\n\ndefs = Definitions(\n    assets=[raw_sales_data, snowflake_analytics_models],\n    jobs=[analytics_pipeline],\n    resources={\n        "snowflake": SnowflakeResource(\n            account="your_account",\n            user="your_user",\n            password="your_password",\n            warehouse="COMPUTE_WH",\n            database="ANALYTICS",\n            schema="PUBLIC"\n        ),\n        "dbt": DbtCliResource(project_dir="my_dbt_project")\n    }\n)\n'})}),"\n",(0,r.jsx)(n.h2,{id:"106-\uc2e4\uc2b5-\ud504\ub85c\uc81d\ud2b8",children:"10.6 \uc2e4\uc2b5 \ud504\ub85c\uc81d\ud2b8"}),"\n",(0,r.jsx)(n.h3,{id:"1061-end-to-end-\ub370\uc774\ud130-\ud30c\uc774\ud504\ub77c\uc778",children:"10.6.1 End-to-End \ub370\uc774\ud130 \ud30c\uc774\ud504\ub77c\uc778"}),"\n",(0,r.jsx)(n.p,{children:"\uc804\uccb4 \ud1b5\ud569\uc744 \ud65c\uc6a9\ud55c \uc885\ud569 \ud30c\uc774\ud504\ub77c\uc778\uc744 \uad6c\ucd95\ud574\ubd05\uc2dc\ub2e4."}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'# complete_pipeline.py\n"""\nAirbyte \u2192 S3 \u2192 dbt \u2192 Snowflake \u2192 BigQuery \ud30c\uc774\ud504\ub77c\uc778\n"""\nfrom dagster import (\n    asset,\n    AssetExecutionContext,\n    Definitions,\n    define_asset_job,\n    ScheduleDefinition,\n    Output,\n    MetadataValue\n)\nfrom dagster_airbyte import AirbyteResource\nfrom dagster_dbt import DbtCliResource, dbt_assets, DbtProject\nimport pandas as pd\n\n# \ub9ac\uc18c\uc2a4 import\nfrom aws.s3_assets import S3Resource\nfrom snowflake_integration import SnowflakeResource\nfrom gcp.bigquery_integration import BigQueryResource\n\n# 1\ub2e8\uacc4: Airbyte\ub85c \ub370\uc774\ud130 \ucd94\ucd9c\n@asset\ndef extract_from_source(\n    context: AssetExecutionContext,\n    airbyte: AirbyteResource\n) -> Output[dict]:\n    """Airbyte\ub85c \uc6d0\ubcf8 \ub370\uc774\ud130 \ucd94\ucd9c"""\n    connection_id = "your-airbyte-connection-id"\n\n    sync_result = airbyte.sync_and_poll(connection_id)\n\n    context.log.info(\n        f"Synced {sync_result.records_synced} records from source"\n    )\n\n    return Output(\n        {"status": "completed", "records": sync_result.records_synced},\n        metadata={\n            "records_synced": MetadataValue.int(sync_result.records_synced),\n            "bytes_synced": MetadataValue.int(sync_result.bytes_synced)\n        }\n    )\n\n# 2\ub2e8\uacc4: S3\uc5d0 \uc6d0\uc2dc \ub370\uc774\ud130 \uc800\uc7a5\n@asset\ndef stage_to_s3(\n    context: AssetExecutionContext,\n    s3: S3Resource,\n    extract_from_source: dict\n) -> Output[str]:\n    """S3\uc5d0 \uc2a4\ud14c\uc774\uc9d5"""\n    # \ub370\uc774\ud130 \ub85c\ub4dc (\uc2e4\uc81c\ub85c\ub294 Airbyte \uacb0\uacfc\uc5d0\uc11c \uac00\uc838\uc634)\n    df = pd.DataFrame({\n        "id": range(1, 1001),\n        "data": [f"data_{i}" for i in range(1, 1001)]\n    })\n\n    key = f"staging/raw_data_{context.run_id}.parquet"\n    s3.upload_dataframe(df, key, format="parquet")\n\n    return Output(\n        key,\n        metadata={\n            "s3_uri": MetadataValue.text(f"s3://{s3.bucket_name}/{key}"),\n            "row_count": MetadataValue.int(len(df))\n        }\n    )\n\n# 3\ub2e8\uacc4: dbt\ub85c \ub370\uc774\ud130 \ubcc0\ud658\ndbt_project = DbtProject(project_dir="my_dbt_project")\ndbt_project.prepare_if_dev()\n\n@dbt_assets(\n    manifest=dbt_project.manifest_path,\n    project=dbt_project,\n)\ndef transform_with_dbt(\n    context: AssetExecutionContext,\n    dbt: DbtCliResource\n):\n    """dbt\ub85c \ub370\uc774\ud130 \ubcc0\ud658"""\n    yield from dbt.cli(["build"], context=context).stream()\n\n# 4\ub2e8\uacc4: Snowflake\uc5d0 \ub85c\ub4dc\n@asset\ndef load_to_snowflake(\n    context: AssetExecutionContext,\n    snowflake: SnowflakeResource,\n    transform_with_dbt\n) -> Output[dict]:\n    """\ubcc0\ud658\ub41c \ub370\uc774\ud130\ub97c Snowflake\uc5d0 \ub85c\ub4dc"""\n    # dbt \uacb0\uacfc \uc77d\uae30\n    query = "SELECT * FROM dbt_prod.fct_orders"\n    df = snowflake.execute_query(query)\n\n    # \ucd94\uac00 \ucc98\ub9ac\n    df["loaded_at"] = pd.Timestamp.now()\n\n    # Snowflake\uc5d0 \ucd5c\uc885 \ud14c\uc774\ube14 \uc0dd\uc131\n    result = snowflake.upload_dataframe(\n        df,\n        table_name="final_orders",\n        if_exists="replace"\n    )\n\n    return Output(\n        result,\n        metadata={\n            "table": MetadataValue.text("PUBLIC.final_orders"),\n            "rows": MetadataValue.int(result["nrows"])\n        }\n    )\n\n# 5\ub2e8\uacc4: BigQuery\ub85c \ubcf5\uc81c (\ubd84\uc11d\uc6a9)\n@asset\ndef replicate_to_bigquery(\n    context: AssetExecutionContext,\n    bigquery_resource: BigQueryResource,\n    snowflake: SnowflakeResource,\n    load_to_snowflake: dict\n) -> Output[str]:\n    """Snowflake \u2192 BigQuery \ubcf5\uc81c"""\n    # Snowflake\uc5d0\uc11c \ub370\uc774\ud130 \uc77d\uae30\n    query = "SELECT * FROM PUBLIC.final_orders"\n    df = snowflake.execute_query(query)\n\n    # BigQuery\uc5d0 \uc5c5\ub85c\ub4dc\n    bigquery_resource.upload_dataframe(\n        df,\n        dataset_id="analytics",\n        table_id="orders"\n    )\n\n    table_ref = f"{bigquery_resource.project_id}.analytics.orders"\n\n    return Output(\n        table_ref,\n        metadata={\n            "table": MetadataValue.text(table_ref),\n            "row_count": MetadataValue.int(len(df))\n        }\n    )\n\n# 6\ub2e8\uacc4: \ub370\uc774\ud130 \ud488\uc9c8 \uccb4\ud06c\n@asset\ndef data_quality_check(\n    context: AssetExecutionContext,\n    bigquery_resource: BigQueryResource,\n    replicate_to_bigquery: str\n) -> Output[dict]:\n    """\ub370\uc774\ud130 \ud488\uc9c8 \uccb4\ud06c"""\n    query = """\n    SELECT\n        COUNT(*) as total_records,\n        COUNT(DISTINCT customer_id) as unique_customers,\n        SUM(CASE WHEN amount < 0 THEN 1 ELSE 0 END) as negative_amounts,\n        SUM(CASE WHEN customer_id IS NULL THEN 1 ELSE 0 END) as null_customers\n    FROM `my-project.analytics.orders`\n    """\n\n    df = bigquery_resource.execute_query(query)\n    result = df.iloc[0].to_dict()\n\n    # \ud488\uc9c8 \uccb4\ud06c\n    issues = []\n    if result["negative_amounts"] > 0:\n        issues.append(f"Found {result[\'negative_amounts\']} negative amounts")\n    if result["null_customers"] > 0:\n        issues.append(f"Found {result[\'null_customers\']} null customers")\n\n    quality_passed = len(issues) == 0\n\n    return Output(\n        {\n            "passed": quality_passed,\n            "issues": issues,\n            **result\n        },\n        metadata={\n            "quality_passed": MetadataValue.bool(quality_passed),\n            "total_records": MetadataValue.int(result["total_records"]),\n            "issues": MetadataValue.md(\n                "\\n".join([f"- {issue}" for issue in issues])\n                if issues else "No issues found"\n            )\n        }\n    )\n\n# Job \uc815\uc758\ncomplete_pipeline_job = define_asset_job(\n    name="complete_etl_pipeline",\n    selection=[\n        extract_from_source,\n        stage_to_s3,\n        transform_with_dbt,\n        load_to_snowflake,\n        replicate_to_bigquery,\n        data_quality_check\n    ]\n)\n\n# \uc2a4\ucf00\uc904 (\ub9e4\uc77c \uc2e4\ud589)\ndaily_pipeline_schedule = ScheduleDefinition(\n    name="daily_complete_pipeline",\n    job=complete_pipeline_job,\n    cron_schedule="0 2 * * *"  # \ub9e4\uc77c \uc624\uc804 2\uc2dc\n)\n\n# Definitions\ndefs = Definitions(\n    assets=[\n        extract_from_source,\n        stage_to_s3,\n        transform_with_dbt,\n        load_to_snowflake,\n        replicate_to_bigquery,\n        data_quality_check\n    ],\n    jobs=[complete_pipeline_job],\n    schedules=[daily_pipeline_schedule],\n    resources={\n        "airbyte": AirbyteResource(\n            host="localhost",\n            port="8000"\n        ),\n        "s3": S3Resource(\n            bucket_name="my-data-bucket",\n            region_name="us-east-1"\n        ),\n        "dbt": DbtCliResource(\n            project_dir="my_dbt_project"\n        ),\n        "snowflake": SnowflakeResource(\n            account="your_account",\n            user="your_user",\n            password="your_password",\n            warehouse="COMPUTE_WH",\n            database="ANALYTICS",\n            schema="PUBLIC"\n        ),\n        "bigquery_resource": BigQueryResource(\n            project_id="my-gcp-project"\n        )\n    }\n)\n'})}),"\n",(0,r.jsx)(n.h3,{id:"1062-\ud1b5\ud569-\ubaa8\ubc94-\uc0ac\ub840",children:"10.6.2 \ud1b5\ud569 \ubaa8\ubc94 \uc0ac\ub840"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'# integration_best_practices.py\n"""\n\ud1b5\ud569 \ubaa8\ubc94 \uc0ac\ub840\n"""\n\nINTEGRATION_BEST_PRACTICES = """\n# Dagster \ud1b5\ud569 \ubaa8\ubc94 \uc0ac\ub840\n\n## 1. dbt \ud1b5\ud569\n- dbt manifest \uc790\ub3d9 \uc0c8\ub85c\uace0\uce68\n- \ubaa8\ub378\ubcc4 \uc120\ud0dd\uc801 \uc2e4\ud589\n- dbt \ud14c\uc2a4\ud2b8\ub97c Asset Check\ub85c \ubcc0\ud658\n- \uc758\uc874\uc131 \uba85\ud655\ud788 \uc815\uc758\n\n## 2. Airbyte \ud1b5\ud569\n- Connection ID \ud658\uacbd \ubcc0\uc218\ub85c \uad00\ub9ac\n- \ub3d9\uae30\ud654 \uc0c1\ud0dc \ubaa8\ub2c8\ud130\ub9c1\n- \uc2e4\ud328 \uc2dc \uc7ac\uc2dc\ub3c4 \ub85c\uc9c1\n- \uc99d\ubd84 \ub3d9\uae30\ud654 \ud65c\uc6a9\n\n## 3. \ud074\ub77c\uc6b0\ub4dc \ud1b5\ud569 (AWS/GCP)\n- \uc790\uaca9 \uc99d\uba85 \uc548\uc804\ud558\uac8c \uad00\ub9ac\n- \ub9ac\uc804 \uc124\uc815 \uba85\ud655\ud788\n- \ube44\uc6a9 \ubaa8\ub2c8\ud130\ub9c1\n- \uc801\uc808\ud55c \uc2a4\ud1a0\ub9ac\uc9c0 \ud074\ub798\uc2a4 \uc120\ud0dd\n\n## 4. Snowflake \ud1b5\ud569\n- \uc6e8\uc5b4\ud558\uc6b0\uc2a4 \ud06c\uae30 \ucd5c\uc801\ud654\n- \ucffc\ub9ac \uacb0\uacfc \uce90\uc2f1 \ud65c\uc6a9\n- \ub370\uc774\ud130 \uacf5\uc720 \uae30\ub2a5 \ud65c\uc6a9\n- \ube44\uc6a9 \ud0dc\uadf8 \uc124\uc815\n\n## 5. \uc77c\ubc18 \uc6d0\uce59\n- \uac01 \ud1b5\ud569\uc744 Resource\ub85c \ucd94\uc0c1\ud654\n- \ud658\uacbd\ubcc4 \uc124\uc815 \ubd84\ub9ac\n- \uc5d0\ub7ec \ucc98\ub9ac \ubc0f \uc7ac\uc2dc\ub3c4\n- \ub85c\uae45 \ubc0f \ubaa8\ub2c8\ud130\ub9c1\n- \ub370\uc774\ud130 \ud488\uc9c8 \uccb4\ud06c\n- \ubb38\uc11c\ud654\n\n## 6. \uc131\ub2a5 \ucd5c\uc801\ud654\n- \ubcd1\ub82c \uc2e4\ud589 \ud65c\uc6a9\n- \ud30c\ud2f0\uc158 \ud65c\uc6a9\n- \uc99d\ubd84 \ucc98\ub9ac\n- \uce90\uc2f1 \uc804\ub7b5\n\n## 7. \ubcf4\uc548\n- \ucd5c\uc18c \uad8c\ud55c \uc6d0\uce59\n- \uc790\uaca9 \uc99d\uba85 \uc554\ud638\ud654\n- \ub124\ud2b8\uc6cc\ud06c \ubcf4\uc548\n- \uac10\uc0ac \ub85c\uadf8\n\n## 8. \ud14c\uc2a4\ud2b8\n- \uac01 \ud1b5\ud569 \ub2e8\uc704 \ud14c\uc2a4\ud2b8\n- Mock \ud65c\uc6a9\n- \ud1b5\ud569 \ud14c\uc2a4\ud2b8\n- End-to-end \ud14c\uc2a4\ud2b8\n"""\n\nprint(INTEGRATION_BEST_PRACTICES)\n'})}),"\n",(0,r.jsx)(n.h2,{id:"107-\uc694\uc57d",children:"10.7 \uc694\uc57d"}),"\n",(0,r.jsx)(n.p,{children:"\uc774 \uc7a5\uc5d0\uc11c\ub294 Dagster\uc640 \ub2e4\uc591\ud55c \ub3c4\uad6c/\ud50c\ub7ab\ud3fc\uc744 \ud1b5\ud569\ud558\ub294 \ubc29\ubc95\uc744 \ud559\uc2b5\ud588\uc2b5\ub2c8\ub2e4:"}),"\n",(0,r.jsx)(n.h3,{id:"\ud575\uc2ec-\uac1c\ub150",children:"\ud575\uc2ec \uac1c\ub150"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"dbt \ud1b5\ud569"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"dbt \ud504\ub85c\uc81d\ud2b8\ub97c Dagster Asset\uc73c\ub85c \ub178\ucd9c"}),"\n",(0,r.jsx)(n.li,{children:"\uc120\ud0dd\uc801 \ubaa8\ub378 \uc2e4\ud589"}),"\n",(0,r.jsx)(n.li,{children:"dbt \ud14c\uc2a4\ud2b8 \ud1b5\ud569"}),"\n",(0,r.jsx)(n.li,{children:"Upstream Dagster Asset \ud65c\uc6a9"}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Airbyte \ud1b5\ud569"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Connection\uc744 Asset\uc73c\ub85c \ubcc0\ud658"}),"\n",(0,r.jsx)(n.li,{children:"\ub3d9\uae30\ud654 \uc0c1\ud0dc \ubaa8\ub2c8\ud130\ub9c1"}),"\n",(0,r.jsx)(n.li,{children:"Airbyte + dbt \ud30c\uc774\ud504\ub77c\uc778"}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"AWS \ud1b5\ud569"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"S3: \ud30c\uc77c \uc800\uc7a5\uc18c"}),"\n",(0,r.jsx)(n.li,{children:"Glue: ETL \uc791\uc5c5"}),"\n",(0,r.jsx)(n.li,{children:"Athena: SQL \ucffc\ub9ac"}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"GCP \ud1b5\ud569"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"BigQuery: \ub370\uc774\ud130 \uc6e8\uc5b4\ud558\uc6b0\uc2a4"}),"\n",(0,r.jsx)(n.li,{children:"GCS: \uac1d\uccb4 \uc2a4\ud1a0\ub9ac\uc9c0"}),"\n",(0,r.jsx)(n.li,{children:"\ucffc\ub9ac \ubc0f \ub370\uc774\ud130 \uc5c5\ub85c\ub4dc"}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Snowflake \ud1b5\ud569"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"\ub370\uc774\ud130 \ub85c\ub4dc/\ucffc\ub9ac"}),"\n",(0,r.jsx)(n.li,{children:"DataFrame \uc5c5\ub85c\ub4dc"}),"\n",(0,r.jsx)(n.li,{children:"dbt\uc640 \ud568\uaed8 \uc0ac\uc6a9"}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"End-to-End \ud30c\uc774\ud504\ub77c\uc778"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"\uc5ec\ub7ec \ud1b5\ud569\uc744 \uc870\ud569"}),"\n",(0,r.jsx)(n.li,{children:"\ub370\uc774\ud130 \ud488\uc9c8 \uccb4\ud06c"}),"\n",(0,r.jsx)(n.li,{children:"\uc2a4\ucf00\uc904\ub9c1"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"best-practices",children:"Best Practices"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"\ucd94\uc0c1\ud654"}),": \uac01 \ud1b5\ud569\uc744 ConfigurableResource\ub85c \uad6c\ud604"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"\ud658\uacbd \ubd84\ub9ac"}),": \uac1c\ubc1c/\uc2a4\ud14c\uc774\uc9d5/\ud504\ub85c\ub355\uc158 \uc124\uc815 \ubd84\ub9ac"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"\uc5d0\ub7ec \ucc98\ub9ac"}),": \uc7ac\uc2dc\ub3c4 \ub85c\uc9c1 \ubc0f \uc54c\ub9bc"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"\ubaa8\ub2c8\ud130\ub9c1"}),": \uac01 \ub2e8\uacc4\uc758 \uba54\ud2b8\ub9ad \uc218\uc9d1"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"\ud14c\uc2a4\ud2b8"}),": Mock\uc744 \ud65c\uc6a9\ud55c \ub2e8\uc704 \ud14c\uc2a4\ud2b8"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"\ubb38\uc11c\ud654"}),": \ud1b5\ud569 \uc124\uc815 \ubc0f \uc0ac\uc6a9\ubc95 \ubb38\uc11c\ud654"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"\ub9c8\ubb34\ub9ac",children:"\ub9c8\ubb34\ub9ac"}),"\n",(0,r.jsx)(n.p,{children:"\uc774\uac83\uc73c\ub85c Dagster\uc758 \ud575\uc2ec \uae30\ub2a5\uacfc \ub2e4\uc591\ud55c \ud1b5\ud569 \ubc29\ubc95\uc744 \ubaa8\ub450 \ud559\uc2b5\ud588\uc2b5\ub2c8\ub2e4. \uc774\uc81c \uc2e4\uc81c \ud504\ub85c\uc81d\ud2b8\uc5d0\uc11c Dagster\ub97c \ud65c\uc6a9\ud558\uc5ec \uacac\uace0\ud558\uace0 \ud655\uc7a5 \uac00\ub2a5\ud55c \ub370\uc774\ud130 \ud30c\uc774\ud504\ub77c\uc778\uc744 \uad6c\ucd95\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4!"}),"\n",(0,r.jsx)(n.p,{children:"Dagster\uc758 \uac15\uc810:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"\ubaa8\ub4c8\ud654"}),": Asset \uae30\ubc18 \uc544\ud0a4\ud14d\ucc98"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"\ud0c0\uc785 \uc548\uc804\uc131"}),": Python \ud0c0\uc785 \ud78c\ud2b8 \ud65c\uc6a9"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"\ud14c\uc2a4\ud2b8 \uac00\ub2a5\uc131"}),": \ub2e8\uc704 \ud14c\uc2a4\ud2b8 \uc6a9\uc774"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"\ud1b5\ud569"}),": \ub2e4\uc591\ud55c \ub3c4\uad6c\uc640 \uc26c\uc6b4 \ud1b5\ud569"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"\ubaa8\ub2c8\ud130\ub9c1"}),": \uac15\ub825\ud55c UI \ubc0f \ub85c\uae45"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"\ud655\uc7a5\uc131"}),": \ud074\ub77c\uc6b0\ub4dc \ud658\uacbd\uc5d0\uc11c \uc27d\uac8c \ud655\uc7a5"]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"\ud589\ubcf5\ud55c \ub370\uc774\ud130 \uc5d4\uc9c0\ub2c8\uc5b4\ub9c1 \ub418\uc138\uc694! \ud83d\ude80"})]})}function u(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(c,{...e})}):c(e)}},8453:(e,n,t)=>{t.d(n,{R:()=>o,x:()=>i});var s=t(6540);const r={},a=s.createContext(r);function o(e){const n=s.useContext(a);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function i(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:o(e.components),s.createElement(a.Provider,{value:n},e.children)}}}]);