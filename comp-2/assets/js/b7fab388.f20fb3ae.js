"use strict";(globalThis.webpackChunkgithub_docs=globalThis.webpackChunkgithub_docs||[]).push([[432],{4249:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>o,contentTitle:()=>i,default:()=>_,frontMatter:()=>d,metadata:()=>a,toc:()=>l});const a=JSON.parse('{"id":"dagster/examples","title":"11. \uc2e4\uc804 \uc608\uc81c","description":"\uc2e4\ubb34\uc5d0\uc11c \ud65c\uc6a9\ud560 \uc218 \uc788\ub294 Dagster \ud30c\uc774\ud504\ub77c\uc778 \uc608\uc81c\ub97c \ub2e4\ub8f9\ub2c8\ub2e4.","source":"@site/docs/dagster/examples.md","sourceDirName":"dagster","slug":"/dagster/examples","permalink":"/docs/dagster/examples","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/dagster/examples.md","tags":[],"version":"current","sidebarPosition":12,"frontMatter":{"sidebar_position":12},"sidebar":"dagsterSidebar","previous":{"title":"10. \ud1b5\ud569","permalink":"/docs/dagster/integrations"}}');var r=t(4848),s=t(8453);const d={sidebar_position:12},i="11. \uc2e4\uc804 \uc608\uc81c",o={},l=[{value:"11.1 ETL \ud30c\uc774\ud504\ub77c\uc778",id:"111-etl-\ud30c\uc774\ud504\ub77c\uc778",level:2},{value:"\uc644\uc804\ud55c \ub370\uc774\ud130 \uc6e8\uc5b4\ud558\uc6b0\uc2a4 ETL",id:"\uc644\uc804\ud55c-\ub370\uc774\ud130-\uc6e8\uc5b4\ud558\uc6b0\uc2a4-etl",level:3},{value:"11.2 \uba38\uc2e0\ub7ec\ub2dd \ud30c\uc774\ud504\ub77c\uc778",id:"112-\uba38\uc2e0\ub7ec\ub2dd-\ud30c\uc774\ud504\ub77c\uc778",level:2},{value:"11.3 \uc2e4\uc2dc\uac04 \ub370\uc774\ud130 \ud30c\uc774\ud504\ub77c\uc778",id:"113-\uc2e4\uc2dc\uac04-\ub370\uc774\ud130-\ud30c\uc774\ud504\ub77c\uc778",level:2},{value:"11.4 \ub370\uc774\ud130 \ud488\uc9c8 \ud30c\uc774\ud504\ub77c\uc778",id:"114-\ub370\uc774\ud130-\ud488\uc9c8-\ud30c\uc774\ud504\ub77c\uc778",level:2},{value:"11.5 Multi-tenant \ub370\uc774\ud130 \ud30c\uc774\ud504\ub77c\uc778",id:"115-multi-tenant-\ub370\uc774\ud130-\ud30c\uc774\ud504\ub77c\uc778",level:2},{value:"11.6 \uc2a4\ucf00\uc904\ub9c1\ub41c \ub9ac\ud3ec\ud2b8 \ud30c\uc774\ud504\ub77c\uc778",id:"116-\uc2a4\ucf00\uc904\ub9c1\ub41c-\ub9ac\ud3ec\ud2b8-\ud30c\uc774\ud504\ub77c\uc778",level:2},{value:"11.7 Data Lake \ud30c\uc774\ud504\ub77c\uc778",id:"117-data-lake-\ud30c\uc774\ud504\ub77c\uc778",level:2},{value:"11.8 \uc2e4\uc2b5 \ud504\ub85c\uc81d\ud2b8: E-commerce Analytics",id:"118-\uc2e4\uc2b5-\ud504\ub85c\uc81d\ud2b8-e-commerce-analytics",level:2}];function c(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",...(0,s.R)(),...e.components},{Details:t}=n;return t||function(e,n){throw new Error("Expected "+(n?"component":"object")+" `"+e+"` to be defined: you likely forgot to import, pass, or provide it.")}("Details",!0),(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.header,{children:(0,r.jsx)(n.h1,{id:"11-\uc2e4\uc804-\uc608\uc81c",children:"11. \uc2e4\uc804 \uc608\uc81c"})}),"\n",(0,r.jsx)(n.p,{children:"\uc2e4\ubb34\uc5d0\uc11c \ud65c\uc6a9\ud560 \uc218 \uc788\ub294 Dagster \ud30c\uc774\ud504\ub77c\uc778 \uc608\uc81c\ub97c \ub2e4\ub8f9\ub2c8\ub2e4."}),"\n",(0,r.jsx)(n.h2,{id:"111-etl-\ud30c\uc774\ud504\ub77c\uc778",children:"11.1 ETL \ud30c\uc774\ud504\ub77c\uc778"}),"\n",(0,r.jsx)(n.h3,{id:"\uc644\uc804\ud55c-\ub370\uc774\ud130-\uc6e8\uc5b4\ud558\uc6b0\uc2a4-etl",children:"\uc644\uc804\ud55c \ub370\uc774\ud130 \uc6e8\uc5b4\ud558\uc6b0\uc2a4 ETL"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'from dagster import asset, AssetExecutionContext, Definitions\nimport pandas as pd\nfrom sqlalchemy import create_engine\n\n# 1. Extract: \ub2e4\uc591\ud55c \uc18c\uc2a4\uc5d0\uc11c \ub370\uc774\ud130 \ucd94\ucd9c\n@asset\ndef raw_orders() -> pd.DataFrame:\n    """\uc8fc\ubb38 \ub370\uc774\ud130 \ucd94\ucd9c (API)"""\n    import requests\n    response = requests.get("https://api.example.com/orders")\n    return pd.DataFrame(response.json())\n\n@asset\ndef raw_customers() -> pd.DataFrame:\n    """\uace0\uac1d \ub370\uc774\ud130 \ucd94\ucd9c (Database)"""\n    engine = create_engine("postgresql://user:pass@localhost/db")\n    return pd.read_sql("SELECT * FROM customers", engine)\n\n@asset\ndef raw_products() -> pd.DataFrame:\n    """\uc81c\ud488 \ub370\uc774\ud130 \ucd94\ucd9c (CSV)"""\n    return pd.read_csv("s3://bucket/products.csv")\n\n# 2. Transform: \ub370\uc774\ud130 \ubcc0\ud658 \ubc0f \uc815\uc81c\n@asset\ndef cleaned_orders(\n    context: AssetExecutionContext,\n    raw_orders: pd.DataFrame\n) -> pd.DataFrame:\n    """\uc8fc\ubb38 \ub370\uc774\ud130 \uc815\uc81c"""\n    df = raw_orders.copy()\n\n    # \uacb0\uce21\uce58 \uc81c\uac70\n    df = df.dropna(subset=[\'order_id\', \'customer_id\'])\n\n    # \ub0a0\uc9dc \ubcc0\ud658\n    df[\'order_date\'] = pd.to_datetime(df[\'order_date\'])\n\n    # \uc911\ubcf5 \uc81c\uac70\n    df = df.drop_duplicates(subset=[\'order_id\'])\n\n    context.log.info(f"Cleaned {len(df)} orders")\n\n    return df\n\n@asset\ndef enriched_orders(\n    cleaned_orders: pd.DataFrame,\n    raw_customers: pd.DataFrame,\n    raw_products: pd.DataFrame\n) -> pd.DataFrame:\n    """\uc8fc\ubb38 \ub370\uc774\ud130 enrichment"""\n    # \uace0\uac1d \uc815\ubcf4 \uc870\uc778\n    df = cleaned_orders.merge(\n        raw_customers[[\'customer_id\', \'customer_name\', \'segment\']],\n        on=\'customer_id\',\n        how=\'left\'\n    )\n\n    # \uc81c\ud488 \uc815\ubcf4 \uc870\uc778\n    df = df.merge(\n        raw_products[[\'product_id\', \'product_name\', \'category\']],\n        on=\'product_id\',\n        how=\'left\'\n    )\n\n    # \ub9e4\ucd9c \uacc4\uc0b0\n    df[\'revenue\'] = df[\'quantity\'] * df[\'unit_price\']\n\n    return df\n\n# 3. Load: \ub370\uc774\ud130 \uc6e8\uc5b4\ud558\uc6b0\uc2a4\uc5d0 \uc801\uc7ac\n@asset\ndef orders_fact_table(\n    context: AssetExecutionContext,\n    enriched_orders: pd.DataFrame\n) -> None:\n    """Fact \ud14c\uc774\ube14 \uc801\uc7ac"""\n    engine = create_engine("postgresql://user:pass@warehouse/dwh")\n\n    enriched_orders.to_sql(\n        \'fact_orders\',\n        engine,\n        if_exists=\'replace\',\n        index=False\n    )\n\n    context.add_output_metadata({\n        "num_rows": len(enriched_orders),\n        "total_revenue": float(enriched_orders[\'revenue\'].sum())\n    })\n'})}),"\n",(0,r.jsx)(n.h2,{id:"112-\uba38\uc2e0\ub7ec\ub2dd-\ud30c\uc774\ud504\ub77c\uc778",children:"11.2 \uba38\uc2e0\ub7ec\ub2dd \ud30c\uc774\ud504\ub77c\uc778"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"from dagster import asset\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nimport joblib\n\n@asset\ndef training_data() -> pd.DataFrame:\n    \"\"\"\ud559\uc2b5 \ub370\uc774\ud130 \ub85c\ub4dc\"\"\"\n    return pd.read_csv(\"data/train.csv\")\n\n@asset\ndef feature_engineering(training_data: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\ud53c\ucc98 \uc5d4\uc9c0\ub2c8\uc5b4\ub9c1\"\"\"\n    df = training_data.copy()\n\n    # \ud30c\uc0dd \ubcc0\uc218 \uc0dd\uc131\n    df['age_group'] = pd.cut(df['age'], bins=[0, 18, 35, 60, 100])\n    df['income_per_age'] = df['income'] / df['age']\n\n    # \uc6d0-\ud56b \uc778\ucf54\ub529\n    df = pd.get_dummies(df, columns=['category', 'region'])\n\n    return df\n\n@asset\ndef train_test_split_data(\n    feature_engineering: pd.DataFrame\n) -> dict:\n    \"\"\"\ud559\uc2b5/\ud14c\uc2a4\ud2b8 \ubd84\ub9ac\"\"\"\n    X = feature_engineering.drop('target', axis=1)\n    y = feature_engineering['target']\n\n    X_train, X_test, y_train, y_test = train_test_split(\n        X, y, test_size=0.2, random_state=42\n    )\n\n    return {\n        'X_train': X_train,\n        'X_test': X_test,\n        'y_train': y_train,\n        'y_test': y_test\n    }\n\n@asset\ndef trained_model(\n    context: AssetExecutionContext,\n    train_test_split_data: dict\n) -> RandomForestClassifier:\n    \"\"\"\ubaa8\ub378 \ud559\uc2b5\"\"\"\n    X_train = train_test_split_data['X_train']\n    y_train = train_test_split_data['y_train']\n\n    model = RandomForestClassifier(n_estimators=100, random_state=42)\n    model.fit(X_train, y_train)\n\n    # \ubaa8\ub378 \uc800\uc7a5\n    joblib.dump(model, 'models/rf_model.pkl')\n\n    context.log.info(\"Model trained successfully\")\n\n    return model\n\n@asset\ndef model_evaluation(\n    context: AssetExecutionContext,\n    trained_model: RandomForestClassifier,\n    train_test_split_data: dict\n) -> dict:\n    \"\"\"\ubaa8\ub378 \ud3c9\uac00\"\"\"\n    X_test = train_test_split_data['X_test']\n    y_test = train_test_split_data['y_test']\n\n    from sklearn.metrics import accuracy_score, precision_score, recall_score\n\n    y_pred = trained_model.predict(X_test)\n\n    metrics = {\n        'accuracy': accuracy_score(y_test, y_pred),\n        'precision': precision_score(y_test, y_pred, average='weighted'),\n        'recall': recall_score(y_test, y_pred, average='weighted')\n    }\n\n    context.add_output_metadata(metrics)\n\n    return metrics\n"})}),"\n",(0,r.jsx)(n.h2,{id:"113-\uc2e4\uc2dc\uac04-\ub370\uc774\ud130-\ud30c\uc774\ud504\ub77c\uc778",children:"11.3 \uc2e4\uc2dc\uac04 \ub370\uc774\ud130 \ud30c\uc774\ud504\ub77c\uc778"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'from dagster import asset, sensor, RunRequest, SensorEvaluationContext\nimport os\nfrom datetime import datetime\n\n@asset\ndef process_new_file(context: AssetExecutionContext) -> dict:\n    """\uc0c8 \ud30c\uc77c \ucc98\ub9ac"""\n    filename = context.run.tags.get("filename")\n\n    # \ud30c\uc77c \uc77d\uae30\n    df = pd.read_csv(f"uploads/{filename}")\n\n    # \ucc98\ub9ac\n    processed = df.groupby(\'category\')[\'amount\'].sum()\n\n    # \uacb0\uacfc \uc800\uc7a5\n    output_file = f"processed/{filename}"\n    processed.to_csv(output_file)\n\n    return {\n        "input_file": filename,\n        "output_file": output_file,\n        "processed_at": datetime.now().isoformat()\n    }\n\n@sensor(job=process_new_file_job)\ndef file_upload_sensor(context: SensorEvaluationContext):\n    """\uc0c8 \ud30c\uc77c \uac10\uc9c0 \uc13c\uc11c"""\n    upload_dir = "uploads/"\n\n    # \uc774\uc804 \uc2e4\ud589 \uc774\ud6c4 \uc0c8 \ud30c\uc77c \ud655\uc778\n    last_run = context.cursor or "0"\n\n    new_files = []\n    for filename in os.listdir(upload_dir):\n        file_path = os.path.join(upload_dir, filename)\n        file_mtime = os.path.getmtime(file_path)\n\n        if file_mtime > float(last_run):\n            new_files.append({\n                "filename": filename,\n                "mtime": file_mtime\n            })\n\n    if new_files:\n        # \uac00\uc7a5 \ucd5c\uadfc \ud30c\uc77c \uc2dc\uac04\uc744 cursor\ub85c \uc800\uc7a5\n        latest_mtime = max(f["mtime"] for f in new_files)\n        context.update_cursor(str(latest_mtime))\n\n        # \uac01 \ud30c\uc77c\uc5d0 \ub300\ud574 RunRequest \uc0dd\uc131\n        for file_info in new_files:\n            yield RunRequest(\n                run_key=file_info["filename"],\n                tags={"filename": file_info["filename"]}\n            )\n'})}),"\n",(0,r.jsx)(n.h2,{id:"114-\ub370\uc774\ud130-\ud488\uc9c8-\ud30c\uc774\ud504\ub77c\uc778",children:"11.4 \ub370\uc774\ud130 \ud488\uc9c8 \ud30c\uc774\ud504\ub77c\uc778"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'from dagster import asset, AssetCheckResult, asset_check\n\n@asset\ndef customer_data() -> pd.DataFrame:\n    """\uace0\uac1d \ub370\uc774\ud130"""\n    return pd.read_csv("customers.csv")\n\n@asset_check(asset=customer_data)\ndef check_no_nulls(customer_data: pd.DataFrame) -> AssetCheckResult:\n    """\ud544\uc218 \uceec\ub7fc NULL \uccb4\ud06c"""\n    required_columns = [\'customer_id\', \'email\', \'name\']\n\n    for col in required_columns:\n        null_count = customer_data[col].isnull().sum()\n        if null_count > 0:\n            return AssetCheckResult(\n                passed=False,\n                metadata={\n                    "column": col,\n                    "null_count": null_count\n                }\n            )\n\n    return AssetCheckResult(passed=True)\n\n@asset_check(asset=customer_data)\ndef check_email_format(customer_data: pd.DataFrame) -> AssetCheckResult:\n    """\uc774\uba54\uc77c \ud615\uc2dd \uac80\uc99d"""\n    import re\n    email_pattern = r\'^[\\w\\.-]+@[\\w\\.-]+\\.\\w+$\'\n\n    invalid_emails = customer_data[\n        ~customer_data[\'email\'].str.match(email_pattern)\n    ]\n\n    if len(invalid_emails) > 0:\n        return AssetCheckResult(\n            passed=False,\n            metadata={\n                "invalid_count": len(invalid_emails),\n                "sample": invalid_emails[\'email\'].head().tolist()\n            }\n        )\n\n    return AssetCheckResult(passed=True)\n\n@asset_check(asset=customer_data)\ndef check_duplicate_ids(customer_data: pd.DataFrame) -> AssetCheckResult:\n    """\uc911\ubcf5 ID \uccb4\ud06c"""\n    duplicates = customer_data[\n        customer_data.duplicated(subset=[\'customer_id\'], keep=False)\n    ]\n\n    if len(duplicates) > 0:\n        return AssetCheckResult(\n            passed=False,\n            metadata={"duplicate_count": len(duplicates)}\n        )\n\n    return AssetCheckResult(passed=True)\n'})}),"\n",(0,r.jsx)(n.h2,{id:"115-multi-tenant-\ub370\uc774\ud130-\ud30c\uc774\ud504\ub77c\uc778",children:"11.5 Multi-tenant \ub370\uc774\ud130 \ud30c\uc774\ud504\ub77c\uc778"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'from dagster import StaticPartitionsDefinition, asset, multi_asset, AssetOut\n\n# \uace0\uac1d\uc0ac\ubcc4 partition\ntenants = StaticPartitionsDefinition(["tenant_a", "tenant_b", "tenant_c"])\n\n@asset(partitions_def=tenants)\ndef tenant_raw_data(context: AssetExecutionContext) -> pd.DataFrame:\n    """\uace0\uac1d\uc0ac\ubcc4 \uc6d0\uc2dc \ub370\uc774\ud130"""\n    tenant_id = context.partition_key\n\n    # \uace0\uac1d\uc0ac\ubcc4 \ub370\uc774\ud130 \ub85c\ub4dc\n    return pd.read_csv(f"data/{tenant_id}/raw_data.csv")\n\n@asset(partitions_def=tenants)\ndef tenant_processed_data(\n    context: AssetExecutionContext,\n    tenant_raw_data: pd.DataFrame\n) -> pd.DataFrame:\n    """\uace0\uac1d\uc0ac\ubcc4 \ub370\uc774\ud130 \ucc98\ub9ac"""\n    tenant_id = context.partition_key\n\n    df = tenant_raw_data.copy()\n\n    # \uace0\uac1d\uc0ac\ubcc4 \ube44\uc988\ub2c8\uc2a4 \ub85c\uc9c1\n    tenant_config = load_tenant_config(tenant_id)\n    df = apply_tenant_rules(df, tenant_config)\n\n    return df\n\n@asset\ndef aggregated_report(context: AssetExecutionContext) -> pd.DataFrame:\n    """\uc804\uccb4 \uace0\uac1d\uc0ac \uc9d1\uacc4 \ub9ac\ud3ec\ud2b8"""\n    all_data = []\n\n    # \ubaa8\ub4e0 \uace0\uac1d\uc0ac \ub370\uc774\ud130 \ub85c\ub4dc\n    for tenant_id in ["tenant_a", "tenant_b", "tenant_c"]:\n        df = load_partition_data(tenant_id)\n        df[\'tenant\'] = tenant_id\n        all_data.append(df)\n\n    # \ud1b5\ud569 \ub9ac\ud3ec\ud2b8\n    combined = pd.concat(all_data, ignore_index=True)\n\n    report = combined.groupby(\'tenant\').agg({\n        \'revenue\': \'sum\',\n        \'users\': \'count\'\n    }).reset_index()\n\n    return report\n'})}),"\n",(0,r.jsx)(n.h2,{id:"116-\uc2a4\ucf00\uc904\ub9c1\ub41c-\ub9ac\ud3ec\ud2b8-\ud30c\uc774\ud504\ub77c\uc778",children:"11.6 \uc2a4\ucf00\uc904\ub9c1\ub41c \ub9ac\ud3ec\ud2b8 \ud30c\uc774\ud504\ub77c\uc778"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'from dagster import (\n    asset,\n    ScheduleDefinition,\n    define_asset_job,\n    DailyPartitionsDefinition\n)\n\ndaily = DailyPartitionsDefinition(start_date="2024-01-01")\n\n@asset(partitions_def=daily)\ndef daily_metrics(context: AssetExecutionContext) -> dict:\n    """\uc77c\ubcc4 \uba54\ud2b8\ub9ad \uc218\uc9d1"""\n    date = context.partition_key\n\n    return {\n        "date": date,\n        "revenue": calculate_daily_revenue(date),\n        "users": count_active_users(date),\n        "orders": count_orders(date)\n    }\n\n@asset(partitions_def=daily)\ndef daily_report(\n    context: AssetExecutionContext,\n    daily_metrics: dict\n) -> str:\n    """\uc77c\ubcc4 \ub9ac\ud3ec\ud2b8 \uc0dd\uc131"""\n    import smtplib\n    from email.mime.text import MIMEText\n\n    # \ub9ac\ud3ec\ud2b8 \uc0dd\uc131\n    report_html = f"""\n    <h1>Daily Report - {daily_metrics[\'date\']}</h1>\n    <ul>\n        <li>Revenue: ${daily_metrics[\'revenue\']:,.2f}</li>\n        <li>Active Users: {daily_metrics[\'users\']}</li>\n        <li>Orders: {daily_metrics[\'orders\']}</li>\n    </ul>\n    """\n\n    # \uc774\uba54\uc77c \ubc1c\uc1a1\n    send_email("team@company.com", "Daily Report", report_html)\n\n    return report_html\n\n# \ub9e4\uc77c \uc790\uc815 \uc2e4\ud589\ndaily_report_schedule = ScheduleDefinition(\n    job=define_asset_job("daily_report_job", selection=[daily_report]),\n    cron_schedule="0 0 * * *"  # \ub9e4\uc77c \uc790\uc815\n)\n'})}),"\n",(0,r.jsx)(n.h2,{id:"117-data-lake-\ud30c\uc774\ud504\ub77c\uc778",children:"11.7 Data Lake \ud30c\uc774\ud504\ub77c\uc778"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"from dagster import asset, multi_asset, AssetOut\nimport boto3\n\n@asset\ndef raw_events_bronze() -> pd.DataFrame:\n    \"\"\"Bronze Layer: \uc6d0\uc2dc \uc774\ubca4\ud2b8 \ub370\uc774\ud130\"\"\"\n    # S3\uc5d0\uc11c \uc6d0\uc2dc JSON \ub85c\ub4dc\n    s3 = boto3.client('s3')\n    response = s3.get_object(Bucket='datalake', Key='raw/events.json')\n\n    import json\n    events = json.loads(response['Body'].read())\n    return pd.DataFrame(events)\n\n@asset\ndef cleaned_events_silver(raw_events_bronze: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"Silver Layer: \uc815\uc81c\ub41c \ub370\uc774\ud130\"\"\"\n    df = raw_events_bronze.copy()\n\n    # \ub370\uc774\ud130 \uc815\uc81c\n    df = df.dropna()\n    df['timestamp'] = pd.to_datetime(df['timestamp'])\n    df = df.drop_duplicates(subset=['event_id'])\n\n    # Parquet\uc73c\ub85c S3 \uc800\uc7a5\n    df.to_parquet('s3://datalake/silver/events.parquet')\n\n    return df\n\n@multi_asset(\n    outs={\n        \"user_metrics_gold\": AssetOut(),\n        \"product_metrics_gold\": AssetOut()\n    }\n)\ndef aggregated_metrics_gold(\n    cleaned_events_silver: pd.DataFrame\n) -> tuple:\n    \"\"\"Gold Layer: \ube44\uc988\ub2c8\uc2a4 \uba54\ud2b8\ub9ad\"\"\"\n\n    # \uc0ac\uc6a9\uc790\ubcc4 \uba54\ud2b8\ub9ad\n    user_metrics = cleaned_events_silver.groupby('user_id').agg({\n        'event_id': 'count',\n        'value': 'sum'\n    }).reset_index()\n\n    # \uc81c\ud488\ubcc4 \uba54\ud2b8\ub9ad\n    product_metrics = cleaned_events_silver.groupby('product_id').agg({\n        'event_id': 'count',\n        'value': 'sum'\n    }).reset_index()\n\n    return user_metrics, product_metrics\n"})}),"\n",(0,r.jsx)(n.h2,{id:"118-\uc2e4\uc2b5-\ud504\ub85c\uc81d\ud2b8-e-commerce-analytics",children:"11.8 \uc2e4\uc2b5 \ud504\ub85c\uc81d\ud2b8: E-commerce Analytics"}),"\n",(0,r.jsx)(n.p,{children:"\uc644\uc804\ud55c E-commerce \ubd84\uc11d \ud50c\ub7ab\ud3fc\uc744 \uad6c\ucd95\ud558\uc138\uc694."}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"\uc694\uad6c\uc0ac\ud56d:"})}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsx)(n.li,{children:"\uc8fc\ubb38, \uace0\uac1d, \uc81c\ud488 \ub370\uc774\ud130 \ucd94\ucd9c"}),"\n",(0,r.jsx)(n.li,{children:"\ub370\uc774\ud130 \uc815\uc81c \ubc0f \ud1b5\ud569"}),"\n",(0,r.jsx)(n.li,{children:"\uc77c\ubcc4 \ub9e4\ucd9c \uc9d1\uacc4 (partitioned)"}),"\n",(0,r.jsx)(n.li,{children:"\uace0\uac1d \uc138\uadf8\uba3c\ud2b8 \ubd84\uc11d"}),"\n",(0,r.jsx)(n.li,{children:"\ucd94\ucc9c \uc5d4\uc9c4\uc6a9 \ub370\uc774\ud130 \uc900\ube44"}),"\n",(0,r.jsx)(n.li,{children:"\ub9e4\uc77c \uc790\uc815 \ub9ac\ud3ec\ud2b8 \uc774\uba54\uc77c \ubc1c\uc1a1"}),"\n"]}),"\n",(0,r.jsxs)(t,{children:[(0,r.jsx)("summary",{children:"\uc804\uccb4 \ucf54\ub4dc"}),(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"from dagster import (\n    asset,\n    AssetExecutionContext,\n    DailyPartitionsDefinition,\n    ScheduleDefinition,\n    define_asset_job,\n    Definitions\n)\nimport pandas as pd\n\ndaily = DailyPartitionsDefinition(start_date=\"2024-01-01\")\n\n# 1. Extract\n@asset\ndef raw_orders() -> pd.DataFrame:\n    return pd.read_csv(\"data/orders.csv\")\n\n@asset\ndef raw_customers() -> pd.DataFrame:\n    return pd.read_csv(\"data/customers.csv\")\n\n@asset\ndef raw_products() -> pd.DataFrame:\n    return pd.read_csv(\"data/products.csv\")\n\n# 2. Transform\n@asset\ndef cleaned_orders(raw_orders: pd.DataFrame) -> pd.DataFrame:\n    df = raw_orders.dropna().drop_duplicates()\n    df['order_date'] = pd.to_datetime(df['order_date'])\n    return df\n\n@asset\ndef integrated_orders(\n    cleaned_orders: pd.DataFrame,\n    raw_customers: pd.DataFrame,\n    raw_products: pd.DataFrame\n) -> pd.DataFrame:\n    df = cleaned_orders.merge(raw_customers, on='customer_id')\n    df = df.merge(raw_products, on='product_id')\n    df['revenue'] = df['quantity'] * df['price']\n    return df\n\n# 3. Aggregate\n@asset(partitions_def=daily)\ndef daily_sales(\n    context: AssetExecutionContext,\n    integrated_orders: pd.DataFrame\n) -> pd.DataFrame:\n    date = context.partition_key\n\n    daily_data = integrated_orders[\n        integrated_orders['order_date'].dt.strftime('%Y-%m-%d') == date\n    ]\n\n    return daily_data.groupby('product_id').agg({\n        'revenue': 'sum',\n        'quantity': 'sum'\n    }).reset_index()\n\n# 4. Analytics\n@asset\ndef customer_segments(integrated_orders: pd.DataFrame) -> pd.DataFrame:\n    customer_stats = integrated_orders.groupby('customer_id').agg({\n        'revenue': 'sum',\n        'order_id': 'count'\n    }).reset_index()\n\n    customer_stats['segment'] = pd.cut(\n        customer_stats['revenue'],\n        bins=[0, 100, 500, float('inf')],\n        labels=['Low', 'Medium', 'High']\n    )\n\n    return customer_stats\n\n# 5. ML Prep\n@asset\ndef recommendation_data(integrated_orders: pd.DataFrame) -> pd.DataFrame:\n    return integrated_orders.pivot_table(\n        index='customer_id',\n        columns='product_id',\n        values='quantity',\n        fill_value=0\n    )\n\n# 6. Reporting\n@asset(partitions_def=daily)\ndef daily_report(\n    context: AssetExecutionContext,\n    daily_sales: pd.DataFrame\n) -> dict:\n    date = context.partition_key\n\n    report = {\n        \"date\": date,\n        \"total_revenue\": daily_sales['revenue'].sum(),\n        \"total_orders\": daily_sales['quantity'].sum(),\n        \"top_products\": daily_sales.nlargest(5, 'revenue').to_dict()\n    }\n\n    # \uc774\uba54\uc77c \ubc1c\uc1a1\n    send_daily_report_email(report)\n\n    return report\n\n# Definitions\nreport_job = define_asset_job(\"daily_report_job\", selection=[daily_report])\n\ndaily_schedule = ScheduleDefinition(\n    job=report_job,\n    cron_schedule=\"0 0 * * *\"\n)\n\ndefs = Definitions(\n    assets=[\n        raw_orders,\n        raw_customers,\n        raw_products,\n        cleaned_orders,\n        integrated_orders,\n        daily_sales,\n        customer_segments,\n        recommendation_data,\n        daily_report\n    ],\n    schedules=[daily_schedule]\n)\n"})})]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsxs)(n.p,{children:["\uc774\uc81c ",(0,r.jsx)(n.a,{href:"./intro",children:"Dagster \uae30\ucd08"}),"\ubd80\ud130 \uc2dc\uc791\ud558\uc5ec \uc2e4\uc804 \ud30c\uc774\ud504\ub77c\uc778\uc744 \uad6c\ucd95\ud574\ubcf4\uc138\uc694!"]})]})}function _(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(c,{...e})}):c(e)}},8453:(e,n,t)=>{t.d(n,{R:()=>d,x:()=>i});var a=t(6540);const r={},s=a.createContext(r);function d(e){const n=a.useContext(s);return a.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function i(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:d(e.components),a.createElement(s.Provider,{value:n},e.children)}}}]);