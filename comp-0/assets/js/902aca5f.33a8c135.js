"use strict";(globalThis.webpackChunkgithub_docs=globalThis.webpackChunkgithub_docs||[]).push([[8630],{337:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>o,default:()=>p,frontMatter:()=>r,metadata:()=>s,toc:()=>m});const s=JSON.parse('{"id":"fastapi/examples/scenario-14-ai-ml-service","title":"\uc2dc\ub098\ub9ac\uc624 14: AI/ML \uc11c\ube44\uc2a4 API","description":"AI \ubaa8\ub378\uc744 \ud65c\uc6a9\ud55c \uc778\uacf5\uc9c0\ub2a5 \uc11c\ube44\uc2a4 API\ub97c \ub9cc\ub4e4\uc5b4\ubd05\uc2dc\ub2e4!","source":"@site/docs/fastapi/examples/scenario-14-ai-ml-service.md","sourceDirName":"fastapi/examples","slug":"/fastapi/examples/scenario-14-ai-ml-service","permalink":"/tobias-docs/comp-0/docs/fastapi/examples/scenario-14-ai-ml-service","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/fastapi/examples/scenario-14-ai-ml-service.md","tags":[],"version":"current","sidebarPosition":15,"frontMatter":{"sidebar_position":15},"sidebar":"fastapiSidebar","previous":{"title":"\uc2dc\ub098\ub9ac\uc624 13: \ub370\uc774\ud130 \ubd84\uc11d \ub300\uc2dc\ubcf4\ub4dc API","permalink":"/tobias-docs/comp-0/docs/fastapi/examples/scenario-13-analytics-dashboard"},"next":{"title":"\uc2dc\ub098\ub9ac\uc624 15: \uc6f9 \ud06c\ub864\ub9c1/\uc2a4\ud06c\ub798\ud551 API","permalink":"/tobias-docs/comp-0/docs/fastapi/examples/scenario-15-web-scraper"}}');var a=t(4848),i=t(8453);const r={sidebar_position:15},o="\uc2dc\ub098\ub9ac\uc624 14: AI/ML \uc11c\ube44\uc2a4 API",l={},m=[{value:"\ud83d\udccc \uae30\ub2a5",id:"-\uae30\ub2a5",level:2},{value:"\ud83d\udcc1 \ud504\ub85c\uc81d\ud2b8 \uad6c\uc870",id:"-\ud504\ub85c\uc81d\ud2b8-\uad6c\uc870",level:2},{value:"\ud83d\udcca \uc2dc\ud000\uc2a4 \ub2e4\uc774\uc5b4\uadf8\ub7a8",id:"-\uc2dc\ud000\uc2a4-\ub2e4\uc774\uc5b4\uadf8\ub7a8",level:2},{value:"\ud83d\udcdd \ud575\uc2ec \ucf54\ub4dc",id:"-\ud575\uc2ec-\ucf54\ub4dc",level:2},{value:"models.py",id:"modelspy",level:3},{value:"image_classifier.py",id:"image_classifierpy",level:3},{value:"sentiment_analyzer.py",id:"sentiment_analyzerpy",level:3},{value:"text_generator.py",id:"text_generatorpy",level:3},{value:"translator.py",id:"translatorpy",level:3},{value:"openai_service.py",id:"openai_servicepy",level:3},{value:"main.py",id:"mainpy",level:3},{value:"schemas.py",id:"schemaspy",level:3},{value:"\ud83d\udd11 \ud575\uc2ec \uac1c\ub150",id:"-\ud575\uc2ec-\uac1c\ub150",level:2},{value:"Hugging Face Transformers",id:"hugging-face-transformers",level:3},{value:"\ubaa8\ub378 \ub85c\ub529 \ucd5c\uc801\ud654",id:"\ubaa8\ub378-\ub85c\ub529-\ucd5c\uc801\ud654",level:3},{value:"\ube44\ub3d9\uae30 \ucc98\ub9ac",id:"\ube44\ub3d9\uae30-\ucc98\ub9ac",level:3},{value:"\ubc30\uce58 \ucc98\ub9ac",id:"\ubc30\uce58-\ucc98\ub9ac",level:3},{value:"\ud83d\udcda \ub2e4\uc74c \ub2e8\uacc4",id:"-\ub2e4\uc74c-\ub2e8\uacc4",level:2}];function d(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",p:"p",pre:"pre",ul:"ul",...(0,i.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(n.header,{children:(0,a.jsx)(n.h1,{id:"\uc2dc\ub098\ub9ac\uc624-14-aiml-\uc11c\ube44\uc2a4-api",children:"\uc2dc\ub098\ub9ac\uc624 14: AI/ML \uc11c\ube44\uc2a4 API"})}),"\n",(0,a.jsx)(n.p,{children:"AI \ubaa8\ub378\uc744 \ud65c\uc6a9\ud55c \uc778\uacf5\uc9c0\ub2a5 \uc11c\ube44\uc2a4 API\ub97c \ub9cc\ub4e4\uc5b4\ubd05\uc2dc\ub2e4!"}),"\n",(0,a.jsx)(n.h2,{id:"-\uae30\ub2a5",children:"\ud83d\udccc \uae30\ub2a5"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"\u2705 \uc774\ubbf8\uc9c0 \ubd84\ub958 (Image Classification)"}),"\n",(0,a.jsx)(n.li,{children:"\u2705 \ud14d\uc2a4\ud2b8 \uac10\uc815 \ubd84\uc11d (Sentiment Analysis)"}),"\n",(0,a.jsx)(n.li,{children:"\u2705 \ud14d\uc2a4\ud2b8 \uc0dd\uc131 (Text Generation)"}),"\n",(0,a.jsx)(n.li,{children:"\u2705 \uc774\ubbf8\uc9c0 \uc0dd\uc131 (Stable Diffusion)"}),"\n",(0,a.jsx)(n.li,{children:"\u2705 \uc74c\uc131-\ud14d\uc2a4\ud2b8 \ubcc0\ud658 (Speech-to-Text)"}),"\n",(0,a.jsx)(n.li,{children:"\u2705 \ubc88\uc5ed (Translation)"}),"\n",(0,a.jsx)(n.li,{children:"\u2705 \ubaa8\ub378 \ubc84\uc804 \uad00\ub9ac"}),"\n",(0,a.jsx)(n.li,{children:"\u2705 \ubc30\uce58 \ucc98\ub9ac"}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"-\ud504\ub85c\uc81d\ud2b8-\uad6c\uc870",children:"\ud83d\udcc1 \ud504\ub85c\uc81d\ud2b8 \uad6c\uc870"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{children:"ai-ml-service/\n\u251c\u2500\u2500 main.py                 # FastAPI \uc560\ud50c\ub9ac\ucf00\uc774\uc158\n\u251c\u2500\u2500 models.py               # DB \ubaa8\ub378 (MLTask)\n\u251c\u2500\u2500 schemas.py              # Pydantic \uc2a4\ud0a4\ub9c8\n\u251c\u2500\u2500 database.py             # \ub370\uc774\ud130\ubca0\uc774\uc2a4 \uc5f0\uacb0\n\u251c\u2500\u2500 ml_models/\n\u2502   \u251c\u2500\u2500 image_classifier.py # \uc774\ubbf8\uc9c0 \ubd84\ub958 (ViT)\n\u2502   \u251c\u2500\u2500 sentiment_analyzer.py # \uac10\uc815 \ubd84\uc11d (BERT)\n\u2502   \u251c\u2500\u2500 text_generator.py   # \ud14d\uc2a4\ud2b8 \uc0dd\uc131 (GPT)\n\u2502   \u2514\u2500\u2500 translator.py       # \ubc88\uc5ed (MarianMT)\n\u251c\u2500\u2500 task_processor.py       # \ube44\ub3d9\uae30 \uc791\uc5c5 \ucc98\ub9ac\n\u2514\u2500\u2500 requirements.txt        # \uc758\uc874\uc131 \ud328\ud0a4\uc9c0\n"})}),"\n",(0,a.jsx)(n.h2,{id:"-\uc2dc\ud000\uc2a4-\ub2e4\uc774\uc5b4\uadf8\ub7a8",children:"\ud83d\udcca \uc2dc\ud000\uc2a4 \ub2e4\uc774\uc5b4\uadf8\ub7a8"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-mermaid",children:"sequenceDiagram\n    participant U as User\n    participant API as FastAPI\n    participant TP as TaskProcessor\n    participant ML as ML Model\n    participant DB as Database\n\n    Note over U,DB: 1. \uc774\ubbf8\uc9c0 \ubd84\ub958 (\ub3d9\uae30)\n    U->>API: POST /classify/image<br/>multipart file\n    API->>TP: classify_image()\n    TP->>ML: Load model (cached)\n    ML--\x3e>TP: Model ready\n    TP->>ML: predict(image)\n    ML->>ML: Forward pass\n    ML--\x3e>TP: Predictions + confidence\n    TP->>DB: INSERT ml_task (completed)\n    TP--\x3e>API: Classification results\n    API--\x3e>U: 200 OK + Results\n\n    Note over U,DB: 2. \ud14d\uc2a4\ud2b8 \uc0dd\uc131 (\ube44\ub3d9\uae30)\n    U->>API: POST /generate/text<br/>{prompt, max_length}\n    API->>DB: INSERT ml_task (pending)\n    DB--\x3e>API: Task ID\n    API--\x3e>U: 202 Accepted + task_id\n\n    API->>TP: Queue task (background)\n    TP->>ML: Load GPT model\n    TP->>ML: generate(prompt)\n    ML->>ML: Generate text (slow)\n    ML--\x3e>TP: Generated text\n    TP->>DB: UPDATE ml_task (completed)\n\n    U->>API: GET /tasks/{task_id}\n    API->>DB: SELECT ml_task\n    DB--\x3e>API: Task status + result\n    API--\x3e>U: 200 OK + Generated text\n\n    Note over U,DB: 3. \ubc30\uce58 \ucc98\ub9ac\n    U->>API: POST /batch/sentiment<br/>{texts: [...]\n    API->>DB: INSERT multiple tasks\n    loop For each text\n        TP->>ML: analyze_sentiment()\n        ML--\x3e>TP: Sentiment score\n        TP->>DB: UPDATE task status\n    end\n"})}),"\n",(0,a.jsx)(n.h2,{id:"-\ud575\uc2ec-\ucf54\ub4dc",children:"\ud83d\udcdd \ud575\uc2ec \ucf54\ub4dc"}),"\n",(0,a.jsx)(n.h3,{id:"modelspy",children:"models.py"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'from sqlalchemy import Column, Integer, String, DateTime, Text, Float, JSON, Enum as SQLEnum\nfrom sqlalchemy.sql import func\nfrom database import Base\nimport enum\n\nclass TaskStatus(str, enum.Enum):\n    pending = "pending"\n    processing = "processing"\n    completed = "completed"\n    failed = "failed"\n\nclass MLTask(Base):\n    """ML \uc791\uc5c5"""\n    __tablename__ = "ml_tasks"\n\n    id = Column(Integer, primary_key=True)\n    task_type = Column(String, index=True)  # classify, sentiment, generate, etc.\n    status = Column(SQLEnum(TaskStatus), default=TaskStatus.pending, index=True)\n\n    # \uc785\ub825\n    input_data = Column(Text)\n    input_file = Column(String, nullable=True)\n\n    # \ucd9c\ub825\n    output_data = Column(JSON, nullable=True)\n    output_file = Column(String, nullable=True)\n\n    # \ubaa8\ub378 \uc815\ubcf4\n    model_name = Column(String)\n    model_version = Column(String)\n\n    # \uba54\ud0c0\ub370\uc774\ud130\n    confidence_score = Column(Float, nullable=True)\n    processing_time = Column(Float, nullable=True)  # \ucd08\n    error_message = Column(Text, nullable=True)\n\n    created_at = Column(DateTime(timezone=True), server_default=func.now())\n    completed_at = Column(DateTime(timezone=True), nullable=True)\n'})}),"\n",(0,a.jsx)(n.h3,{id:"image_classifierpy",children:"image_classifier.py"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'from transformers import AutoImageProcessor, AutoModelForImageClassification\nfrom PIL import Image\nimport torch\n\nclass ImageClassifier:\n    def __init__(self, model_name: str = "google/vit-base-patch16-224"):\n        """\uc774\ubbf8\uc9c0 \ubd84\ub958 \ubaa8\ub378 \ub85c\ub4dc"""\n        self.processor = AutoImageProcessor.from_pretrained(model_name)\n        self.model = AutoModelForImageClassification.from_pretrained(model_name)\n        self.model_name = model_name\n\n    def classify(self, image: Image.Image, top_k: int = 5) -> list:\n        """\uc774\ubbf8\uc9c0 \ubd84\ub958"""\n        inputs = self.processor(images=image, return_tensors="pt")\n\n        with torch.no_grad():\n            outputs = self.model(**inputs)\n\n        logits = outputs.logits\n        probabilities = torch.nn.functional.softmax(logits, dim=-1)\n\n        # Top K \uacb0\uacfc\n        top_probs, top_indices = torch.topk(probabilities, top_k)\n\n        results = []\n        for prob, idx in zip(top_probs[0], top_indices[0]):\n            label = self.model.config.id2label[idx.item()]\n            results.append({\n                "label": label,\n                "confidence": float(prob)\n            })\n\n        return results\n'})}),"\n",(0,a.jsx)(n.h3,{id:"sentiment_analyzerpy",children:"sentiment_analyzer.py"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'from transformers import pipeline\n\nclass SentimentAnalyzer:\n    def __init__(self, model_name: str = "distilbert-base-uncased-finetuned-sst-2-english"):\n        """\uac10\uc815 \ubd84\uc11d \ubaa8\ub378 \ub85c\ub4dc"""\n        self.classifier = pipeline(\n            "sentiment-analysis",\n            model=model_name,\n            tokenizer=model_name\n        )\n        self.model_name = model_name\n\n    def analyze(self, text: str) -> dict:\n        """\uac10\uc815 \ubd84\uc11d"""\n        result = self.classifier(text)[0]\n\n        return {\n            "sentiment": result[\'label\'],\n            "confidence": float(result[\'score\'])\n        }\n\n    def analyze_batch(self, texts: list) -> list:\n        """\ubc30\uce58 \uac10\uc815 \ubd84\uc11d"""\n        results = self.classifier(texts)\n\n        return [\n            {\n                "text": text,\n                "sentiment": result[\'label\'],\n                "confidence": float(result[\'score\'])\n            }\n            for text, result in zip(texts, results)\n        ]\n'})}),"\n",(0,a.jsx)(n.h3,{id:"text_generatorpy",children:"text_generator.py"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'from transformers import AutoTokenizer, AutoModelForCausalLM\nimport torch\n\nclass TextGenerator:\n    def __init__(self, model_name: str = "gpt2"):\n        """\ud14d\uc2a4\ud2b8 \uc0dd\uc131 \ubaa8\ub378 \ub85c\ub4dc"""\n        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n        self.model = AutoModelForCausalLM.from_pretrained(model_name)\n        self.model_name = model_name\n\n        # Pad token \uc124\uc815\n        if self.tokenizer.pad_token is None:\n            self.tokenizer.pad_token = self.tokenizer.eos_token\n\n    def generate(\n        self,\n        prompt: str,\n        max_length: int = 100,\n        temperature: float = 0.7,\n        top_p: float = 0.9,\n        num_return_sequences: int = 1\n    ) -> list:\n        """\ud14d\uc2a4\ud2b8 \uc0dd\uc131"""\n        inputs = self.tokenizer(prompt, return_tensors="pt")\n\n        with torch.no_grad():\n            outputs = self.model.generate(\n                **inputs,\n                max_length=max_length,\n                temperature=temperature,\n                top_p=top_p,\n                num_return_sequences=num_return_sequences,\n                do_sample=True,\n                pad_token_id=self.tokenizer.eos_token_id\n            )\n\n        results = []\n        for output in outputs:\n            text = self.tokenizer.decode(output, skip_special_tokens=True)\n            results.append(text)\n\n        return results\n'})}),"\n",(0,a.jsx)(n.h3,{id:"translatorpy",children:"translator.py"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'from transformers import MarianMTModel, MarianTokenizer\n\nclass Translator:\n    def __init__(self, source_lang: str = "en", target_lang: str = "ko"):\n        """\ubc88\uc5ed \ubaa8\ub378 \ub85c\ub4dc"""\n        model_name = f"Helsinki-NLP/opus-mt-{source_lang}-{target_lang}"\n        self.tokenizer = MarianTokenizer.from_pretrained(model_name)\n        self.model = MarianMTModel.from_pretrained(model_name)\n        self.model_name = model_name\n        self.source_lang = source_lang\n        self.target_lang = target_lang\n\n    def translate(self, text: str) -> str:\n        """\ud14d\uc2a4\ud2b8 \ubc88\uc5ed"""\n        inputs = self.tokenizer(text, return_tensors="pt", padding=True)\n\n        with torch.no_grad():\n            outputs = self.model.generate(**inputs)\n\n        translated = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n        return translated\n\n    def translate_batch(self, texts: list) -> list:\n        """\ubc30\uce58 \ubc88\uc5ed"""\n        inputs = self.tokenizer(texts, return_tensors="pt", padding=True, truncation=True)\n\n        with torch.no_grad():\n            outputs = self.model.generate(**inputs)\n\n        translated = [\n            self.tokenizer.decode(output, skip_special_tokens=True)\n            for output in outputs\n        ]\n\n        return translated\n'})}),"\n",(0,a.jsx)(n.h3,{id:"openai_servicepy",children:"openai_service.py"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'import openai\nimport os\nfrom typing import List\n\nclass OpenAIService:\n    def __init__(self):\n        openai.api_key = os.getenv(\'OPENAI_API_KEY\')\n        self.enabled = bool(openai.api_key)\n\n    def chat_completion(\n        self,\n        messages: List[dict],\n        model: str = "gpt-3.5-turbo",\n        temperature: float = 0.7,\n        max_tokens: int = 500\n    ) -> dict:\n        """ChatGPT API \ud638\ucd9c"""\n        if not self.enabled:\n            return {"error": "OpenAI API key not configured"}\n\n        response = openai.ChatCompletion.create(\n            model=model,\n            messages=messages,\n            temperature=temperature,\n            max_tokens=max_tokens\n        )\n\n        return {\n            "response": response.choices[0].message.content,\n            "usage": {\n                "prompt_tokens": response.usage.prompt_tokens,\n                "completion_tokens": response.usage.completion_tokens,\n                "total_tokens": response.usage.total_tokens\n            }\n        }\n\n    def generate_image(\n        self,\n        prompt: str,\n        n: int = 1,\n        size: str = "512x512"\n    ) -> list:\n        """DALL-E \uc774\ubbf8\uc9c0 \uc0dd\uc131"""\n        if not self.enabled:\n            return {"error": "OpenAI API key not configured"}\n\n        response = openai.Image.create(\n            prompt=prompt,\n            n=n,\n            size=size\n        )\n\n        return [img[\'url\'] for img in response.data]\n\n    def transcribe_audio(self, audio_file) -> str:\n        """Whisper \uc74c\uc131-\ud14d\uc2a4\ud2b8 \ubcc0\ud658"""\n        if not self.enabled:\n            return {"error": "OpenAI API key not configured"}\n\n        transcript = openai.Audio.transcribe(\n            model="whisper-1",\n            file=audio_file\n        )\n\n        return transcript.text\n'})}),"\n",(0,a.jsx)(n.h3,{id:"mainpy",children:"main.py"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'from fastapi import FastAPI, File, UploadFile, Depends, BackgroundTasks, HTTPException\nfrom sqlalchemy.orm import Session\nfrom typing import List, Optional\nfrom PIL import Image\nimport models, schemas\nfrom database import engine, get_db\nfrom image_classifier import ImageClassifier\nfrom sentiment_analyzer import SentimentAnalyzer\nfrom text_generator import TextGenerator\nfrom translator import Translator\nfrom openai_service import OpenAIService\nimport io\nimport time\nfrom datetime import datetime\n\nmodels.Base.metadata.create_all(bind=engine)\n\napp = FastAPI(title="AI/ML Service API")\n\n# \ubaa8\ub378 \ub85c\ub4dc (\uc2f1\uae00\ud1a4)\nimage_classifier = ImageClassifier()\nsentiment_analyzer = SentimentAnalyzer()\ntext_generator = TextGenerator()\ntranslator_en_ko = Translator("en", "ko")\nopenai_service = OpenAIService()\n\n# ==================== \uc774\ubbf8\uc9c0 \ubd84\ub958 ====================\n\n@app.post("/ai/classify-image")\nasync def classify_image(\n    file: UploadFile = File(...),\n    top_k: int = 5,\n    background_tasks: BackgroundTasks,\n    db: Session = Depends(get_db)\n):\n    """\uc774\ubbf8\uc9c0 \ubd84\ub958"""\n    # \uc791\uc5c5 \uc0dd\uc131\n    task = models.MLTask(\n        task_type="classify_image",\n        status=models.TaskStatus.processing,\n        model_name=image_classifier.model_name\n    )\n    db.add(task)\n    db.commit()\n    db.refresh(task)\n\n    # \ubc31\uadf8\ub77c\uc6b4\ub4dc\uc5d0\uc11c \ucc98\ub9ac\n    background_tasks.add_task(\n        process_image_classification,\n        task.id,\n        file,\n        top_k,\n        db\n    )\n\n    return {"task_id": task.id, "status": "processing"}\n\nasync def process_image_classification(\n    task_id: int,\n    file: UploadFile,\n    top_k: int,\n    db: Session\n):\n    """\uc774\ubbf8\uc9c0 \ubd84\ub958 \ucc98\ub9ac"""\n    task = db.query(models.MLTask).filter(models.MLTask.id == task_id).first()\n\n    try:\n        start_time = time.time()\n\n        # \uc774\ubbf8\uc9c0 \uc77d\uae30\n        image_bytes = await file.read()\n        image = Image.open(io.BytesIO(image_bytes))\n\n        # \ubd84\ub958\n        results = image_classifier.classify(image, top_k)\n\n        processing_time = time.time() - start_time\n\n        # \uacb0\uacfc \uc800\uc7a5\n        task.status = models.TaskStatus.completed\n        task.output_data = {"predictions": results}\n        task.confidence_score = results[0][\'confidence\']\n        task.processing_time = processing_time\n        task.completed_at = datetime.utcnow()\n\n    except Exception as e:\n        task.status = models.TaskStatus.failed\n        task.error_message = str(e)\n\n    db.commit()\n\n# ==================== \uac10\uc815 \ubd84\uc11d ====================\n\n@app.post("/ai/sentiment-analysis")\ndef analyze_sentiment(\n    request: schemas.SentimentRequest,\n    db: Session = Depends(get_db)\n):\n    """\uac10\uc815 \ubd84\uc11d"""\n    start_time = time.time()\n\n    result = sentiment_analyzer.analyze(request.text)\n    processing_time = time.time() - start_time\n\n    # \uc791\uc5c5 \uc800\uc7a5\n    task = models.MLTask(\n        task_type="sentiment_analysis",\n        status=models.TaskStatus.completed,\n        input_data=request.text,\n        output_data=result,\n        model_name=sentiment_analyzer.model_name,\n        confidence_score=result[\'confidence\'],\n        processing_time=processing_time,\n        completed_at=datetime.utcnow()\n    )\n    db.add(task)\n    db.commit()\n\n    return {\n        "task_id": task.id,\n        "result": result,\n        "processing_time": processing_time\n    }\n\n@app.post("/ai/sentiment-analysis/batch")\ndef analyze_sentiment_batch(\n    request: schemas.BatchSentimentRequest,\n    db: Session = Depends(get_db)\n):\n    """\ubc30\uce58 \uac10\uc815 \ubd84\uc11d"""\n    start_time = time.time()\n\n    results = sentiment_analyzer.analyze_batch(request.texts)\n    processing_time = time.time() - start_time\n\n    return {\n        "results": results,\n        "processing_time": processing_time,\n        "total_items": len(results)\n    }\n\n# ==================== \ud14d\uc2a4\ud2b8 \uc0dd\uc131 ====================\n\n@app.post("/ai/generate-text")\ndef generate_text(\n    request: schemas.TextGenerationRequest,\n    db: Session = Depends(get_db)\n):\n    """\ud14d\uc2a4\ud2b8 \uc0dd\uc131"""\n    start_time = time.time()\n\n    results = text_generator.generate(\n        prompt=request.prompt,\n        max_length=request.max_length,\n        temperature=request.temperature,\n        num_return_sequences=request.num_sequences\n    )\n\n    processing_time = time.time() - start_time\n\n    # \uc791\uc5c5 \uc800\uc7a5\n    task = models.MLTask(\n        task_type="text_generation",\n        status=models.TaskStatus.completed,\n        input_data=request.prompt,\n        output_data={"generated_texts": results},\n        model_name=text_generator.model_name,\n        processing_time=processing_time,\n        completed_at=datetime.utcnow()\n    )\n    db.add(task)\n    db.commit()\n\n    return {\n        "task_id": task.id,\n        "generated_texts": results,\n        "processing_time": processing_time\n    }\n\n# ==================== \ubc88\uc5ed ====================\n\n@app.post("/ai/translate")\ndef translate_text(\n    request: schemas.TranslationRequest,\n    db: Session = Depends(get_db)\n):\n    """\ud14d\uc2a4\ud2b8 \ubc88\uc5ed"""\n    start_time = time.time()\n\n    translated = translator_en_ko.translate(request.text)\n    processing_time = time.time() - start_time\n\n    # \uc791\uc5c5 \uc800\uc7a5\n    task = models.MLTask(\n        task_type="translation",\n        status=models.TaskStatus.completed,\n        input_data=request.text,\n        output_data={"translated": translated},\n        model_name=translator_en_ko.model_name,\n        processing_time=processing_time,\n        completed_at=datetime.utcnow()\n    )\n    db.add(task)\n    db.commit()\n\n    return {\n        "task_id": task.id,\n        "source_text": request.text,\n        "translated_text": translated,\n        "source_lang": "en",\n        "target_lang": "ko",\n        "processing_time": processing_time\n    }\n\n@app.post("/ai/translate/batch")\ndef translate_batch(\n    request: schemas.BatchTranslationRequest,\n    db: Session = Depends(get_db)\n):\n    """\ubc30\uce58 \ubc88\uc5ed"""\n    start_time = time.time()\n\n    translated = translator_en_ko.translate_batch(request.texts)\n    processing_time = time.time() - start_time\n\n    return {\n        "results": [\n            {"source": src, "translated": tgt}\n            for src, tgt in zip(request.texts, translated)\n        ],\n        "processing_time": processing_time\n    }\n\n# ==================== OpenAI \uc11c\ube44\uc2a4 ====================\n\n@app.post("/ai/chat")\ndef chat_completion(request: schemas.ChatRequest):\n    """ChatGPT \ub300\ud654"""\n    result = openai_service.chat_completion(\n        messages=request.messages,\n        model=request.model,\n        temperature=request.temperature,\n        max_tokens=request.max_tokens\n    )\n\n    return result\n\n@app.post("/ai/generate-image")\ndef generate_image(request: schemas.ImageGenerationRequest):\n    """DALL-E \uc774\ubbf8\uc9c0 \uc0dd\uc131"""\n    image_urls = openai_service.generate_image(\n        prompt=request.prompt,\n        n=request.n,\n        size=request.size\n    )\n\n    return {"images": image_urls}\n\n@app.post("/ai/transcribe")\nasync def transcribe_audio(file: UploadFile = File(...)):\n    """Whisper \uc74c\uc131-\ud14d\uc2a4\ud2b8 \ubcc0\ud658"""\n    transcript = openai_service.transcribe_audio(file.file)\n\n    return {"transcript": transcript}\n\n# ==================== \uc791\uc5c5 \uc870\ud68c ====================\n\n@app.get("/ai/tasks/{task_id}")\ndef get_task(task_id: int, db: Session = Depends(get_db)):\n    """\uc791\uc5c5 \uc870\ud68c"""\n    task = db.query(models.MLTask).filter(models.MLTask.id == task_id).first()\n\n    if not task:\n        raise HTTPException(status_code=404, detail="Task not found")\n\n    return task\n\n@app.get("/ai/tasks")\ndef list_tasks(\n    task_type: Optional[str] = None,\n    status: Optional[str] = None,\n    skip: int = 0,\n    limit: int = 20,\n    db: Session = Depends(get_db)\n):\n    """\uc791\uc5c5 \ubaa9\ub85d"""\n    query = db.query(models.MLTask)\n\n    if task_type:\n        query = query.filter(models.MLTask.task_type == task_type)\n\n    if status:\n        query = query.filter(models.MLTask.status == status)\n\n    tasks = query.order_by(\n        models.MLTask.created_at.desc()\n    ).offset(skip).limit(limit).all()\n\n    return tasks\n\n# ==================== \ubaa8\ub378 \uc815\ubcf4 ====================\n\n@app.get("/ai/models")\ndef list_models():\n    """\uc0ac\uc6a9 \uac00\ub2a5\ud55c \ubaa8\ub378 \ubaa9\ub85d"""\n    return {\n        "models": [\n            {\n                "task": "image_classification",\n                "name": image_classifier.model_name,\n                "description": "Vision Transformer for image classification"\n            },\n            {\n                "task": "sentiment_analysis",\n                "name": sentiment_analyzer.model_name,\n                "description": "DistilBERT for sentiment analysis"\n            },\n            {\n                "task": "text_generation",\n                "name": text_generator.model_name,\n                "description": "GPT-2 for text generation"\n            },\n            {\n                "task": "translation",\n                "name": translator_en_ko.model_name,\n                "description": "Marian MT for English to Korean translation"\n            }\n        ]\n    }\n'})}),"\n",(0,a.jsx)(n.h3,{id:"schemaspy",children:"schemas.py"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'from pydantic import BaseModel\nfrom typing import List, Optional\n\nclass SentimentRequest(BaseModel):\n    text: str\n\nclass BatchSentimentRequest(BaseModel):\n    texts: List[str]\n\nclass TextGenerationRequest(BaseModel):\n    prompt: str\n    max_length: int = 100\n    temperature: float = 0.7\n    num_sequences: int = 1\n\nclass TranslationRequest(BaseModel):\n    text: str\n\nclass BatchTranslationRequest(BaseModel):\n    texts: List[str]\n\nclass ChatRequest(BaseModel):\n    messages: List[dict]\n    model: str = "gpt-3.5-turbo"\n    temperature: float = 0.7\n    max_tokens: int = 500\n\nclass ImageGenerationRequest(BaseModel):\n    prompt: str\n    n: int = 1\n    size: str = "512x512"\n'})}),"\n",(0,a.jsx)(n.h2,{id:"-\ud575\uc2ec-\uac1c\ub150",children:"\ud83d\udd11 \ud575\uc2ec \uac1c\ub150"}),"\n",(0,a.jsx)(n.h3,{id:"hugging-face-transformers",children:"Hugging Face Transformers"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"pipeline"}),": \uac04\ub2e8\ud55c \uace0\uc218\uc900 API"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"AutoModel"}),": \uc720\uc5f0\ud55c \uc800\uc218\uc900 API"]}),"\n",(0,a.jsx)(n.li,{children:"\ub2e4\uc591\ud55c \uc0ac\uc804 \ud559\uc2b5 \ubaa8\ub378 \ud65c\uc6a9"}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"\ubaa8\ub378-\ub85c\ub529-\ucd5c\uc801\ud654",children:"\ubaa8\ub378 \ub85c\ub529 \ucd5c\uc801\ud654"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"\uc571 \uc2dc\uc791 \uc2dc \ud55c \ubc88\ub9cc \ub85c\ub4dc (\uc2f1\uae00\ud1a4)"}),"\n",(0,a.jsx)(n.li,{children:"GPU \uc0ac\uc6a9 \uac00\ub2a5 \uc2dc \uc790\ub3d9 \ud65c\uc6a9"}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"torch.no_grad()"}),"\ub85c \uba54\ubaa8\ub9ac \uc808\uc57d"]}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"\ube44\ub3d9\uae30-\ucc98\ub9ac",children:"\ube44\ub3d9\uae30 \ucc98\ub9ac"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"BackgroundTasks\ub85c \ubb34\uac70\uc6b4 \uc791\uc5c5 \ucc98\ub9ac"}),"\n",(0,a.jsx)(n.li,{children:"\uc791\uc5c5 \uc0c1\ud0dc \ucd94\uc801"}),"\n",(0,a.jsx)(n.li,{children:"\uacb0\uacfc \uc870\ud68c API"}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"\ubc30\uce58-\ucc98\ub9ac",children:"\ubc30\uce58 \ucc98\ub9ac"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"\uc5ec\ub7ec \uc785\ub825\uc744 \ud55c \ubc88\uc5d0 \ucc98\ub9ac"}),"\n",(0,a.jsx)(n.li,{children:"\ud6a8\uc728\uc801\uc778 GPU \ud65c\uc6a9"}),"\n",(0,a.jsx)(n.li,{children:"\ucc98\ub9ac \uc2dc\uac04 \ub2e8\ucd95"}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"-\ub2e4\uc74c-\ub2e8\uacc4",children:"\ud83d\udcda \ub2e4\uc74c \ub2e8\uacc4"}),"\n",(0,a.jsxs)(n.p,{children:["\ud83d\udc49 ",(0,a.jsx)(n.a,{href:"./scenario-15-web-scraper",children:"\ud06c\ub864\ub9c1/\uc2a4\ud06c\ub798\ud551 API"})]})]})}function p(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,a.jsx)(n,{...e,children:(0,a.jsx)(d,{...e})}):d(e)}},8453:(e,n,t)=>{t.d(n,{R:()=>r,x:()=>o});var s=t(6540);const a={},i=s.createContext(a);function r(e){const n=s.useContext(i);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:r(e.components),s.createElement(i.Provider,{value:n},e.children)}}}]);