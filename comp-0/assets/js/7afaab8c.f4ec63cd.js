"use strict";(globalThis.webpackChunkgithub_docs=globalThis.webpackChunkgithub_docs||[]).push([[5386],{8453:(e,n,s)=>{s.d(n,{R:()=>i,x:()=>o});var t=s(6540);const a={},r=t.createContext(a);function i(e){const n=t.useContext(r);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:i(e.components),t.createElement(r.Provider,{value:n},e.children)}},9336:(e,n,s)=>{s.r(n),s.d(n,{assets:()=>l,contentTitle:()=>o,default:()=>c,frontMatter:()=>i,metadata:()=>t,toc:()=>d});const t=JSON.parse('{"id":"fastapi/examples/scenario-16-streaming-pipeline","title":"\uc2dc\ub098\ub9ac\uc624 16: \uc2a4\ud2b8\ub9ac\ubc0d \ub370\uc774\ud130 \ud30c\uc774\ud504\ub77c\uc778","description":"\uc2e4\uc2dc\uac04 \ub370\uc774\ud130 \uc2a4\ud2b8\ub9ac\ubc0d \ubc0f \ucc98\ub9ac \ud30c\uc774\ud504\ub77c\uc778\uc744 \ub9cc\ub4e4\uc5b4\ubd05\uc2dc\ub2e4!","source":"@site/docs/fastapi/examples/scenario-16-streaming-pipeline.md","sourceDirName":"fastapi/examples","slug":"/fastapi/examples/scenario-16-streaming-pipeline","permalink":"/tobias-docs/comp-0/docs/fastapi/examples/scenario-16-streaming-pipeline","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/fastapi/examples/scenario-16-streaming-pipeline.md","tags":[],"version":"current","sidebarPosition":17,"frontMatter":{"sidebar_position":17},"sidebar":"fastapiSidebar","previous":{"title":"\uc2dc\ub098\ub9ac\uc624 15: \uc6f9 \ud06c\ub864\ub9c1/\uc2a4\ud06c\ub798\ud551 API","permalink":"/tobias-docs/comp-0/docs/fastapi/examples/scenario-15-web-scraper"},"next":{"title":"\uc911\uae09\ud3b8 \uc18c\uac1c","permalink":"/tobias-docs/comp-0/docs/fastapi/intermediate/intro"}}');var a=s(4848),r=s(8453);const i={sidebar_position:17},o="\uc2dc\ub098\ub9ac\uc624 16: \uc2a4\ud2b8\ub9ac\ubc0d \ub370\uc774\ud130 \ud30c\uc774\ud504\ub77c\uc778",l={},d=[{value:"\ud83d\udccc \uae30\ub2a5",id:"-\uae30\ub2a5",level:2},{value:"\ud83d\udcc1 \ud504\ub85c\uc81d\ud2b8 \uad6c\uc870",id:"-\ud504\ub85c\uc81d\ud2b8-\uad6c\uc870",level:2},{value:"\ud83d\udcca \uc2dc\ud000\uc2a4 \ub2e4\uc774\uc5b4\uadf8\ub7a8",id:"-\uc2dc\ud000\uc2a4-\ub2e4\uc774\uc5b4\uadf8\ub7a8",level:2},{value:"\ud83d\udcdd \ud575\uc2ec \ucf54\ub4dc",id:"-\ud575\uc2ec-\ucf54\ub4dc",level:2},{value:"models.py",id:"modelspy",level:3},{value:"sse_streamer.py",id:"sse_streamerpy",level:3},{value:"kafka_consumer.py",id:"kafka_consumerpy",level:3},{value:"redis_stream.py",id:"redis_streampy",level:3},{value:"pipeline_processor.py",id:"pipeline_processorpy",level:3},{value:"main.py",id:"mainpy",level:3},{value:"\ud83d\udd11 \ud575\uc2ec \uac1c\ub150",id:"-\ud575\uc2ec-\uac1c\ub150",level:2},{value:"Server-Sent Events (SSE)",id:"server-sent-events-sse",level:3},{value:"Redis Streams",id:"redis-streams",level:3},{value:"\ub370\uc774\ud130 \ud30c\uc774\ud504\ub77c\uc778",id:"\ub370\uc774\ud130-\ud30c\uc774\ud504\ub77c\uc778",level:3},{value:"\uc2a4\ud2b8\ub9ac\ubc0d \ud328\ud134",id:"\uc2a4\ud2b8\ub9ac\ubc0d-\ud328\ud134",level:3},{value:"\ud83d\udcda \ud559\uc2b5 \uc644\ub8cc!",id:"-\ud559\uc2b5-\uc644\ub8cc",level:2}];function m(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",p:"p",pre:"pre",ul:"ul",...(0,r.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(n.header,{children:(0,a.jsx)(n.h1,{id:"\uc2dc\ub098\ub9ac\uc624-16-\uc2a4\ud2b8\ub9ac\ubc0d-\ub370\uc774\ud130-\ud30c\uc774\ud504\ub77c\uc778",children:"\uc2dc\ub098\ub9ac\uc624 16: \uc2a4\ud2b8\ub9ac\ubc0d \ub370\uc774\ud130 \ud30c\uc774\ud504\ub77c\uc778"})}),"\n",(0,a.jsx)(n.p,{children:"\uc2e4\uc2dc\uac04 \ub370\uc774\ud130 \uc2a4\ud2b8\ub9ac\ubc0d \ubc0f \ucc98\ub9ac \ud30c\uc774\ud504\ub77c\uc778\uc744 \ub9cc\ub4e4\uc5b4\ubd05\uc2dc\ub2e4!"}),"\n",(0,a.jsx)(n.h2,{id:"-\uae30\ub2a5",children:"\ud83d\udccc \uae30\ub2a5"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"\u2705 Server-Sent Events (SSE)"}),"\n",(0,a.jsx)(n.li,{children:"\u2705 \uc2e4\uc2dc\uac04 \ub85c\uadf8 \uc2a4\ud2b8\ub9ac\ubc0d"}),"\n",(0,a.jsx)(n.li,{children:"\u2705 \ub370\uc774\ud130 \ud30c\uc774\ud504\ub77c\uc778 \ucc98\ub9ac"}),"\n",(0,a.jsx)(n.li,{children:"\u2705 Apache Kafka \uc5f0\ub3d9"}),"\n",(0,a.jsx)(n.li,{children:"\u2705 Redis Streams \ud65c\uc6a9"}),"\n",(0,a.jsx)(n.li,{children:"\u2705 \uba54\ud2b8\ub9ad \uc218\uc9d1 \ubc0f \ubaa8\ub2c8\ud130\ub9c1"}),"\n",(0,a.jsx)(n.li,{children:"\u2705 \ubc31\ud504\ub808\uc154 \ucc98\ub9ac"}),"\n",(0,a.jsx)(n.li,{children:"\u2705 \uc2a4\ud2b8\ub9bc \ubcc0\ud658 \ubc0f \ud544\ud130\ub9c1"}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"-\ud504\ub85c\uc81d\ud2b8-\uad6c\uc870",children:"\ud83d\udcc1 \ud504\ub85c\uc81d\ud2b8 \uad6c\uc870"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{children:"streaming-pipeline/\n\u251c\u2500\u2500 main.py                 # FastAPI \uc560\ud50c\ub9ac\ucf00\uc774\uc158\n\u251c\u2500\u2500 models.py               # DB \ubaa8\ub378 (DataStream, StreamMetric)\n\u251c\u2500\u2500 schemas.py              # Pydantic \uc2a4\ud0a4\ub9c8\n\u251c\u2500\u2500 database.py             # \ub370\uc774\ud130\ubca0\uc774\uc2a4 \uc5f0\uacb0\n\u251c\u2500\u2500 streaming/\n\u2502   \u251c\u2500\u2500 sse_streamer.py     # Server-Sent Events\n\u2502   \u251c\u2500\u2500 kafka_producer.py   # Kafka \ud504\ub85c\ub4c0\uc11c\n\u2502   \u251c\u2500\u2500 kafka_consumer.py   # Kafka \ucee8\uc288\uba38\n\u2502   \u251c\u2500\u2500 redis_streamer.py   # Redis Streams\n\u2502   \u2514\u2500\u2500 pipeline_processor.py # \ud30c\uc774\ud504\ub77c\uc778 \ucc98\ub9ac\n\u251c\u2500\u2500 metrics_collector.py    # \uba54\ud2b8\ub9ad \uc218\uc9d1\n\u2514\u2500\u2500 requirements.txt        # \uc758\uc874\uc131 \ud328\ud0a4\uc9c0\n"})}),"\n",(0,a.jsx)(n.h2,{id:"-\uc2dc\ud000\uc2a4-\ub2e4\uc774\uc5b4\uadf8\ub7a8",children:"\ud83d\udcca \uc2dc\ud000\uc2a4 \ub2e4\uc774\uc5b4\uadf8\ub7a8"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-mermaid",children:"sequenceDiagram\n    participant C as Client\n    participant API as FastAPI\n    participant SSE as SSE Streamer\n    participant KP as Kafka Producer\n    participant Kafka as Apache Kafka\n    participant KC as Kafka Consumer\n    participant PP as PipelineProcessor\n    participant Redis as Redis Streams\n    participant DB as Database\n\n    Note over C,DB: 1. SSE \uc2e4\uc2dc\uac04 \uc2a4\ud2b8\ub9ac\ubc0d\n    C->>API: GET /stream/logs (SSE)\n    API->>SSE: Start SSE connection\n    SSE--\x3e>C: Connection established\n    loop Continuous stream\n        SSE->>SSE: Generate/receive event\n        SSE--\x3e>C: data: {log_entry}\\n\\n\n    end\n\n    Note over C,DB: 2. Kafka \ud30c\uc774\ud504\ub77c\uc778\n    C->>API: POST /events<br/>{event_data}\n    API->>KP: produce_event()\n    KP->>Kafka: Send to topic\n    Kafka--\x3e>KP: Acknowledged\n    KP--\x3e>API: Event sent\n    API--\x3e>C: 202 Accepted\n\n    KC->>Kafka: Subscribe to topic\n    loop Consume events\n        Kafka--\x3e>KC: Event batch\n        KC->>PP: process_events(batch)\n        PP->>PP: Transform data\n        PP->>PP: Filter events\n        PP->>PP: Aggregate metrics\n        PP->>DB: Batch INSERT\n        PP->>Redis: Publish to Redis Stream\n    end\n\n    Note over C,DB: 3. \uc2e4\uc2dc\uac04 \uba54\ud2b8\ub9ad \uc870\ud68c\n    C->>API: GET /metrics/realtime (SSE)\n    API->>Redis: Subscribe to stream\n    loop Stream data\n        Redis--\x3e>API: New metric data\n        API--\x3e>C: data: {metrics}\\n\\n\n    end\n\n    Note over C,DB: 4. \ubc31\ud504\ub808\uc154 \ucc98\ub9ac\n    alt Consumer slow\n        KC->>KC: Check lag\n        KC->>KC: Apply backpressure\n        KC->>Kafka: Pause consumption\n        KC->>KC: Process backlog\n        KC->>Kafka: Resume consumption\n    end\n"})}),"\n",(0,a.jsx)(n.h2,{id:"-\ud575\uc2ec-\ucf54\ub4dc",children:"\ud83d\udcdd \ud575\uc2ec \ucf54\ub4dc"}),"\n",(0,a.jsx)(n.h3,{id:"modelspy",children:"models.py"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'from sqlalchemy import Column, Integer, String, DateTime, Float, JSON, Boolean\nfrom sqlalchemy.sql import func\nfrom database import Base\n\nclass DataStream(Base):\n    """\ub370\uc774\ud130 \uc2a4\ud2b8\ub9bc"""\n    __tablename__ = "data_streams"\n\n    id = Column(Integer, primary_key=True)\n    name = Column(String, unique=True, index=True)\n    stream_type = Column(String)  # "kafka", "redis", "sse"\n    topic = Column(String)\n    is_active = Column(Boolean, default=True)\n\n    # \uc124\uc815\n    config = Column(JSON)\n\n    # \uba54\ud2b8\ub9ad\n    messages_count = Column(Integer, default=0)\n    last_message_at = Column(DateTime(timezone=True), nullable=True)\n\n    created_at = Column(DateTime(timezone=True), server_default=func.now())\n\nclass StreamMetric(Base):\n    """\uc2a4\ud2b8\ub9bc \uba54\ud2b8\ub9ad"""\n    __tablename__ = "stream_metrics"\n\n    id = Column(Integer, primary_key=True)\n    stream_id = Column(Integer)\n    metric_type = Column(String)  # "throughput", "latency", "error_rate"\n    value = Column(Float)\n    timestamp = Column(DateTime(timezone=True), server_default=func.now(), index=True)\n\nclass ProcessedEvent(Base):\n    """\ucc98\ub9ac\ub41c \uc774\ubca4\ud2b8"""\n    __tablename__ = "processed_events"\n\n    id = Column(Integer, primary_key=True)\n    stream_id = Column(Integer)\n    event_type = Column(String, index=True)\n    data = Column(JSON)\n    processed_at = Column(DateTime(timezone=True), server_default=func.now())\n'})}),"\n",(0,a.jsx)(n.h3,{id:"sse_streamerpy",children:"sse_streamer.py"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'import asyncio\nimport json\nfrom typing import AsyncGenerator\nfrom datetime import datetime\n\nclass SSEStreamer:\n    """Server-Sent Events \uc2a4\ud2b8\ub9ac\uba38"""\n\n    def __init__(self):\n        self.clients = set()\n\n    async def stream_generator(self) -> AsyncGenerator[str, None]:\n        """SSE \uc2a4\ud2b8\ub9bc \uc0dd\uc131"""\n        queue = asyncio.Queue()\n        self.clients.add(queue)\n\n        try:\n            while True:\n                data = await queue.get()\n                yield f"data: {json.dumps(data)}\\n\\n"\n\n        finally:\n            self.clients.remove(queue)\n\n    async def broadcast(self, data: dict):\n        """\ubaa8\ub4e0 \ud074\ub77c\uc774\uc5b8\ud2b8\uc5d0\uac8c \ube0c\ub85c\ub4dc\uce90\uc2a4\ud2b8"""\n        for queue in self.clients:\n            await queue.put(data)\n\n    async def stream_logs(self, log_file: str) -> AsyncGenerator[str, None]:\n        """\ub85c\uadf8 \ud30c\uc77c \uc2e4\uc2dc\uac04 \uc2a4\ud2b8\ub9ac\ubc0d (tail -f)"""\n        import aiofiles\n\n        try:\n            async with aiofiles.open(log_file, \'r\') as f:\n                # \ud30c\uc77c \ub05d\uc73c\ub85c \uc774\ub3d9\n                await f.seek(0, 2)\n\n                while True:\n                    line = await f.readline()\n\n                    if line:\n                        yield f"data: {json.dumps({\'log\': line.strip(), \'timestamp\': datetime.utcnow().isoformat()})}\\n\\n"\n                    else:\n                        await asyncio.sleep(0.1)\n\n        except Exception as e:\n            yield f"data: {json.dumps({\'error\': str(e)})}\\n\\n"\n\n    async def stream_metrics(self) -> AsyncGenerator[str, None]:\n        """\uc2dc\uc2a4\ud15c \uba54\ud2b8\ub9ad \uc2a4\ud2b8\ub9ac\ubc0d"""\n        import psutil\n\n        while True:\n            metrics = {\n                "cpu_percent": psutil.cpu_percent(),\n                "memory_percent": psutil.virtual_memory().percent,\n                "disk_percent": psutil.disk_usage(\'/\').percent,\n                "timestamp": datetime.utcnow().isoformat()\n            }\n\n            yield f"data: {json.dumps(metrics)}\\n\\n"\n            await asyncio.sleep(1)\n'})}),"\n",(0,a.jsx)(n.h3,{id:"kafka_consumerpy",children:"kafka_consumer.py"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'from kafka import KafkaConsumer, KafkaProducer\nimport json\nfrom typing import Callable, Optional\n\nclass KafkaStreamConsumer:\n    """Kafka \uc2a4\ud2b8\ub9bc \ucee8\uc288\uba38"""\n\n    def __init__(\n        self,\n        bootstrap_servers: str = "localhost:9092",\n        group_id: str = "fastapi-consumer"\n    ):\n        self.bootstrap_servers = bootstrap_servers\n        self.group_id = group_id\n        self.consumer = None\n\n    def consume(\n        self,\n        topic: str,\n        callback: Callable[[dict], None],\n        auto_offset_reset: str = "latest"\n    ):\n        """\uba54\uc2dc\uc9c0 \uc18c\ube44"""\n        self.consumer = KafkaConsumer(\n            topic,\n            bootstrap_servers=self.bootstrap_servers,\n            group_id=self.group_id,\n            auto_offset_reset=auto_offset_reset,\n            value_deserializer=lambda m: json.loads(m.decode(\'utf-8\'))\n        )\n\n        try:\n            for message in self.consumer:\n                callback(message.value)\n\n        finally:\n            if self.consumer:\n                self.consumer.close()\n\n    async def consume_async(\n        self,\n        topic: str,\n        callback: Callable[[dict], None]\n    ):\n        """\ube44\ub3d9\uae30 \uba54\uc2dc\uc9c0 \uc18c\ube44"""\n        import asyncio\n\n        loop = asyncio.get_event_loop()\n        await loop.run_in_executor(None, self.consume, topic, callback)\n\n    def close(self):\n        """\ucee8\uc288\uba38 \uc885\ub8cc"""\n        if self.consumer:\n            self.consumer.close()\n\nclass KafkaStreamProducer:\n    """Kafka \uc2a4\ud2b8\ub9bc \ud504\ub85c\ub4c0\uc11c"""\n\n    def __init__(self, bootstrap_servers: str = "localhost:9092"):\n        self.producer = KafkaProducer(\n            bootstrap_servers=bootstrap_servers,\n            value_serializer=lambda v: json.dumps(v).encode(\'utf-8\')\n        )\n\n    def send(self, topic: str, data: dict):\n        """\uba54\uc2dc\uc9c0 \ubc1c\ud589"""\n        self.producer.send(topic, value=data)\n        self.producer.flush()\n\n    def close(self):\n        """\ud504\ub85c\ub4c0\uc11c \uc885\ub8cc"""\n        if self.producer:\n            self.producer.close()\n'})}),"\n",(0,a.jsx)(n.h3,{id:"redis_streampy",children:"redis_stream.py"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'import redis\nimport json\nfrom typing import List, Dict, Optional\n\nclass RedisStreamClient:\n    """Redis Streams \ud074\ub77c\uc774\uc5b8\ud2b8"""\n\n    def __init__(self, host: str = "localhost", port: int = 6379):\n        self.client = redis.Redis(host=host, port=port, decode_responses=True)\n\n    def add(self, stream_name: str, data: dict, maxlen: int = 10000) -> str:\n        """\uc2a4\ud2b8\ub9bc\uc5d0 \uba54\uc2dc\uc9c0 \ucd94\uac00"""\n        message_id = self.client.xadd(\n            stream_name,\n            data,\n            maxlen=maxlen  # \ucd5c\ub300 \uae38\uc774 \uc81c\ud55c\n        )\n        return message_id\n\n    def read(\n        self,\n        stream_name: str,\n        count: int = 10,\n        block: Optional[int] = None,\n        last_id: str = \'0\'\n    ) -> List[Dict]:\n        """\uc2a4\ud2b8\ub9bc\uc5d0\uc11c \uba54\uc2dc\uc9c0 \uc77d\uae30"""\n        messages = self.client.xread(\n            {stream_name: last_id},\n            count=count,\n            block=block\n        )\n\n        results = []\n        for stream, entries in messages:\n            for entry_id, data in entries:\n                results.append({\n                    "id": entry_id,\n                    "data": data\n                })\n\n        return results\n\n    def read_group(\n        self,\n        group_name: str,\n        consumer_name: str,\n        stream_name: str,\n        count: int = 10,\n        block: Optional[int] = None\n    ) -> List[Dict]:\n        """\ucee8\uc288\uba38 \uadf8\ub8f9\uc73c\ub85c \uba54\uc2dc\uc9c0 \uc77d\uae30"""\n        # \uadf8\ub8f9\uc774 \uc5c6\uc73c\uba74 \uc0dd\uc131\n        try:\n            self.client.xgroup_create(stream_name, group_name, id=\'0\', mkstream=True)\n        except redis.exceptions.ResponseError:\n            pass  # \uadf8\ub8f9\uc774 \uc774\ubbf8 \uc874\uc7ac\n\n        messages = self.client.xreadgroup(\n            group_name,\n            consumer_name,\n            {stream_name: \'>\'},\n            count=count,\n            block=block\n        )\n\n        results = []\n        for stream, entries in messages:\n            for entry_id, data in entries:\n                results.append({\n                    "id": entry_id,\n                    "data": data\n                })\n\n        return results\n\n    def ack(self, stream_name: str, group_name: str, message_id: str):\n        """\uba54\uc2dc\uc9c0 \ud655\uc778 \uc751\ub2f5"""\n        self.client.xack(stream_name, group_name, message_id)\n\n    def get_stream_info(self, stream_name: str) -> dict:\n        """\uc2a4\ud2b8\ub9bc \uc815\ubcf4 \uc870\ud68c"""\n        info = self.client.xinfo_stream(stream_name)\n        return info\n'})}),"\n",(0,a.jsx)(n.h3,{id:"pipeline_processorpy",children:"pipeline_processor.py"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'from typing import Callable, List, Dict, Any\nimport asyncio\n\nclass StreamPipeline:\n    """\uc2a4\ud2b8\ub9bc \ub370\uc774\ud130 \ud30c\uc774\ud504\ub77c\uc778"""\n\n    def __init__(self):\n        self.transforms: List[Callable] = []\n        self.filters: List[Callable] = []\n\n    def add_transform(self, func: Callable[[Dict], Dict]):\n        """\ubcc0\ud658 \ud568\uc218 \ucd94\uac00"""\n        self.transforms.append(func)\n        return self\n\n    def add_filter(self, func: Callable[[Dict], bool]):\n        """\ud544\ud130 \ud568\uc218 \ucd94\uac00"""\n        self.filters.append(func)\n        return self\n\n    def process(self, data: Dict) -> Dict:\n        """\ub370\uc774\ud130 \ucc98\ub9ac"""\n        # \ud544\ud130 \uc801\uc6a9\n        for filter_func in self.filters:\n            if not filter_func(data):\n                return None\n\n        # \ubcc0\ud658 \uc801\uc6a9\n        result = data\n        for transform_func in self.transforms:\n            result = transform_func(result)\n\n        return result\n\n    async def process_stream(\n        self,\n        input_stream,\n        output_callback: Callable[[Dict], None]\n    ):\n        """\uc2a4\ud2b8\ub9bc \ucc98\ub9ac"""\n        async for data in input_stream:\n            processed = self.process(data)\n\n            if processed:\n                await output_callback(processed)\n\n# \uc720\ud2f8\ub9ac\ud2f0 \ubcc0\ud658 \ud568\uc218\ub4e4\ndef normalize_fields(field_mapping: Dict[str, str]):\n    """\ud544\ub4dc\uba85 \uc815\uaddc\ud654"""\n    def transform(data: Dict) -> Dict:\n        result = {}\n        for old_key, new_key in field_mapping.items():\n            if old_key in data:\n                result[new_key] = data[old_key]\n        return result\n\n    return transform\n\ndef enrich_with_timestamp(data: Dict) -> Dict:\n    """\ud0c0\uc784\uc2a4\ud0ec\ud504 \ucd94\uac00"""\n    from datetime import datetime\n    data[\'processed_at\'] = datetime.utcnow().isoformat()\n    return data\n\ndef filter_by_field(field: str, value: Any):\n    """\ud544\ub4dc \uac12\uc73c\ub85c \ud544\ud130\ub9c1"""\n    def filter_func(data: Dict) -> bool:\n        return data.get(field) == value\n\n    return filter_func\n\ndef aggregate_count(window_size: int = 100):\n    """\uc9d1\uacc4 (\uce74\uc6b4\ud2b8)"""\n    count = 0\n\n    def transform(data: Dict) -> Dict:\n        nonlocal count\n        count += 1\n\n        if count % window_size == 0:\n            data[\'aggregate\'] = {\'count\': count, \'window\': window_size}\n\n        return data\n\n    return transform\n'})}),"\n",(0,a.jsx)(n.h3,{id:"mainpy",children:"main.py"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'from fastapi import FastAPI, Depends, BackgroundTasks\nfrom fastapi.responses import StreamingResponse\nfrom sqlalchemy.orm import Session\nfrom typing import Optional\nimport models\nfrom database import engine, get_db\nfrom sse_streamer import SSEStreamer\nfrom redis_stream import RedisStreamClient\nfrom kafka_consumer import KafkaStreamProducer\nfrom pipeline_processor import StreamPipeline, enrich_with_timestamp\nimport asyncio\nimport json\n\nmodels.Base.metadata.create_all(bind=engine)\n\napp = FastAPI(title="Streaming Data Pipeline API")\n\n# \uc11c\ube44\uc2a4 \uc778\uc2a4\ud134\uc2a4\nsse_streamer = SSEStreamer()\nredis_stream = RedisStreamClient()\nkafka_producer = KafkaStreamProducer()\n\n# ==================== SSE \uc2a4\ud2b8\ub9ac\ubc0d ====================\n\n@app.get("/stream/sse")\nasync def sse_stream():\n    """Server-Sent Events \uc2a4\ud2b8\ub9bc"""\n    return StreamingResponse(\n        sse_streamer.stream_generator(),\n        media_type="text/event-stream"\n    )\n\n@app.post("/stream/broadcast")\nasync def broadcast_message(message: dict):\n    """\ubaa8\ub4e0 SSE \ud074\ub77c\uc774\uc5b8\ud2b8\uc5d0\uac8c \ube0c\ub85c\ub4dc\uce90\uc2a4\ud2b8"""\n    await sse_streamer.broadcast(message)\n    return {"message": "Broadcasted"}\n\n@app.get("/stream/logs")\nasync def stream_logs(log_file: str = "/var/log/app.log"):\n    """\ub85c\uadf8 \ud30c\uc77c \uc2e4\uc2dc\uac04 \uc2a4\ud2b8\ub9ac\ubc0d"""\n    return StreamingResponse(\n        sse_streamer.stream_logs(log_file),\n        media_type="text/event-stream"\n    )\n\n@app.get("/stream/metrics")\nasync def stream_metrics():\n    """\uc2dc\uc2a4\ud15c \uba54\ud2b8\ub9ad \uc2a4\ud2b8\ub9ac\ubc0d"""\n    return StreamingResponse(\n        sse_streamer.stream_metrics(),\n        media_type="text/event-stream"\n    )\n\n# ==================== Redis Streams ====================\n\n@app.post("/redis-stream/{stream_name}/add")\ndef add_to_redis_stream(stream_name: str, data: dict):\n    """Redis Stream\uc5d0 \uba54\uc2dc\uc9c0 \ucd94\uac00"""\n    message_id = redis_stream.add(stream_name, data)\n\n    return {\n        "stream": stream_name,\n        "message_id": message_id,\n        "data": data\n    }\n\n@app.get("/redis-stream/{stream_name}/read")\ndef read_from_redis_stream(\n    stream_name: str,\n    count: int = 10,\n    last_id: str = \'0\'\n):\n    """Redis Stream\uc5d0\uc11c \uba54\uc2dc\uc9c0 \uc77d\uae30"""\n    messages = redis_stream.read(stream_name, count=count, last_id=last_id)\n\n    return {\n        "stream": stream_name,\n        "messages": messages,\n        "count": len(messages)\n    }\n\n@app.get("/redis-stream/{stream_name}/consume")\ndef consume_redis_stream(\n    stream_name: str,\n    group_name: str,\n    consumer_name: str,\n    count: int = 10\n):\n    """Redis Stream \ucee8\uc288\uba38 \uadf8\ub8f9\uc73c\ub85c \uc18c\ube44"""\n    messages = redis_stream.read_group(\n        group_name,\n        consumer_name,\n        stream_name,\n        count=count\n    )\n\n    # \uba54\uc2dc\uc9c0 \ud655\uc778\n    for msg in messages:\n        redis_stream.ack(stream_name, group_name, msg[\'id\'])\n\n    return {\n        "stream": stream_name,\n        "group": group_name,\n        "consumer": consumer_name,\n        "messages": messages\n    }\n\n@app.get("/redis-stream/{stream_name}/info")\ndef get_stream_info(stream_name: str):\n    """\uc2a4\ud2b8\ub9bc \uc815\ubcf4 \uc870\ud68c"""\n    info = redis_stream.get_stream_info(stream_name)\n    return info\n\n# ==================== Kafka ====================\n\n@app.post("/kafka/{topic}/publish")\ndef publish_to_kafka(topic: str, data: dict):\n    """Kafka \ud1a0\ud53d\uc5d0 \uba54\uc2dc\uc9c0 \ubc1c\ud589"""\n    kafka_producer.send(topic, data)\n\n    return {\n        "topic": topic,\n        "data": data,\n        "status": "published"\n    }\n\n# ==================== \ub370\uc774\ud130 \ud30c\uc774\ud504\ub77c\uc778 ====================\n\n@app.post("/pipeline/process")\nasync def process_pipeline(data: dict):\n    """\ub370\uc774\ud130 \ud30c\uc774\ud504\ub77c\uc778 \ucc98\ub9ac"""\n    # \ud30c\uc774\ud504\ub77c\uc778 \uad6c\uc131\n    pipeline = StreamPipeline()\n\n    pipeline.add_transform(enrich_with_timestamp)\n    pipeline.add_transform(lambda d: {**d, "processed": True})\n    pipeline.add_filter(lambda d: d.get(\'valid\', True))\n\n    # \ucc98\ub9ac\n    result = pipeline.process(data)\n\n    if result:\n        # Redis\uc5d0 \uc800\uc7a5\n        redis_stream.add("processed_events", result)\n\n        return {"status": "processed", "data": result}\n    else:\n        return {"status": "filtered"}\n\n@app.get("/pipeline/stats")\nasync def get_pipeline_stats(db: Session = Depends(get_db)):\n    """\ud30c\uc774\ud504\ub77c\uc778 \ud1b5\uacc4"""\n    from sqlalchemy import func\n\n    total_events = db.query(func.count(models.ProcessedEvent.id)).scalar()\n\n    recent_events = db.query(models.ProcessedEvent).order_by(\n        models.ProcessedEvent.processed_at.desc()\n    ).limit(10).all()\n\n    return {\n        "total_events": total_events,\n        "recent_events": [\n            {\n                "id": e.id,\n                "event_type": e.event_type,\n                "processed_at": e.processed_at\n            }\n            for e in recent_events\n        ]\n    }\n\n# ==================== \uc2a4\ud2b8\ub9bc \uc0dd\uc131\uae30 ====================\n\nasync def generate_sample_stream():\n    """\uc0d8\ud50c \ub370\uc774\ud130 \uc2a4\ud2b8\ub9bc \uc0dd\uc131"""\n    import random\n\n    while True:\n        data = {\n            "sensor_id": f"sensor_{random.randint(1, 10)}",\n            "value": random.uniform(20, 30),\n            "timestamp": datetime.utcnow().isoformat()\n        }\n\n        # SSE\ub85c \ube0c\ub85c\ub4dc\uce90\uc2a4\ud2b8\n        await sse_streamer.broadcast(data)\n\n        # Redis\uc5d0 \ucd94\uac00\n        redis_stream.add("sensor_data", data)\n\n        await asyncio.sleep(1)\n\n@app.on_event("startup")\nasync def startup_event():\n    """\uc571 \uc2dc\uc791 \uc2dc \ubc31\uadf8\ub77c\uc6b4\ub4dc \uc2a4\ud2b8\ub9bc \uc2dc\uc791"""\n    asyncio.create_task(generate_sample_stream())\n\n# ==================== \uccad\ud06c \uc2a4\ud2b8\ub9ac\ubc0d ====================\n\n@app.get("/stream/large-file")\nasync def stream_large_file():\n    """\ub300\uc6a9\ub7c9 \ud30c\uc77c \uccad\ud06c \uc2a4\ud2b8\ub9ac\ubc0d"""\n    async def file_generator():\n        chunk_size = 8192\n\n        # \uc608\uc81c: \ub300\uc6a9\ub7c9 \ub370\uc774\ud130 \uc0dd\uc131\n        for i in range(1000):\n            chunk = f"Chunk {i}\\n" * 100\n            yield chunk.encode()\n            await asyncio.sleep(0.01)\n\n    return StreamingResponse(\n        file_generator(),\n        media_type="text/plain"\n    )\n\n@app.get("/stream/json-lines")\nasync def stream_json_lines(db: Session = Depends(get_db)):\n    """JSON Lines \uc2a4\ud2b8\ub9ac\ubc0d"""\n    async def json_lines_generator():\n        events = db.query(models.ProcessedEvent).yield_per(100)\n\n        for event in events:\n            line = json.dumps({\n                "id": event.id,\n                "event_type": event.event_type,\n                "data": event.data\n            })\n            yield f"{line}\\n"\n\n    return StreamingResponse(\n        json_lines_generator(),\n        media_type="application/x-ndjson"\n    )\n'})}),"\n",(0,a.jsx)(n.h2,{id:"-\ud575\uc2ec-\uac1c\ub150",children:"\ud83d\udd11 \ud575\uc2ec \uac1c\ub150"}),"\n",(0,a.jsx)(n.h3,{id:"server-sent-events-sse",children:"Server-Sent Events (SSE)"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"\uc11c\ubc84\uc5d0\uc11c \ud074\ub77c\uc774\uc5b8\ud2b8\ub85c \ub2e8\ubc29\ud5a5 \uc2e4\uc2dc\uac04 \ud478\uc2dc"}),"\n",(0,a.jsx)(n.li,{children:"HTTP \uae30\ubc18\uc73c\ub85c WebSocket\ubcf4\ub2e4 \uac04\ub2e8"}),"\n",(0,a.jsx)(n.li,{children:"\uc790\ub3d9 \uc7ac\uc5f0\uacb0 \uc9c0\uc6d0"}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"redis-streams",children:"Redis Streams"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"\ub85c\uadf8 \uad6c\uc870 \ub370\uc774\ud130 \uc800\uc7a5"}),"\n",(0,a.jsx)(n.li,{children:"\ucee8\uc288\uba38 \uadf8\ub8f9\uc73c\ub85c \uba54\uc2dc\uc9c0 \ubd84\uc0b0 \ucc98\ub9ac"}),"\n",(0,a.jsx)(n.li,{children:"ACK \uae30\ubc18 \uc2e0\ub8b0\uc131 \ubcf4\uc7a5"}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"\ub370\uc774\ud130-\ud30c\uc774\ud504\ub77c\uc778",children:"\ub370\uc774\ud130 \ud30c\uc774\ud504\ub77c\uc778"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Transform: \ub370\uc774\ud130 \ubcc0\ud658"}),"\n",(0,a.jsx)(n.li,{children:"Filter: \uc870\uac74\ubd80 \ud544\ud130\ub9c1"}),"\n",(0,a.jsx)(n.li,{children:"\uccb4\uc774\ub2dd\uc73c\ub85c \ubcf5\uc7a1\ud55c \ucc98\ub9ac"}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"\uc2a4\ud2b8\ub9ac\ubc0d-\ud328\ud134",children:"\uc2a4\ud2b8\ub9ac\ubc0d \ud328\ud134"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"StreamingResponse"}),"\ub85c \uccad\ud06c \uc804\uc1a1"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"yield"}),"\ub85c \uc9c0\uc5f0 \uc0dd\uc131"]}),"\n",(0,a.jsx)(n.li,{children:"\uba54\ubaa8\ub9ac \ud6a8\uc728\uc801\uc778 \ub300\uc6a9\ub7c9 \ucc98\ub9ac"}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"-\ud559\uc2b5-\uc644\ub8cc",children:"\ud83d\udcda \ud559\uc2b5 \uc644\ub8cc!"}),"\n",(0,a.jsx)(n.p,{children:"\ucd95\ud558\ud569\ub2c8\ub2e4! FastAPI \uc2e4\uc804 \uc608\uc81c 16\uac1c\ub97c \ubaa8\ub450 \uc644\ub8cc\ud588\uc2b5\ub2c8\ub2e4!"}),"\n",(0,a.jsx)(n.p,{children:"\uc774\uc81c \uc5ec\ub7ec\ubd84\uc740 \ub2e4\uc74c\uc744 \uad6c\ud604\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"REST API, GraphQL"}),"\n",(0,a.jsx)(n.li,{children:"\uc2e4\uc2dc\uac04 \ud1b5\uc2e0 (WebSocket, SSE)"}),"\n",(0,a.jsx)(n.li,{children:"\ub370\uc774\ud130 \ubd84\uc11d \ubc0f \uc2dc\uac01\ud654"}),"\n",(0,a.jsx)(n.li,{children:"AI/ML \uc11c\ube44\uc2a4"}),"\n",(0,a.jsx)(n.li,{children:"\uc6f9 \uc2a4\ud06c\ub798\ud551"}),"\n",(0,a.jsx)(n.li,{children:"\uc2a4\ud2b8\ub9ac\ubc0d \ud30c\uc774\ud504\ub77c\uc778"}),"\n"]}),"\n",(0,a.jsx)(n.p,{children:"\ud83d\udc49 \uc2e4\uc81c \ud504\ub85c\ub355\uc158 \uc11c\ube44\uc2a4\ub97c \ub9cc\ub4e4\uc5b4\ubcf4\uc138\uc694!"})]})}function c(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,a.jsx)(n,{...e,children:(0,a.jsx)(m,{...e})}):m(e)}}}]);