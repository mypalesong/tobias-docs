"use strict";(globalThis.webpackChunkgithub_docs=globalThis.webpackChunkgithub_docs||[]).push([[8297],{2782:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>o,default:()=>p,frontMatter:()=>a,metadata:()=>r,toc:()=>c});const r=JSON.parse('{"id":"fastapi/examples/scenario-15-web-scraper","title":"\uc2dc\ub098\ub9ac\uc624 15: \uc6f9 \ud06c\ub864\ub9c1/\uc2a4\ud06c\ub798\ud551 API","description":"BeautifulSoup\uacfc Selenium\uc744 \ud65c\uc6a9\ud55c \uc6f9 \uc2a4\ud06c\ub798\ud551 \uc11c\ube44\uc2a4\ub97c \ub9cc\ub4e4\uc5b4\ubd05\uc2dc\ub2e4!","source":"@site/docs/fastapi/examples/scenario-15-web-scraper.md","sourceDirName":"fastapi/examples","slug":"/fastapi/examples/scenario-15-web-scraper","permalink":"/docs/fastapi/examples/scenario-15-web-scraper","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/fastapi/examples/scenario-15-web-scraper.md","tags":[],"version":"current","sidebarPosition":16,"frontMatter":{"sidebar_position":16},"sidebar":"fastapiSidebar","previous":{"title":"\uc2dc\ub098\ub9ac\uc624 14: AI/ML \uc11c\ube44\uc2a4 API","permalink":"/docs/fastapi/examples/scenario-14-ai-ml-service"},"next":{"title":"\uc2dc\ub098\ub9ac\uc624 16: \uc2a4\ud2b8\ub9ac\ubc0d \ub370\uc774\ud130 \ud30c\uc774\ud504\ub77c\uc778","permalink":"/docs/fastapi/examples/scenario-16-streaming-pipeline"}}');var s=t(4848),i=t(8453);const a={sidebar_position:16},o="\uc2dc\ub098\ub9ac\uc624 15: \uc6f9 \ud06c\ub864\ub9c1/\uc2a4\ud06c\ub798\ud551 API",l={},c=[{value:"\ud83d\udccc \uae30\ub2a5",id:"-\uae30\ub2a5",level:2},{value:"\ud83d\udcc1 \ud504\ub85c\uc81d\ud2b8 \uad6c\uc870",id:"-\ud504\ub85c\uc81d\ud2b8-\uad6c\uc870",level:2},{value:"\ud83d\udcca \uc2dc\ud000\uc2a4 \ub2e4\uc774\uc5b4\uadf8\ub7a8",id:"-\uc2dc\ud000\uc2a4-\ub2e4\uc774\uc5b4\uadf8\ub7a8",level:2},{value:"\ud83d\udcdd \ud575\uc2ec \ucf54\ub4dc",id:"-\ud575\uc2ec-\ucf54\ub4dc",level:2},{value:"models.py",id:"modelspy",level:3},{value:"static_scraper.py",id:"static_scraperpy",level:3},{value:"dynamic_scraper.py",id:"dynamic_scraperpy",level:3},{value:"scraper_service.py",id:"scraper_servicepy",level:3},{value:"main.py",id:"mainpy",level:3},{value:"\ud83d\udd11 \ud575\uc2ec \uac1c\ub150",id:"-\ud575\uc2ec-\uac1c\ub150",level:2},{value:"\uc815\uc801 vs \ub3d9\uc801 \uc2a4\ud06c\ub798\ud551",id:"\uc815\uc801-vs-\ub3d9\uc801-\uc2a4\ud06c\ub798\ud551",level:3},{value:"\ubd07 \ud0d0\uc9c0 \uc6b0\ud68c",id:"\ubd07-\ud0d0\uc9c0-\uc6b0\ud68c",level:3},{value:"\uc18d\ub3c4 \uc81c\ud55c \uc900\uc218",id:"\uc18d\ub3c4-\uc81c\ud55c-\uc900\uc218",level:3},{value:"\ub370\uc774\ud130 \ucd94\ucd9c \ud328\ud134",id:"\ub370\uc774\ud130-\ucd94\ucd9c-\ud328\ud134",level:3},{value:"\ud83d\udcda \ub2e4\uc74c \ub2e8\uacc4",id:"-\ub2e4\uc74c-\ub2e8\uacc4",level:2}];function d(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,i.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"\uc2dc\ub098\ub9ac\uc624-15-\uc6f9-\ud06c\ub864\ub9c1\uc2a4\ud06c\ub798\ud551-api",children:"\uc2dc\ub098\ub9ac\uc624 15: \uc6f9 \ud06c\ub864\ub9c1/\uc2a4\ud06c\ub798\ud551 API"})}),"\n",(0,s.jsx)(n.p,{children:"BeautifulSoup\uacfc Selenium\uc744 \ud65c\uc6a9\ud55c \uc6f9 \uc2a4\ud06c\ub798\ud551 \uc11c\ube44\uc2a4\ub97c \ub9cc\ub4e4\uc5b4\ubd05\uc2dc\ub2e4!"}),"\n",(0,s.jsx)(n.h2,{id:"-\uae30\ub2a5",children:"\ud83d\udccc \uae30\ub2a5"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"\u2705 HTML \ud30c\uc2f1 \ubc0f \ub370\uc774\ud130 \ucd94\ucd9c"}),"\n",(0,s.jsx)(n.li,{children:"\u2705 JavaScript \ub80c\ub354\ub9c1 (Selenium)"}),"\n",(0,s.jsx)(n.li,{children:"\u2705 \uc2a4\ud06c\ub798\ud551 \uc791\uc5c5 \uc2a4\ucf00\uc904\ub9c1"}),"\n",(0,s.jsx)(n.li,{children:"\u2705 \ud504\ub85d\uc2dc \ub85c\ud14c\uc774\uc158"}),"\n",(0,s.jsx)(n.li,{children:"\u2705 \ubd07 \ud0d0\uc9c0 \uc6b0\ud68c"}),"\n",(0,s.jsx)(n.li,{children:"\u2705 \ub370\uc774\ud130 \uc815\uc81c \ubc0f \uc800\uc7a5"}),"\n",(0,s.jsx)(n.li,{children:"\u2705 \uc18d\ub3c4 \uc81c\ud55c (Rate Limiting)"}),"\n",(0,s.jsx)(n.li,{children:"\u2705 \uc5d0\ub7ec \ucc98\ub9ac \ubc0f \uc7ac\uc2dc\ub3c4"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"-\ud504\ub85c\uc81d\ud2b8-\uad6c\uc870",children:"\ud83d\udcc1 \ud504\ub85c\uc81d\ud2b8 \uad6c\uc870"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:"web-scraper/\n\u251c\u2500\u2500 main.py                 # FastAPI \uc560\ud50c\ub9ac\ucf00\uc774\uc158\n\u251c\u2500\u2500 models.py               # DB \ubaa8\ub378 (ScrapingJob, ScrapingResult)\n\u251c\u2500\u2500 schemas.py              # Pydantic \uc2a4\ud0a4\ub9c8\n\u251c\u2500\u2500 database.py             # \ub370\uc774\ud130\ubca0\uc774\uc2a4 \uc5f0\uacb0\n\u251c\u2500\u2500 scrapers/\n\u2502   \u251c\u2500\u2500 static_scraper.py   # BeautifulSoup \uc2a4\ud06c\ub798\ud37c\n\u2502   \u251c\u2500\u2500 dynamic_scraper.py  # Selenium \uc2a4\ud06c\ub798\ud37c\n\u2502   \u2514\u2500\u2500 base_scraper.py     # \ubca0\uc774\uc2a4 \uc2a4\ud06c\ub798\ud37c\n\u251c\u2500\u2500 proxy_manager.py        # \ud504\ub85d\uc2dc \uad00\ub9ac\n\u251c\u2500\u2500 scheduler.py            # APScheduler \uc791\uc5c5 \uc2a4\ucf00\uc904\ub9c1\n\u2514\u2500\u2500 requirements.txt        # \uc758\uc874\uc131 \ud328\ud0a4\uc9c0\n"})}),"\n",(0,s.jsx)(n.h2,{id:"-\uc2dc\ud000\uc2a4-\ub2e4\uc774\uc5b4\uadf8\ub7a8",children:"\ud83d\udcca \uc2dc\ud000\uc2a4 \ub2e4\uc774\uc5b4\uadf8\ub7a8"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-mermaid",children:"sequenceDiagram\n    participant U as User\n    participant API as FastAPI\n    participant Sch as Scheduler\n    participant SS as StaticScraper\n    participant DS as DynamicScraper\n    participant PM as ProxyManager\n    participant DB as Database\n\n    Note over U,DB: 1. \uc2a4\ud06c\ub798\ud551 \uc791\uc5c5 \uc0dd\uc131\n    U->>API: POST /scraping/jobs<br/>{url, selectors, schedule}\n    API->>DB: INSERT scraping_job\n    DB--\x3e>API: Job created\n    API->>Sch: Schedule job\n    API--\x3e>U: 201 Created + Job info\n\n    Note over U,DB: 2. \uc815\uc801 \ud398\uc774\uc9c0 \uc2a4\ud06c\ub798\ud551\n    U->>API: POST /scraping/run/{job_id}\n    API->>DB: SELECT job config\n    DB--\x3e>API: Job config\n    API->>SS: scrape(url, selectors)\n    SS->>PM: Get proxy (if enabled)\n    PM--\x3e>SS: Proxy URL\n    SS->>SS: HTTP GET request\n    SS->>SS: Parse HTML with BeautifulSoup\n    SS->>SS: Extract data using selectors\n    SS--\x3e>API: Scraped data\n    API->>DB: INSERT scraping_result\n    API--\x3e>U: 200 OK + Data\n\n    Note over U,DB: 3. \ub3d9\uc801 \ud398\uc774\uc9c0 \uc2a4\ud06c\ub798\ud551 (JS)\n    U->>API: POST /scraping/run/{job_id}\n    API->>DS: scrape_dynamic()\n    DS->>PM: Get proxy\n    PM--\x3e>DS: Proxy\n    DS->>DS: Start Selenium WebDriver\n    DS->>DS: Navigate to URL\n    DS->>DS: Wait for JS rendering\n    DS->>DS: Execute selectors\n    DS->>DS: Extract data\n    DS->>DS: Close WebDriver\n    DS--\x3e>API: Scraped data\n    API->>DB: INSERT result\n    API--\x3e>U: 200 OK + Data\n\n    Note over U,DB: 4. \uc2a4\ucf00\uc904\ub41c \uc2e4\ud589 (\ubc31\uadf8\ub77c\uc6b4\ub4dc)\n    Sch->>Sch: Cron trigger\n    Sch->>DB: SELECT active jobs\n    DB--\x3e>Sch: Jobs to run\n    loop For each job\n        Sch->>SS: scrape()\n        SS--\x3e>Sch: Data\n        Sch->>DB: INSERT result\n    end\n"})}),"\n",(0,s.jsx)(n.h2,{id:"-\ud575\uc2ec-\ucf54\ub4dc",children:"\ud83d\udcdd \ud575\uc2ec \ucf54\ub4dc"}),"\n",(0,s.jsx)(n.h3,{id:"modelspy",children:"models.py"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'from sqlalchemy import Column, Integer, String, DateTime, Text, Boolean, JSON, Float\nfrom sqlalchemy.sql import func\nfrom database import Base\n\nclass ScrapingJob(Base):\n    """\uc2a4\ud06c\ub798\ud551 \uc791\uc5c5"""\n    __tablename__ = "scraping_jobs"\n\n    id = Column(Integer, primary_key=True)\n    name = Column(String)\n    url = Column(String)\n    scraper_type = Column(String)  # "static", "dynamic"\n    selectors = Column(JSON)  # CSS \uc120\ud0dd\uc790\n    schedule = Column(String, nullable=True)  # Cron \ud615\uc2dd\n\n    # \uc124\uc815\n    use_proxy = Column(Boolean, default=False)\n    wait_time = Column(Float, default=1.0)  # \uc694\uccad \uac04 \ub300\uae30 \uc2dc\uac04 (\ucd08)\n    max_retries = Column(Integer, default=3)\n\n    # \uc0c1\ud0dc\n    is_active = Column(Boolean, default=True)\n    last_run = Column(DateTime(timezone=True), nullable=True)\n    next_run = Column(DateTime(timezone=True), nullable=True)\n\n    created_at = Column(DateTime(timezone=True), server_default=func.now())\n\nclass ScrapingResult(Base):\n    """\uc2a4\ud06c\ub798\ud551 \uacb0\uacfc"""\n    __tablename__ = "scraping_results"\n\n    id = Column(Integer, primary_key=True)\n    job_id = Column(Integer)\n    data = Column(JSON)\n    status = Column(String)  # "success", "failed"\n    error_message = Column(Text, nullable=True)\n    items_count = Column(Integer, default=0)\n    processing_time = Column(Float)  # \ucd08\n\n    created_at = Column(DateTime(timezone=True), server_default=func.now())\n'})}),"\n",(0,s.jsx)(n.h3,{id:"static_scraperpy",children:"static_scraper.py"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"import requests\nfrom bs4 import BeautifulSoup\nfrom typing import List, Dict, Optional\nimport time\n\nclass StaticScraper:\n    \"\"\"\uc815\uc801 \uc6f9\ud398\uc774\uc9c0 \uc2a4\ud06c\ub798\ud37c (BeautifulSoup)\"\"\"\n\n    def __init__(\n        self,\n        user_agent: Optional[str] = None,\n        proxy: Optional[str] = None,\n        timeout: int = 30\n    ):\n        self.session = requests.Session()\n        self.timeout = timeout\n\n        # User-Agent \uc124\uc815\n        headers = {\n            'User-Agent': user_agent or 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'\n        }\n        self.session.headers.update(headers)\n\n        # \ud504\ub85d\uc2dc \uc124\uc815\n        if proxy:\n            self.session.proxies = {\n                'http': proxy,\n                'https': proxy\n            }\n\n    def scrape(self, url: str, selectors: Dict[str, str]) -> List[Dict]:\n        \"\"\"\uc6f9\ud398\uc774\uc9c0 \uc2a4\ud06c\ub798\ud551\"\"\"\n        try:\n            response = self.session.get(url, timeout=self.timeout)\n            response.raise_for_status()\n\n            soup = BeautifulSoup(response.content, 'html.parser')\n\n            results = []\n\n            # \ucee8\ud14c\uc774\ub108 \uc120\ud0dd\uc790\uac00 \uc788\uc73c\uba74 \uac01 \uc544\uc774\ud15c\ubcc4\ub85c \ucd94\ucd9c\n            if 'container' in selectors:\n                containers = soup.select(selectors['container'])\n\n                for container in containers:\n                    item = {}\n                    for key, selector in selectors.items():\n                        if key == 'container':\n                            continue\n\n                        element = container.select_one(selector)\n                        if element:\n                            # href, src \ub4f1 \uc18d\uc131 \ucd94\ucd9c\n                            if selector.endswith('[href]'):\n                                item[key] = element.get('href')\n                            elif selector.endswith('[src]'):\n                                item[key] = element.get('src')\n                            else:\n                                item[key] = element.get_text(strip=True)\n\n                    results.append(item)\n\n            # \ub2e8\uc77c \uc694\uc18c \ucd94\ucd9c\n            else:\n                item = {}\n                for key, selector in selectors.items():\n                    elements = soup.select(selector)\n\n                    if len(elements) == 1:\n                        item[key] = elements[0].get_text(strip=True)\n                    else:\n                        item[key] = [el.get_text(strip=True) for el in elements]\n\n                results.append(item)\n\n            return results\n\n        except Exception as e:\n            raise Exception(f\"Scraping failed: {str(e)}\")\n\n    def scrape_table(self, url: str, table_selector: str = 'table') -> List[Dict]:\n        \"\"\"\ud14c\uc774\ube14 \ub370\uc774\ud130 \ucd94\ucd9c\"\"\"\n        response = self.session.get(url, timeout=self.timeout)\n        soup = BeautifulSoup(response.content, 'html.parser')\n\n        table = soup.select_one(table_selector)\n        if not table:\n            return []\n\n        # \ud5e4\ub354 \ucd94\ucd9c\n        headers = []\n        thead = table.find('thead')\n        if thead:\n            headers = [th.get_text(strip=True) for th in thead.find_all('th')]\n\n        # \ub370\uc774\ud130 \ucd94\ucd9c\n        rows = []\n        tbody = table.find('tbody') or table\n        for tr in tbody.find_all('tr'):\n            cells = [td.get_text(strip=True) for td in tr.find_all(['td', 'th'])]\n            if cells:\n                if headers:\n                    rows.append(dict(zip(headers, cells)))\n                else:\n                    rows.append({\"data\": cells})\n\n        return rows\n\n    def scrape_links(self, url: str, link_selector: str = 'a') -> List[str]:\n        \"\"\"\ub9c1\ud06c \ucd94\ucd9c\"\"\"\n        response = self.session.get(url, timeout=self.timeout)\n        soup = BeautifulSoup(response.content, 'html.parser')\n\n        links = []\n        for a in soup.select(link_selector):\n            href = a.get('href')\n            if href:\n                # \uc808\ub300 URL\ub85c \ubcc0\ud658\n                from urllib.parse import urljoin\n                absolute_url = urljoin(url, href)\n                links.append(absolute_url)\n\n        return links\n"})}),"\n",(0,s.jsx)(n.h3,{id:"dynamic_scraperpy",children:"dynamic_scraper.py"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"from selenium import webdriver\nfrom selenium.webdriver.common.by import By\nfrom selenium.webdriver.support.ui import WebDriverWait\nfrom selenium.webdriver.support import expected_conditions as EC\nfrom selenium.webdriver.chrome.options import Options\nfrom typing import List, Dict\nimport time\n\nclass DynamicScraper:\n    \"\"\"\ub3d9\uc801 \uc6f9\ud398\uc774\uc9c0 \uc2a4\ud06c\ub798\ud37c (Selenium)\"\"\"\n\n    def __init__(self, headless: bool = True, proxy: str = None):\n        options = Options()\n\n        if headless:\n            options.add_argument('--headless')\n\n        options.add_argument('--no-sandbox')\n        options.add_argument('--disable-dev-shm-usage')\n        options.add_argument('--disable-gpu')\n        options.add_argument('--window-size=1920,1080')\n\n        # User-Agent\n        options.add_argument('user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36')\n\n        # \ubd07 \ud0d0\uc9c0 \uc6b0\ud68c\n        options.add_experimental_option(\"excludeSwitches\", [\"enable-automation\"])\n        options.add_experimental_option('useAutomationExtension', False)\n\n        # \ud504\ub85d\uc2dc\n        if proxy:\n            options.add_argument(f'--proxy-server={proxy}')\n\n        self.driver = webdriver.Chrome(options=options)\n\n        # \ubd07 \ud0d0\uc9c0 \uc6b0\ud68c \uc2a4\ud06c\ub9bd\ud2b8\n        self.driver.execute_cdp_cmd('Page.addScriptToEvaluateOnNewDocument', {\n            'source': '''\n                Object.defineProperty(navigator, 'webdriver', {\n                    get: () => undefined\n                })\n            '''\n        })\n\n    def scrape(\n        self,\n        url: str,\n        selectors: Dict[str, str],\n        wait_selector: str = None,\n        scroll_to_bottom: bool = False\n    ) -> List[Dict]:\n        \"\"\"\ub3d9\uc801 \uc6f9\ud398\uc774\uc9c0 \uc2a4\ud06c\ub798\ud551\"\"\"\n        try:\n            self.driver.get(url)\n\n            # \ud2b9\uc815 \uc694\uc18c\uac00 \ub85c\ub4dc\ub420 \ub54c\uae4c\uc9c0 \ub300\uae30\n            if wait_selector:\n                WebDriverWait(self.driver, 10).until(\n                    EC.presence_of_element_located((By.CSS_SELECTOR, wait_selector))\n                )\n\n            # \uc2a4\ud06c\ub864 (\ubb34\ud55c \uc2a4\ud06c\ub864 \ud398\uc774\uc9c0\uc6a9)\n            if scroll_to_bottom:\n                self._scroll_to_bottom()\n\n            results = []\n\n            # \ucee8\ud14c\uc774\ub108 \uc120\ud0dd\uc790\uac00 \uc788\uc73c\uba74 \uac01 \uc544\uc774\ud15c\ubcc4\ub85c \ucd94\ucd9c\n            if 'container' in selectors:\n                containers = self.driver.find_elements(By.CSS_SELECTOR, selectors['container'])\n\n                for container in containers:\n                    item = {}\n                    for key, selector in selectors.items():\n                        if key == 'container':\n                            continue\n\n                        try:\n                            element = container.find_element(By.CSS_SELECTOR, selector)\n\n                            # \uc18d\uc131 \ucd94\ucd9c\n                            if selector.endswith('[href]'):\n                                item[key] = element.get_attribute('href')\n                            elif selector.endswith('[src]'):\n                                item[key] = element.get_attribute('src')\n                            else:\n                                item[key] = element.text\n\n                        except:\n                            item[key] = None\n\n                    results.append(item)\n\n            return results\n\n        except Exception as e:\n            raise Exception(f\"Dynamic scraping failed: {str(e)}\")\n\n    def _scroll_to_bottom(self, pause_time: float = 2.0):\n        \"\"\"\ud398\uc774\uc9c0 \ub05d\uae4c\uc9c0 \uc2a4\ud06c\ub864\"\"\"\n        last_height = self.driver.execute_script(\"return document.body.scrollHeight\")\n\n        while True:\n            # \uc2a4\ud06c\ub864 \ub2e4\uc6b4\n            self.driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n            time.sleep(pause_time)\n\n            # \uc0c8\ub85c\uc6b4 \ub192\uc774 \uacc4\uc0b0\n            new_height = self.driver.execute_script(\"return document.body.scrollHeight\")\n\n            if new_height == last_height:\n                break\n\n            last_height = new_height\n\n    def click_and_scrape(self, url: str, click_selector: str, data_selector: str) -> str:\n        \"\"\"\ubc84\ud2bc \ud074\ub9ad \ud6c4 \ub370\uc774\ud130 \ucd94\ucd9c\"\"\"\n        self.driver.get(url)\n\n        # \ubc84\ud2bc \ud074\ub9ad\n        button = WebDriverWait(self.driver, 10).until(\n            EC.element_to_be_clickable((By.CSS_SELECTOR, click_selector))\n        )\n        button.click()\n\n        # \ub370\uc774\ud130 \ucd94\ucd9c\n        time.sleep(1)\n        element = self.driver.find_element(By.CSS_SELECTOR, data_selector)\n        return element.text\n\n    def close(self):\n        \"\"\"\ube0c\ub77c\uc6b0\uc800 \uc885\ub8cc\"\"\"\n        if self.driver:\n            self.driver.quit()\n\n    def __del__(self):\n        self.close()\n"})}),"\n",(0,s.jsx)(n.h3,{id:"scraper_servicepy",children:"scraper_service.py"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'from sqlalchemy.orm import Session\nimport models\nfrom static_scraper import StaticScraper\nfrom dynamic_scraper import DynamicScraper\nimport time\nfrom datetime import datetime\n\nclass ScraperService:\n    def __init__(self, db: Session):\n        self.db = db\n\n    def execute_job(self, job_id: int) -> models.ScrapingResult:\n        """\uc2a4\ud06c\ub798\ud551 \uc791\uc5c5 \uc2e4\ud589"""\n        job = self.db.query(models.ScrapingJob).filter(\n            models.ScrapingJob.id == job_id\n        ).first()\n\n        if not job:\n            raise ValueError("Job not found")\n\n        start_time = time.time()\n        result = models.ScrapingResult(job_id=job_id)\n\n        try:\n            # \uc2a4\ud06c\ub798\ud37c \uc120\ud0dd\n            if job.scraper_type == "static":\n                scraper = StaticScraper(\n                    proxy=os.getenv(\'PROXY_URL\') if job.use_proxy else None\n                )\n                data = scraper.scrape(job.url, job.selectors)\n\n            elif job.scraper_type == "dynamic":\n                scraper = DynamicScraper(\n                    headless=True,\n                    proxy=os.getenv(\'PROXY_URL\') if job.use_proxy else None\n                )\n                try:\n                    data = scraper.scrape(\n                        job.url,\n                        job.selectors,\n                        wait_selector=job.selectors.get(\'wait_for\')\n                    )\n                finally:\n                    scraper.close()\n\n            else:\n                raise ValueError(f"Unknown scraper type: {job.scraper_type}")\n\n            # \uacb0\uacfc \uc800\uc7a5\n            result.status = "success"\n            result.data = data\n            result.items_count = len(data)\n\n            # \uc791\uc5c5 \uc0c1\ud0dc \uc5c5\ub370\uc774\ud2b8\n            job.last_run = datetime.utcnow()\n\n        except Exception as e:\n            result.status = "failed"\n            result.error_message = str(e)\n\n        finally:\n            result.processing_time = time.time() - start_time\n            self.db.add(result)\n            self.db.commit()\n            self.db.refresh(result)\n\n        return result\n\n    def execute_with_retry(self, job_id: int, max_retries: int = 3) -> models.ScrapingResult:\n        """\uc7ac\uc2dc\ub3c4 \ub85c\uc9c1\uc774 \uc788\ub294 \uc2a4\ud06c\ub798\ud551"""\n        for attempt in range(max_retries):\n            try:\n                result = self.execute_job(job_id)\n\n                if result.status == "success":\n                    return result\n\n                # \uc2e4\ud328 \uc2dc \ub300\uae30 \ud6c4 \uc7ac\uc2dc\ub3c4\n                if attempt < max_retries - 1:\n                    time.sleep(2 ** attempt)  # Exponential backoff\n\n            except Exception as e:\n                if attempt == max_retries - 1:\n                    raise\n\n                time.sleep(2 ** attempt)\n\n        return result\n'})}),"\n",(0,s.jsx)(n.h3,{id:"mainpy",children:"main.py"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'from fastapi import FastAPI, Depends, HTTPException, BackgroundTasks\nfrom sqlalchemy.orm import Session\nfrom typing import List, Optional\nimport models, schemas\nfrom database import engine, get_db\nfrom scraper_service import ScraperService\nfrom static_scraper import StaticScraper\n\nmodels.Base.metadata.create_all(bind=engine)\n\napp = FastAPI(title="Web Scraping API")\n\n# ==================== \uc791\uc5c5 \uad00\ub9ac ====================\n\n@app.post("/scraping/jobs", response_model=schemas.ScrapingJob)\ndef create_job(\n    job: schemas.ScrapingJobCreate,\n    db: Session = Depends(get_db)\n):\n    """\uc2a4\ud06c\ub798\ud551 \uc791\uc5c5 \uc0dd\uc131"""\n    db_job = models.ScrapingJob(**job.dict())\n    db.add(db_job)\n    db.commit()\n    db.refresh(db_job)\n\n    return db_job\n\n@app.get("/scraping/jobs", response_model=List[schemas.ScrapingJob])\ndef list_jobs(\n    is_active: Optional[bool] = None,\n    db: Session = Depends(get_db)\n):\n    """\uc791\uc5c5 \ubaa9\ub85d"""\n    query = db.query(models.ScrapingJob)\n\n    if is_active is not None:\n        query = query.filter(models.ScrapingJob.is_active == is_active)\n\n    return query.all()\n\n@app.get("/scraping/jobs/{job_id}", response_model=schemas.ScrapingJob)\ndef get_job(job_id: int, db: Session = Depends(get_db)):\n    """\uc791\uc5c5 \uc870\ud68c"""\n    job = db.query(models.ScrapingJob).filter(\n        models.ScrapingJob.id == job_id\n    ).first()\n\n    if not job:\n        raise HTTPException(status_code=404, detail="Job not found")\n\n    return job\n\n@app.delete("/scraping/jobs/{job_id}")\ndef delete_job(job_id: int, db: Session = Depends(get_db)):\n    """\uc791\uc5c5 \uc0ad\uc81c"""\n    job = db.query(models.ScrapingJob).filter(\n        models.ScrapingJob.id == job_id\n    ).first()\n\n    if not job:\n        raise HTTPException(status_code=404, detail="Job not found")\n\n    db.delete(job)\n    db.commit()\n\n    return {"message": "Job deleted"}\n\n# ==================== \uc791\uc5c5 \uc2e4\ud589 ====================\n\n@app.post("/scraping/jobs/{job_id}/execute")\ndef execute_job(\n    job_id: int,\n    background_tasks: BackgroundTasks,\n    db: Session = Depends(get_db)\n):\n    """\uc791\uc5c5 \uc2e4\ud589"""\n    job = db.query(models.ScrapingJob).filter(\n        models.ScrapingJob.id == job_id\n    ).first()\n\n    if not job:\n        raise HTTPException(status_code=404, detail="Job not found")\n\n    # \ubc31\uadf8\ub77c\uc6b4\ub4dc\uc5d0\uc11c \uc2e4\ud589\n    background_tasks.add_task(run_scraping_job, job_id, db)\n\n    return {"message": "Job started", "job_id": job_id}\n\ndef run_scraping_job(job_id: int, db: Session):\n    """\uc2a4\ud06c\ub798\ud551 \uc791\uc5c5 \uc2e4\ud589"""\n    service = ScraperService(db)\n    service.execute_with_retry(job_id)\n\n# ==================== \uacb0\uacfc \uc870\ud68c ====================\n\n@app.get("/scraping/results", response_model=List[schemas.ScrapingResult])\ndef list_results(\n    job_id: Optional[int] = None,\n    status: Optional[str] = None,\n    skip: int = 0,\n    limit: int = 20,\n    db: Session = Depends(get_db)\n):\n    """\uacb0\uacfc \ubaa9\ub85d"""\n    query = db.query(models.ScrapingResult)\n\n    if job_id:\n        query = query.filter(models.ScrapingResult.job_id == job_id)\n\n    if status:\n        query = query.filter(models.ScrapingResult.status == status)\n\n    results = query.order_by(\n        models.ScrapingResult.created_at.desc()\n    ).offset(skip).limit(limit).all()\n\n    return results\n\n@app.get("/scraping/results/{result_id}", response_model=schemas.ScrapingResult)\ndef get_result(result_id: int, db: Session = Depends(get_db)):\n    """\uacb0\uacfc \uc870\ud68c"""\n    result = db.query(models.ScrapingResult).filter(\n        models.ScrapingResult.id == result_id\n    ).first()\n\n    if not result:\n        raise HTTPException(status_code=404, detail="Result not found")\n\n    return result\n\n# ==================== \uc989\uc2dc \uc2a4\ud06c\ub798\ud551 ====================\n\n@app.post("/scraping/quick")\ndef quick_scrape(request: schemas.QuickScrapeRequest):\n    """\uc989\uc2dc \uc2a4\ud06c\ub798\ud551 (\uc791\uc5c5 \uc800\uc7a5 \uc5c6\uc774)"""\n    scraper = StaticScraper()\n\n    try:\n        data = scraper.scrape(request.url, request.selectors)\n\n        return {\n            "status": "success",\n            "data": data,\n            "items_count": len(data)\n        }\n\n    except Exception as e:\n        return {\n            "status": "failed",\n            "error": str(e)\n        }\n\n@app.post("/scraping/extract-links")\ndef extract_links(url: str):\n    """\ub9c1\ud06c \ucd94\ucd9c"""\n    scraper = StaticScraper()\n\n    try:\n        links = scraper.scrape_links(url)\n\n        return {\n            "url": url,\n            "links": links,\n            "count": len(links)\n        }\n\n    except Exception as e:\n        raise HTTPException(status_code=500, detail=str(e))\n\n@app.post("/scraping/extract-table")\ndef extract_table(url: str, table_selector: str = \'table\'):\n    """\ud14c\uc774\ube14 \ub370\uc774\ud130 \ucd94\ucd9c"""\n    scraper = StaticScraper()\n\n    try:\n        data = scraper.scrape_table(url, table_selector)\n\n        return {\n            "url": url,\n            "data": data,\n            "rows": len(data)\n        }\n\n    except Exception as e:\n        raise HTTPException(status_code=500, detail=str(e))\n'})}),"\n",(0,s.jsx)(n.h2,{id:"-\ud575\uc2ec-\uac1c\ub150",children:"\ud83d\udd11 \ud575\uc2ec \uac1c\ub150"}),"\n",(0,s.jsx)(n.h3,{id:"\uc815\uc801-vs-\ub3d9\uc801-\uc2a4\ud06c\ub798\ud551",children:"\uc815\uc801 vs \ub3d9\uc801 \uc2a4\ud06c\ub798\ud551"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"\uc815\uc801"}),": BeautifulSoup - \ube60\ub974\uace0 \uac00\ubcbc\uc6c0"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"\ub3d9\uc801"}),": Selenium - JavaScript \ub80c\ub354\ub9c1 \ud544\uc694 \uc2dc"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"\ubd07-\ud0d0\uc9c0-\uc6b0\ud68c",children:"\ubd07 \ud0d0\uc9c0 \uc6b0\ud68c"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"User-Agent \uc124\uc815"}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"webdriver"})," \uc18d\uc131 \uc81c\uac70"]}),"\n",(0,s.jsx)(n.li,{children:"\ud5e4\ub4dc\ub9ac\uc2a4 \ubaa8\ub4dc \uc0ac\uc6a9"}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"\uc18d\ub3c4-\uc81c\ud55c-\uc900\uc218",children:"\uc18d\ub3c4 \uc81c\ud55c \uc900\uc218"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"wait_time"}),"\uc73c\ub85c \uc694\uccad \uac04\uaca9 \uc870\uc808"]}),"\n",(0,s.jsx)(n.li,{children:"Exponential backoff \uc7ac\uc2dc\ub3c4"}),"\n",(0,s.jsx)(n.li,{children:"\ud504\ub85d\uc2dc \ub85c\ud14c\uc774\uc158"}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"\ub370\uc774\ud130-\ucd94\ucd9c-\ud328\ud134",children:"\ub370\uc774\ud130 \ucd94\ucd9c \ud328\ud134"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"CSS \uc120\ud0dd\uc790\ub85c \uc720\uc5f0\ud55c \ucd94\ucd9c"}),"\n",(0,s.jsx)(n.li,{children:"\ucee8\ud14c\uc774\ub108 \uae30\ubc18 \ubc18\ubcf5 \ucd94\ucd9c"}),"\n",(0,s.jsx)(n.li,{children:"\ud14c\uc774\ube14, \ub9c1\ud06c \ud2b9\ud654 \uba54\uc11c\ub4dc"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"-\ub2e4\uc74c-\ub2e8\uacc4",children:"\ud83d\udcda \ub2e4\uc74c \ub2e8\uacc4"}),"\n",(0,s.jsxs)(n.p,{children:["\ud83d\udc49 ",(0,s.jsx)(n.a,{href:"./scenario-16-streaming-pipeline",children:"\uc2a4\ud2b8\ub9ac\ubc0d \ub370\uc774\ud130 \ud30c\uc774\ud504\ub77c\uc778"})]})]})}function p(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(d,{...e})}):d(e)}},8453:(e,n,t)=>{t.d(n,{R:()=>a,x:()=>o});var r=t(6540);const s={},i=r.createContext(s);function a(e){const n=r.useContext(i);return r.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:a(e.components),r.createElement(i.Provider,{value:n},e.children)}}}]);